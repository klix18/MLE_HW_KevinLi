# Hybrid RAG Pipeline for arXiv cs.CL Papers - SEMANTIC + KEYWORD SEARCH
### This project implements a Retrieval-Augmented Generation (RAG) pipeline over a collection of arXiv papers.
It downloads PDFs, extracts and chunks text, embeds chunks with SentenceTransformers, builds both a FAISS semantic index and a SQLite keyword (FTS5) index, and supports hybrid retrieval (semantic + keyword). At query time, the pipeline retrieves relevant chunks and asks an OpenAI model to generate an answer.

An evaluation script compares vector-only, keyword-only, and hybrid retrieval methods using Recall@k.

## Project workflow
1. Download recent cs.CL PDFs from arXiv.
2. Extract text from each PDF.
3. Chunk the text.
4. Create embeddings for each chunk.
5. Build a FAISS index over all chunk embeddings and save an IDâ†’(pdf, chunk) mapping.
6. Store metadata in SQLite by loading document/chunk metadata into rag.db with an FTS5 full-text index for keyword search.
7. At query time: Embed user question. Run semantic search via FAISS + keyword search via SQLite FTS5. Merge results with hybrid fusion (weighted sum or reciprocal rank fusion).
8. Concatenate the top-N chunks into context and ask the LLM to answer.
9. Return the answer along with the sources (PDF paths/chunk refs).
10. Evaluation (evaluate_retrieval.py): Load test queries from ground_truth.json. Compare vector-only, keyword-only, and hybrid retrieval. Report Recall@k and per-query diagnostics (hits, found, recall).

## ğŸ“ Project folder structure

```
RAG_W_SQLite_FAISS/
â”œâ”€ master_setup.py         # orchestrates end-to-end pipeline
â”œâ”€ query.py                # query interface with semantic / keyword / hybrid modes
â”œâ”€ retrieval.py            # search utilities (FAISS, SQLite FTS5, hybrid fusion)
â”œâ”€ evaluate_retrieval.py   # evaluation script (Recall@k across methods)
â”œâ”€ step1_scrape.py         # download PDFs from arXiv
â”œâ”€ step2_extract.py        # extract text + OCR
â”œâ”€ step3_chunk.py          # split text into chunks
â”œâ”€ step4_embed.py          # embed chunks with SentenceTransformers
â”œâ”€ step5_faiss.py          # build FAISS index + id mapping
â”œâ”€ rag.db                  # SQLite DB with FTS5 index (created at runtime)
â”œâ”€ pdfs/                   # downloaded PDFs (created at runtime)
â”œâ”€ texts.json              # extracted text per PDF (created)
â”œâ”€ chunks.json             # chunked text per PDF (created)
â”œâ”€ documents.json          # [{chunk, embedding}] per PDF (created)
â”œâ”€ id_mapping.json         # FAISS id â†’ (pdf, chunk_idx) mapping (created)
â”œâ”€ faiss.index             # FAISS vector index (created)
â”œâ”€ ground_truth.json       # evaluation queries + relevant docs (you create)
â””â”€ .env                    # your OpenAI API key (you create)
```

## ğŸ“¦ Requirements
Make sure you have Python 3.9+ and virtualenv or venv set up.

```
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```
## â• Add your OpenAI API key
Create a .env file in the project root:
```
OPENAI_API_KEY=sk-...your-key...
```
## ğŸš€ Running the Pipeline
The workflow is orchestrated by master_setup.py.

### Full Run (scrape, extract, chunk, embed, index)
```
python3 master_setup.py
```

## â“ Querying the RAG System
Once setup is complete, you can ask questions with query.py.

```
python query.py "TYPE YOUR QUESTION HERE" --mode hybrid --k 5 --weight_sem 0.5
```

At inference, you can tune the hyperparameters:
```
--mode hybrid (retrieval mode: semantic / keyword / hybrid)
--k 5 (how many chunks is returned)
--weight_sem 0.5 (how much the model's answer tilts towards semantic search)
```

## Evaluating the RAG System
Once you have collected an amount of Q+A pairs, you can evaluate the system to see which method retrieves relevant documents better.
First enter the query and relevant doc paths into the ground_truth.json
This is the datasource used by evaluate_retrieval.py

```
python evaluate_retrieval.py
```

## ğŸ“‚ Outputs

pdfs/ â†’ Downloaded papers from arXiv

texts.json â†’ Extracted raw text per PDF

chunks.json â†’ Text split into chunks

documents.json â†’ Embedded document chunks

id_mapping.json â†’ Index â†’ (doc, chunk) mapping

faiss.index â†’ FAISS vector index

## ğŸ›  Notes
Chunk size and overlap can be tuned in step3_chunk.py.

Embedding model can be swapped in step4_embed.py or query.py.

The query_rag() function retrieves top-k chunks and feeds them to GPT.
