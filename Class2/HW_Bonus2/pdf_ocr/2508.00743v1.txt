Agentic large language models improve
retrieval-based radiology question answering

Sebastian Wind (1,2), Jeta Sopa (1), Daniel Truhn (3), Mahshad Lotfinia (3), Tri-Thien
Nguyen (1,4), Keno Bressem (5), Lisa Adams (5,6), Mirabela Rusu (6,7), Harald Kostler
(2,8), Gerhard Wellein (2), Andreas Maier (1,2), Soroosh Tayebi Arasteh (1,3,6,7)

(1) Pattern Recognition Lab, Friedrich-Alexander-Universitat Erlangen-Nurnberg, Erlangen,
Germany.

(2) Erlangen National High Performance Computing Center, Friedrich-Alexander-Universitat
Erlangen-Nurnberg, Erlangen, Germany.

(3) Department of Diagnostic and Interventional Radiology, University Hospital RWTH Aachen,
Aachen, Germany.

(4) Institute of Radiology, University Hospital Erlangen, Erlangen, Germany.

(5) Department of Diagnostic and Interventional Radiology, Klinikum Rechts der Isar, Technical
University of Munich, Munich, Germany.

(6) Department of Radiology, Stanford University, Stanford, CA, USA.

Department of Urology, Stanford University, Stanford, CA, USA.

(8) Chair of Computer Science 10, Friedrich-Alexander-Universitat Erlangen-Nurnberg, Erlangen,
Germany.

=
~i
=>

Correspondence

Sebastian Wind, MSc (sebastian.wind@fau.de) or

Soroosh Tayebi Arasteh, PhD, PhD (soroosh.arasteh@fau.de)
Pattern Recognition Lab

Friedrich-Alexander-Universitat Erlangen-Nurnberg
Martensstr. 3

91058 Erlangen, Germany

This is a preprint version submitted to ArXiv.
August 1, 2025


Abstract

Clinical decision-making in radiology increasingly benefits from artificial intelligence (Al),
particularly through large language models (LLMs). However, traditional retrieval-augmented
generation (RAG) systems for radiology question answering (QA) typically rely on single-step
retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an
agentic RAG framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia.org, and dynamically synthesize
evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter
scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically
fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-
RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P = 2.3 x 107”) and conventional online RAG
(73% vs. 68%; 5.0 x 107°). The greatest gains occurred in mid-sized models (e.g., Mistral Large
improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to
71%), while very large models (>200B parameters) demonstrated minimal changes (<2%
improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved
Clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically
fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from
71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight
the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology
QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.

1. Introduction

Artificial intelligence (Al) is rapidly transforming diagnostic radiology by enhancing imaging
interpretation, improving diagnostic precision, and streamlining clinical workflows'?. Recent
advances in large language models (LLMs)*’, such as GPT-4°8, have shown remarkable
capability in tasks ranging from extracting structured information from radiology reports and
assisting in clinical reasoning, to facilitating seamless natural language interfaces**"'”. Despite
these capabilities, a significant limitation persists: the static nature of LLMs' training data, which
can lead to incomplete, outdated, or biased knowledge, thus compromising clinical accuracy and
reliability.

Retrieval-augmented generation (RAG)'%, which combines LLMs with domain-specific
external knowledge sources, has emerged as a promising strategy to address these limitations.
By grounding model-generated outputs in up-to-date and verified information, RAG could
enhance the factual accuracy of LLM responses and reduces the risk of hallucinations, generated
outputs without factual basis®°'*-"”. Tayebi Arasteh et al. recently introduced Radiology RAG
(RadioRAG)'®, an online RAG framework utilizing real-time information from Radiopaedia’?,
demonstrating substantial accuracy improvements in certain LLMs, such as GPT-3.5-turbo


compared to conventional zero-shot inference. Nevertheless, these improvements were
inconsistent across all evaluated models, with models like Llama3-8B showing negligible gains,
highlighting inherent limitations in traditional single-step retrieval architectures. Current online
RAG frameworks", including RadioRAG"®, primarily employ a single-step retrieval and generation
process, limiting their ability to manage complex, multi-part clinical questions effectively’. This
design lacks the capability to iteratively refine queries, dynamically seek additional information,
or systematically evaluate intermediate uncertainty*'. Consequently, there is a clear need to
evolve RAG approaches towards more sophisticated reasoning and retrieval strategies"®.

Recently, agentic frameworks have emerged as an advanced paradigm within Al
research, particularly for LLMs*?**4. These frameworks enable models to autonomously
orchestrate retrieval?°, reasoning, and synthesis in iterative multi-step chains?°?’, allowing for
dynamic adaptation and enhanced problem-solving capabilities?*°°. Agentic approaches have
demonstrated notable success across various domains, including clinical decision-making,
oncology, and scientific research, by enabling models to dynamically select retrieval strategies,
systematically evaluate intermediate results, and adapt their reasoning strategies based on
evolving contexts***'. For example, agent-based systems have improved the accuracy and
interpretability of Al-driven decisions in oncology”?, general clinical tasks, and biomedical
research”?, demonstrating clear advantages over static prompting and traditional RAG
methodologies. However, despite these promising outcomes in other clinical domains, the utility
and effectiveness of agentic LLMs for specialized radiological applications remain largely
unexplored. Radiology presents unique challenges, characterized by diverse and complex clinical
questions often requiring nuanced, multi-step reasoning and domain-specific knowledge
retrieval.

In this study, we address this crucial gap by systematically evaluating the effectiveness of
agentic LLMs in radiology question answering (QA), specifically by integrating them into an online
RAG framework leveraging the comprehensive radiological knowledge base of Radiopaedia.org.
Our approach leverages a multi-agent pipeline that autonomously decomposes each clinical
question into structured diagnostic options, retrieves targeted evidence from Radiopaedia.org,
and synthesizes evidence-based responses through iterative reasoning steps. We utilize a
benchmark dataset comprising 104 expert-curated radiology questions from previously
established RSNA-RadioQA and ExtendedQA datasets, both from the RadioRAG study’®,
comparing the diagnostic performance of zero-shot inference, non-agentic online RAG, and our
agentic approach. Supplementary Table 1 reports the characteristics of the datasets. Our
evaluation spans 24 different LLMs encompassing a broad spectrum of architectures, parameter
scales, and training paradigms. These include proprietary models (e.g., GPT-4-turbo®, GPT-3.5-
turbo), open-weight models (e.g., Mistral Large, Qwen 2.5°°), and domain-specialized variants
fine-tuned for clinical applications (e.g., MedGemma™, Llama3-Med42*°). The models range from
small-scale architectures (as low as 0.5 billion parameters) to mid-sized (17—110B) and very large
models exceeding 200 billion parameters, including substantially sized systems such as
DeepSeek-R1* and 03. For details on different models used in this study, refer to Table 1. This
breadth allows us to systematically assess the impact of agentic retrieval across general-purpose,
medically fine-tuned, and reasoning-optimized LLMs within a heterogeneous model landscape for
radiology QA.


Our results show that agentic retrieval consistently enhances diagnostic accuracy and
factual reliability across most model classes. The improvements are most prominent in small and
mid-sized models, where conventional retrieval methods are often insufficient. In contrast, very
large models (>200B parameters) with strong internal reasoning capabilities tend to benefit less
from external evidence, reflecting their extensive pretraining and broad generalization ability.
Nonetheless, even clinically fine-tuned models exhibit meaningful gains from agentic reasoning,
suggesting that retrieval and fine-tuning offer complementary strengths. Additionally, we show
that agentic retrieval reduces hallucinations and retrieves clinically relevant content that can assist
not only LLMs but also expert radiologists. These findings highlight the potential of agentic
frameworks to improve factuality and diagnostic accuracy in radiology QA, warranting further
investigation into their clinical utility and practical integration. We provide an overview of our entire
pipeline in Figure 1 and illustrate a full worked representative example for a clinical question in
Figure 2, with additional methodological details outlined in Materials and Methods.

MULTI-AGENT LLM RESEARCH PIPELINE ARCHITECTURE

@ INPUT SPECIFICATION

HF) * question id —
unique identifier

= stem — full

@ Finat report is
ADDED TO THE
PROMPT

question text

2 ) boa SUPERVISOR AGENT
= summary -— short

rephrasing for =7 rafts research plan.
search prompts
= options - list[str] Delegates
of candidate sections.
answers

S363 Generates keywords for LL M
@a each option
LLM serving
| Executes the supervisor's plan for
© an individual section j

&
Integrates & exports results.

Retrieves evidence with search
¥ ol tools. Drafts the section using C
<> ‘ the Section tool
EN PROMPT

Ss \ f

ET)

GPT4-mini iseit
Converter

_ SEARCH TOOLS. , REPORTTOOLS , SECTION TOOLS

SearXNG
Radiopaedia

Figure 1: Multi-agent architecture of the agentic retrieval framework for radiology question
answering. The pipeline combines structured retrieval with multi-step reasoning to generate evidence-
grounded diagnostic reports. (1) Each question is preprocessed to extract key diagnostic concepts (using
Mistral Large) and paired with multiple-choice options. (2) A supervisor agent creates a structured research
plan, delegating each diagnostic option to a dedicated research agent. (3) Research agents iteratively
retrieve targeted evidence from www.radiopaedia.org via a SearXNG-powered search tool, refining queries
when needed. (4) Retrieved content is synthesized into structured report sections (using GPT-40-mini and
formatting tools), including supporting and contradicting evidence with citations. (5) The supervisor
compiles all sections into a final diagnostic report (introduction, analysis, and conclusion), which is
appended to the prompt for final answer selection. The entire workflow is coordinated through a stateful
directed graph that preserves shared memory, retrieved context, and intermediate drafts.


Table 1: Specifications of the language models evaluated in this study. Summary of the 24 large language models
assessed across zero-shot prompting, traditional online RAG, and agentic retrieval. Listed for each model are
parameter count (in billions), training category (e.g., instruction-tuned (IT), reasoning-optimized, clinically aligned),
accessibility, knowledge cutoff date, developer, and context length (in thousand tokens). All evaluations were
conducted using outputs generated between July 1-27, 2025.

Model name

Ministral-8B

Parameters
(billion)

Category

Accessibility

Open-source

Knowledge
cutoff date

October 2023

Developer

Mistral Al

Context length
(thousand tokens)

Mistral Large

Open-source

November 2024

Mistral Al

Llama3.3-8B

Open-weights

March 2023

Meta Al

Llama3.3-70B

Open-weights

December 2023

Meta Al

Llama3-Med42-8B

IT, clinically-aligned

Open-weights

August 2024

M42 Health Al Team

8

Llama3-Med42-70B

IT, clinically-aligned

Open-weights

August 2024

M42 Health Al Team

8

Llama4 Scout 16E

IT, 17B active parameters

Open-weights

August 2023

Meta Al

10,000 (10M)

DeepSeek R1-70B

Reasoning

Open-source

January 2025

DeepSeek

128

DeepSeek-R1

Reasoning

Open-source

January 2025

DeepSeek

128

DeepSeek-V3

Mixture of experts

Open-source

July 2024

DeepSeek

128

Qwen 2.5-0.5B

Open-source

September 2024

Alibaba Cloud

32

Qwen 2.5-3B

Open-source

September 2024

Alibaba Cloud

32

Qwen 2.5-7B

Open-source

September 2024

Alibaba Cloud

Qwen 2.5-14B

Open-source

September 2024

Alibaba Cloud

Qwen 2.5-70B

70

Open-source

September 2024

Alibaba Cloud

Qwen 3-8B

8

Reasoning, mixture of experts

Open-source

December 2024

Alibaba Cloud

Qwen 3-235B

235

Reasoning, mixture of experts

Open-source

July 2025

Alibaba Cloud

GPT-3.5-turbo

Undisclosed

IT

Proprietary

September 2021

OpenAl

GPT-4-turbo

Undisclosed

IT

Proprietary

December 2023

OpenAl

03

Undisclosed

Reasoning

Proprietary

June 2024

OpenAl

MedGemma-4B-it

Gemma 3-based,
multimodal, IT, clinical reasoning

Open-weights

July 2025

Google DeepMind

MedGemma-27B-
text-it

Gemma 3-based,
text only, IT, clinical reasoning

Open-weights

July 2025

Google DeepMind

Gemma-3-4B-it

Open-weights

August 2024

Google DeepMind

Gemma-3-27B-it

Open-weights

August 2024

Google DeepMind



2. Results

2.1. Agentic retrieval improves radiology QA performance on
average

We assessed the diagnostic performance of 24 LLMs across three distinct inference strategies:
zero-shot prompting, conventional online RAG, and our proposed agentic RAG framework. The
LLMs __ included: Ministral-8B, Mistral Large, Llama3.3-8B%’°8, — Llama3.3-70B°"*8,
Llama3-Med42-8B**, Llama3-Med42-70B*, Llama4 Scout 16E%, DeepSeek R1-70B*,
DeepSeek-R1°°, DeepSeek-V3°°, Qwen 2.5-0.5B*, Qwen 2.5-3B°, Qwen 2.5-7B*, Qwen
2.5-14B**, Qwen 2.5-70B**, Qwen 3-8B*°, Qwen 3-235B*°, GPT-3.5-turbo, GPT-4-turbo®, 03,
MedGemma-4B-it**, MedGemma-27B-text-it*, Gemma-3-4B-it*'4?, and Gemma-3-27B-it*.
Accuracy was measured using the 104-question RadioRAG benchmark dataset, with detailed
results presented in Table 2. When aggregating results across all LLMs, the agentic RAG
framework demonstrated a statistically significant improvement in accuracy compared to zero-
shot prompting (P = 2.3 x 107”). As previously established, the traditional RAG approach also
outperformed zero-shot prompting, showing a smaller but statistically significant gain (P = 0.013).
Importantly, the proposed agentic framework further outperformed traditional online RAG (P =
5.0 x 107°), underscoring the benefit of iterative retrieval and autonomous reasoning over single-
pass retrieval pipelines. These findings indicate that, at the group level, agentic reasoning
introduces measurable and additive improvements in radiology question answering, even when
compared against established, high-performing RAG systems.

2.2. Agentic retrieval improves factual grounding and reduces

hallucination

To assess factual reliability under the agentic framework, we conducted a hallucination analysis
across all 24 LLMs using the 104-question RadioRAG benchmark. Each response was reviewed
by a board-certified radiologist (TTN) to evaluate (i) whether the retrieved context was clinically
relevant, (ii) whether the model's answer was grounded in that context, and (iii) whether the final
output was factually correct. Context was classified as relevant only if it contained no incorrect or
off-topic content relative to the diagnostic question, a deliberately strict criterion. Under this
definition, clinically relevant evidence was retrieved in 46% of cases (48/104). Detailed results are
provided in Table 3.

When relevant context was available, most models demonstrated strong factual
alignment. Hallucinations, defined as incorrect answers despite the presence of relevant context,
occurred in only 9.4% + 5.9 of questions. The lowest hallucination rates were observed in large-
scale and reasoning-optimized models such as 03 (2%), DeepSeek R1 (3%), and Qwen 3-235B
(5%), reflecting their superior ability to integrate and interpret retrieved content (see Figure 3). In
contrast, smaller models such as Qwen 2.5-0.5B (26%) and Gemma-3-4B-it (20%) struggled to
do so reliably, exhibiting significantly higher rates of unsupported reasoning.


DETAILS OF THE AGENT’S PIPELINE REPORT AND PROMPT ASSEMBLY

Question: 65-year-old woman, weight loss, recurrent fever, exertional dyspnea, chronic cough, syncope for 3 months, left
atrial mass with low signal intensity, associated with interatrial septum.

A: Cardiac myxoma

B: Papillary fibroelastoma
C: Rhabdomyosarcoma
D: Left atrial thrombus

Stage 1: Pipeline Stage 2: Report and answer generation

Preprocessing Introduction
This report investigates the differential diagnosis of a
left atrial mass in a 65-year-old woman presenting
with systemic symptoms including weight loss,
Question summary: 65-year-old woman, weight loss, recurrent fever, exertional dyspnea, chronic cough,
recurrent fever, exertional dyspnes, and syncope over three months. The mass is described
chronic cough, syncopefor 3 months, as having low signal intensity on imaging and is ;
. . . + . associated with the interatrial septum. The differential
left atrial mass with low signal intensity, di a .
: ae . liagnosis includes cardiac myxoma, papillary
associated with interatrial septum fibroelastoma, rhabdomyosarcoma, and left atrial
thrombus. Each entity is explored in terms of clinical
presentation, typical anatomical location, imaging
characteristics, and relevance to the patient's
Diagnostic keywords: cardiac myxoma clinical features, (*...) Doms we PI povide)scomnbrehensie Under abding
of potential diagnoses.
Cardiac Myxoma
Cardiac myxomas are the most common primary

(2) Supervisor Agent benign cardiac tumors in adults, accounting for
p € Research Agent approximately 50% of such tumors (1). (*...)

Calcifications may be present, often peripheral or
patchy, and can be detected on CT or
echocardiography (2). The size of myxomas varies, but
they can cause obstruction of adjacent valves, leading
to symptoms such as dyspnea or syncope. (*...)

Sectioned
> Report
Generation
Formulate Research Plan \ } Sources

OO 1. https://radiopaedia.org/articles/cardiac-myxoma
2. https://radiopaedia.org/cases/left-atrial-myxoma
3. https://radiopaedia.org/cases/atrial-myxoma

Parallel Papillary fibroelastoma

<p» Websearch Papillary fibroelastomas (PFEs) are rare benign primary
cardiac tumors, yet among primary cardiac tumors,
they are relatively (*...)

Delegate Sections

Rhabdomyosarcoma
Synthesize Final Report Cardiac rhabdomyosarcoma is a rare malignant tumor
arising from skeletal muscle cells within the heart. (*...)

Evidence Left atrial thrombus

Display Left atrial thrombus refers to the formation of a blood
clot within the left atrium, most commonly in the left
atrial appendage, which is anatomically attached to
the left inferior portion of the left atrium(1)(2). (*...)

iN Conclusion
v The differential diagnosis of a left atrial mass with
systemic symptoms such as weight loss, fever, and
syncope includes a range of benign and malignant
4 ) cardiac conditions. Cardiac myxomas are common
Prompt benign tumors typically located in the left atrium
attached to the interatrial septum and can present
co with constitutional symptoms and embolic
phenomena. Papillary fibroelastomas, although benign
</ > Report G and often associated with embolic events, usually arise
L ~ “ from cardiac valves rather than the atrial septum and
are less likely to cause systemic symptoms.
| Rhabdomyosarcomas represent rare malignant cardiac

Question tumors that may present with systemic illness and
XM B aggressive features but are less common and...

Figure 2: Representative example of the agentic retrieval process for a radiology question
answering item. This figure shows the full agentic workflow for a representative question (RSNA-RadioQA-
Q53) involving a patient with systemic symptoms and a low signal intensity left atrial mass associated with
the interatrial septum. The pipeline begins with keyword-based summarization to guide retrieval, followed
by parallel evidence searches for each diagnostic option using Radiopaedia.org. Retrieved content is
synthesized into a structured report, including an introduction, citation-backed analyses of all options
(cardiac myxoma, papillary fibroelastoma, rhabdomyosarcoma, and left atrial thrombus), and a neutral
conclusion. The approach supports interpretable, evidence-grounded radiology question answering.


Table 2: Accuracy of language models across zero-shot prompting, traditional online RAG, and agentic
retrieval on the RadioRAG dataset. Accuracy is reported in percentage as mean + standard deviation, with 95%
confidence intervals shown in brackets. Results are based on 104 questions, using bootstrapping with 1,000 repetitions
and replacement while preserving pairing. P-values were calculated for each model using McNemar’s test on paired
outcomes relative to the agentic method and adjusted for multiple comparisons using the false discovery rate. A p-
value < 0.05 was considered statistically significant. Accuracy is presented alongside total correct answers per method.

Zero-shot

Online RAG

Agentic

moastname Accuracy (%) correct (n) Accuracy (%) correct (n) Accuracy (%) correct (n)
Ministral-8B 47 + 5 [38, 57] 51 +5 [41, 61] 66 + 5 [57, 76]
Mistral Large 72 + 4 [63, 81] 74 + 4 [65, 83] 81 +4 [72, 88]
Llama3.3-8B 62 + 5 [53, 71] 63 + 5 [55, 72] 65 + 5 [57, 74]
Llama3.3-70B 76 + 4 [67, 84] 73 + 4 [63, 81] 83 + 4 [75, 89]
Llama3-Med42-8B 67 + 5 [58, 77] 67 + 5 [59, 77] 75 + 4 [66, 84]
Llama3-Med42-70B 72 + 4 [63, 80] 75 + 4 [67, 83] 79 +4 [71, 87]
Llama4 Scout 16E 76 + 4 [67, 85] 80 + 4 [72, 88] 81 +4 [73, 88]
DeepSeek R1-70B 78 + 4 [70, 86] 76 + 4 [67, 84] 80 + 4 [72, 88]
DeepSeek R1 82 + 4 [74, 89] 79 + 4 [71, 87] 80 + 4 [72, 88]
DeepSeek-V3 76 + 4 [67, 84] 80 + 4 [72, 88] 86 + 4 [78, 92]
Qwen 2.5-0.5B 37 + 5 [27, 46] 46 + 5 [37, 56] 42 + 5 [32, 52]
Qwen 2.5-3B 54 + 5 [44, 63] 53 + 5 [43, 62] 65 + 5 [56, 74]
Qwen 2.5-7B 55 + 5 [45, 64] 59 + 5 [49, 68] 71 + 4 [62, 80]
Qwen 2.5-14B 68 + 4 [59, 77] 67 + 5 [57, 76] 72 + 4 [63, 81]
Qwen 2.5-70B 70 + 5 [62, 79] 73 + 4 [64, 82] 78 + 4 [70, 86]
Qwen 3-8B 66 + 5 [57, 75] 73 + 4 [65, 81] 76 + 4 [68, 84]
Qwen 3-235B 84 + 4 [75, 90] 82 + 4 [74, 89] 83 + 4 [75, 89]
GPT-3.5-turbo 57 + 5 [47, 66] 62 + 5 [53, 71] 68 + 5 [60, 77]
GPT-4-turbo 76 + 4 [67, 84] 76 + 4 [67, 84] 77 + 4 [69, 85]
03 86 + 4 [78, 92] 85 + 4[77, 91] 88 + 3 [81, 93]

MedGemma-4B-it

, 65]

52 + 5 [42, 62]

66 + 5 [57, 75]

MedGemma-27B-text-it

, 79)

75 + 4 [66, 84]

81 + 4 [73, 88]

Gemma-3-4B-it

, 56]

53 + 5 [43, 62]

62 + 5 [52, 71]

Gemma-3-27B-it

65 + 5 [57, 75]

66 + 5 [58, 75]

76 + 4 [67, 85]



Interestingly, a substantial proportion of agentic responses were correct despite the
retrieved context being clinically irrelevant. On average, 37.4% + 4.2 of responses fell into this
category. This behavior was particularly pronounced among models with strong internal reasoning
capabilities, DeepSeek-V3, 03, and Qwen 3-235B each exceeded 40%, suggesting that in the
absence of relevant evidence, these models often defaulted to internal knowledge. Similar trends
were observed in mid-sized and clinically aligned models, such as Llama3.3-70B, Mistral Large,
and MedGemma-27B-text-it, which also maintained high accuracy without external grounding.
Conversely, smaller models like Qwen 2.5-0.5B (21%) and Ministral-8B (35%) were less effective
under these conditions, indicating greater dependence on successful retrieval.

Across models, an average of 14.6% + 6.4 of questions were answered incorrectly under
zero-shot prompting but correctly after agentic retrieval, highlighting the additive diagnostic value
of structured evidence acquisition. Supplementary Tables 2 and 3 provide example responses
from GPT-3.5-turbo with and without agentic retrieval, alongside the corresponding retrieved
content. These findings indicate that agentic retrieval improves factual grounding and reduces
hallucination by enabling structured, clinically aware evidence refinement. However, model
behavior in the absence of relevant context varies substantially, with larger and reasoning-tuned
models demonstrating greater resilience through fallback internal reasoning.

2.3. Agentic retrieval improves performance in small-scale LLMs

We next assessed whether model size influences the effectiveness of agentic retrieval in
radiology question answering (see Figure 4). Across the seven smallest models in our study
(including Ministral-8B, Gemma-3-4B-it, Qwen 2.5-7B, Qwen 2.5-3B, Qwen 2.5-0.5B, Qwen 3-
8B, and Llama-3-8B), we observed a consistent trend: conventional online RAG outperformed
zero-shot prompting (P = 0.002), and the agentic framework further improved over both baselines
(P = 0.016 vs. zero-shot; P = 0.035 vs. traditional online RAG). When examining individual
models, only two of the seven demonstrated statistically significant improvements with agentic
retrieval compared to zero-shot prompting: Qwen 2.5-7B (71% + 4 [95% Cl: 62, 80] vs. 55% +5
[95% Cl: 45, 64]; P = 0.040) and Ministral-8B (66% + 5 [95% Cl: 57, 76] vs. 47% + 5 [95% Cl: 38,
57]; P = 0.019). The remaining models exhibited absolute accuracy improvements ranging from
3% to 16%, though these did not reach statistical significance after correction for multiple
comparisons.

These findings suggest that agentic RAG can enhance performance in small-scale LLMs.
However, the degree of benefit varied across models, likely reflecting differences in pretraining
data, instruction tuning, and architectural design, even within a similar parameter range.


a Hallucination rates using the agentic framework

N
a

Hallucinations (%)

LLMs

b Correctness rates despite irrelevant context

43 43 49

4l
MN HITT] |
PS, se

Se
Sy 2S & og

Correct despite irrelevant

context (%)

LLMs

c Agentic gain over zero-shot responses

26 95
1

21 21 21 99 20
|
& °

S © P&S KX OY SX & YP WP & ry
& SX © S oN oes SEES SS wt

Zero-shot incorrect but
a correct (%)

?

~~
¥ ¢
& i oF se ny bie Ow
& oF Fo ES

LLMs

Figure 3. Factuality assessment of LLM responses on the RadioRAG dataset (n = 104). Each bar plot
shows the proportion of cases per model falling into a specific factuality category, with models ordered by
descending percentage. (a) Hallucinations: Cases in which the provided context was relevant, but the model
still generated an incorrect response (context = 1, response = 0). (b) Context irrelevance tolerance: Cases
where the model produced a correct response despite the retrieved context being unhelpful or irrelevant
(context = 0, response = 1). (c) Agentic correction: Instances where the zero-shot response was incorrect
but the Agentic strategy successfully produced a correct response (zero-shot = 0, agentic = 1).

10


2.4. Very large LLMs show no benefit from retrieval augmentation

We next evaluated the effect of agentic retrieval on the largest LLMs in our study, comprising
DeepSeek-R1, DeepSeek-V3, 03, Qwen 3-235B, and GPT-4-turbo, all likely to be exceeding 200
billion parameters. These models demonstrated strong performance under zero-shot prompting
alone, achieving diagnostic accuracies ranging from 76% to 86% on the RadioRAG benchmark
(Table 2). Neither conventional online RAG (P = 0.757) nor agentic retrieval (P = 0.299) led to
meaningful improvements.

Across all five models, accuracy differences between the three inference strategies were
minimal (see Figure 4). For example, DeepSeek-R1 performed at 82% + 4 [95% Cl: 74, 89] with
zero-shot, 80% + 4 [95% Cl: 72, 88] with agentic retrieval, and 79% + 4 [95% Cl: 71, 87] with
conventional online RAG; 03 improved marginally from 86% + 4 [95% Cl: 78, 92] to 88% + 3 [95%
Cl: 81, 93] with agentic RAG; and Qwen3-235B and GPT-4-turbo showed <1% changes across
conditions. DeepSeek-V3 showed slightly higher improvement (from 76% + 4 [95% Cl: 67, 84] to
86% + 4 [95% Cl: 78, 92]) but still not significant. Traditional RAG showed similarly negligible
differences.

These findings indicate that very large LLMs can already handle complex radiology QA
tasks with high accuracy without requiring external retrieval. This likely reflects their extensive
pretraining on large-scale corpora, improved reasoning abilities, and domain-general coverage,
diminishing the marginal value of either conventional or agentic retrieval augmentation in high-
performing settings.

2.5. Agentic RAG yields consistent gains in mid-sized models

Mid-sized models, typically ranging between 17B and 110B parameters, represent a particularly
relevant category for clinical deployment, offering a favorable trade-off between performance and
computational efficiency. This group in our study included GPT-3.5-turbo, Llama 3.3-70B, Mistral
Large, Qwen 2.5-70B, Llama 4 Scout 16E, Gemma-3-27B-it, and DeepSeek-R1-70B. Across this
cohort, the conventional online RAG framework did not yield a statistically significant improvement
in accuracy over zero-shot prompting (P = 0.253). In contrast, the agentic RAG framework
significantly outperformed both zero-shot (P = 0.001) and traditional RAG (P = 0.002), suggesting
that the benefits of agentic reasoning become more apparent in this model size range, where
LLMs are strong enough to follow reasoning chains but may still benefit from structured multi-step
guidance. While every model in this group showed an absolute improvement in diagnostic
accuracy with the agentic system, for example, GPT-3.5-turbo improved from 57% to 68%, Llama
3.3-70B from 76% + 4 [95% Cl: 67, 84] to 83% + 4 [95% Cl: 75, 89], and Mistral Large from 72%
+ 4 [95% Cl: 63, 81] to 81% + 4 [95% Cl: 73, 88], none of these increases reached statistical
significance when evaluated individually (see Figure 4). Nonetheless, the consistency of the
improvements across models suggests a robust and reproducible trend that favors agentic
retrieval strategies in this deployment-friendly tier.

11


Table 3: Hallucination and relevance metrics for agentic responses on the RadioRAG dataset (n = 104). "Context
relevant" was evaluated at the dataset level: each question was labeled as having relevant or irrelevant retrieved
context, and the same label was applied across all models (48/104 questions were judged to have clinically appropriate
context). “Hallucination” refers to incorrect model answers despite relevant context. “Correct despite irrelevant context”
captures correct answers when the retrieved context was not clinically useful. The final column reports the percentage
of questions that were incorrect in zero-shot prompting but answered correctly using the agentic framework.
Percentages and raw counts are shown per model.

Model name

Ministral-8B

Context relevant

46% (48/104)

Hallucination (relevant
context, incorrect response)

14% (15/104)

Correct despite
irrelevant context

35% (36/104)

Zero-shot incorrect
— agentic correct

26% (27/104)

Mistral Large

46% (48/104)

6% (6/104)

40% (42/104)

12% (13/104)

Llama3.3-8B

46% (48/104)

17% (18/104)

37% (38/104)

12% (13/104)

Llama3.3-70B

46% (48/104)

6% (6/104)

42% (44/104)

11% (11/104)

Llama3-Med42-8B

46% (48/104)

11% (11/104)

39% (41/104)

16% (17/104)

Llama3-Med42-70B

46% (48/104)

7% (7/104)

39% (41/104)

12% (13/104)

Llama4 Scout 16E

46% (48/104)

5% (5/104)

39% (41/104)

9% (9/104)

DeepSeek R1-70B

46% (48/104)

5% (5/104)

38% (40/104)

8% (8/104)

DeepSeek R1

46% (48/104)

3% (3/104)

37% (38/104)

6% (6/104)

DeepSeek-V3

46% (48/104)

4% (4/104)

43% (45/104)

12% (13/104)

Qwen 2.5-0.5B

46% (48/104)

26% (27/104)

21% (22/104)

21% (22/104)

Qwen 2.5-3B

46% (48/104)

13% (14/104)

33% (34/104)

21% (22/104)

Qwen 2.5-7B

46% (48/104)

12% (12/104)

37% (38/104)

23% (24/104)

Qwen 2.5-14B

46% (48/104)

10% (10/104)

36% (37/104)

15% (16/104)

Qwen 2.5-70B

46% (48/104)

5% (5/104)

37% (38/104)

12% (13/104)

Qwen 3-8B

46% (48/104)

6% (6/104)

36% (37/104)

17% (18/104)

Qwen 3-235B

46% (48/104)

5% (5/104)

41% (43/104)

6% (6/104)

GPT-3.5-turbo

46% (48/104)

13% (14/104)

36% (37/104)

21% (22/104)

GPT-4-turbo

46% (48/104)

9% (9/104)

39% (41/104)

8% (8/104)

03

46% (48/104)

2% (2/104)

43% (45/104)

3% (3/104)

MedGemma-4B-it

46% (48/104)

17% (18/104)

38% (39/104)

20% (21/104)

MedGemma-27B-text-it

46% (48/104)

3% (3/104)

38% (39/104)

15% (16/104)

Gemma-3-4B-it

46% (48/104)

20% (21/104)

36% (37/104)

25% (26/104)

Gemma-3-27B-it

Average

46% (48/104)

7% (7/104)

9.4% + 5.9

12

37% (38/104)

37.4% + 4.2

20% (21/104)

14.6% + 6.4


To further probe the relationship between model scale and accuracy, we conducted a
targeted scaling experiment using the Qwen 2.5 model family, which spans a wide range of sizes
(Qwen 2.5-70B, 14B, 7B, 3B, and 0.5B) while maintaining consistent architecture and training
procedures. This allowed us to isolate the influence of model size from confounding variables
such as instruction tuning or pretraining corpus. We computed Pearson correlation coefficients
between model size and diagnostic accuracy for each inference strategy. All three methods
including zero-shot (r = 0.68), traditional RAG (r = 0.81), and agentic RAG (r = 0.61) showed
strong positive correlations with parameter count, reflecting the general performance advantage
of larger models. However, as detailed in earlier findings, the relative benefit of retrieval strategies
was not uniformly distributed: conventional RAG was most beneficial for small models, while
agentic reasoning consistently enhanced performance in mid-sized models (see Figure 4). These
findings highlight the importance of aligning retrieval strategies with model capacity and
deployment constraints.

2.6. Agentic retrieval enhances performance in fine-tuned clinical
models

To examine whether domain-specific fine-tuning diminishes the utility of retrieval-based
strategies, we evaluated four clinically optimized language models: MedGemma-27B-text-it,
MedGemma-4B-it, Llama3-Med42-70B, and Llama3-Med42-8B. These models are specifically
fine-tuned for biomedical or radiological applications, making them suitable test cases for
understanding the complementary role of agentic retrieval and reasoning. Despite already
possessing clinical specialization, all four models exhibited improved diagnostic QA performance
under the agentic framework. On average, accuracy increased from 67% + 6 under zero-shot
prompting to 75% + 6 with agentic RAG (P = 0.001). Traditional online RAG, in contrast, did not
show a significant improvement over zero-shot prompting (67% + 9 vs. 67% + 6, P = 0.704).
Notably, agentic RAG also significantly outperformed traditional online RAG (P = 0.034),
suggesting that structured multi-step reasoning contributes meaningfully even when baseline
knowledge is embedded through fine-tuning. Each model in this group followed a similar pattern.
For instance, MedGemma-27B-text-it improved from 71% + 4 [95% Cl: 62, 79] to 81% + 4 [95%
Cl: 73, 88] with agentic inference, MedGemma-4B-it from 56% + 5 [95% Cl: 46, 65] to 66% + 5
[95% Cl: 57, 75], Llama3-Med42-70B from 72% + 4 [95% Cl: 63, 80] to 79% + 4 [95% Cl: 71, 87],
and Llama3-Med42-8B from 67% + 5 [95% Cl: 58, 77] to 75% + 4 [95% Cl: 66, 84] (see Figure
4). While these individual gains were not statistically significant on their own, the collective
improvement supports the hypothesis that retrieval-augmented reasoning provides additive
benefits beyond those conferred by fine-tuning alone.

These results suggest that domain-specific fine-tuning and agentic reasoning serve
complementary roles: fine-tuning equips the model with foundational knowledge, while the agentic
framework enhances factual grounding, synthesis, and contextual accuracy through structured
information-seeking behavior.

13


a Small models b Large models

100 1007
90 * oot | T § : .
— 70 . : _ 7+. i t .
30 1 30
20 Qwen Uama Qwen Qwen Ministral Gemma Qwen 20 03 DeepSeek GPT DeepSeek
3-8B 3.3-8B 25-78 25-38 -8B -3-4B-it 25-0.5B -RI -4-turbo -v3
c Mid-sized models d Qwen models (size effect)
100 80
90; + 7 .
i i 70
5 |? i 7 7
& 60+ } . 8
507 . .
40+
40
307
0~ 30 053 7 14 70
DeepSeek Mistral Qwen Gemma- GPT
-RI-70B Large 2.5-70B 3-27B-it -3.5-turbo Model size (billions of parameters)

e Fine-tuned models f Response time

a -" ae

Accuracy [%]
2
g

Figure 4. Comparative accuracy distributions and inference-time multipliers for zero-shot versus
agentic strategies across model groups. Accuracy results are shown for (a) small-scale models
(Ministral-8B, Gemma-3-4B-it, Qwen 2.5-7B, Qwen 2.5-3B, Qwen 2.5-0.5B, Qwen 3-8B, Llama 3-8B), (b)
large models (DeepSeek-R1, DeepSeek-V3, 03, Qwen3-235B, GPT-4-turbo), (c) mid-sized models
(Mid-Sized Models: GPT-3.5-turbo, Llama 3.3-70B, MistralLarge, Qwen2.5-70B, Llama 4 Scout 16E,
Gemma-3-27B-it, DeepSeek-R1-70B), (d) across Qwen2.5 family for different parameter sizes:
Qwen 2.5-70B, 14B, 7B, 3B and 0.5B, and (e) medically fine-tuned models (MedGemma 27B-text-it,
MedGemma 4B-it, Llama3-Med42-70B, Llama3-Med42-8B). (f) Distribution of agentic-to-zero-shot runtime
multipliers (x slower/faster) across all models. Boxplots display accuracy (%) distributions (n = 1 000) for
zero-shot (orange) and agentic (blue): boxes span Q1—Q3, central line is the median (Q2), whiskers extend
to 1.5xIQR and dots mark outliers. Line chart shows mean accuracy versus model size for zero-shot
(green), online RAG (orange) and agentic (purple) across Qwen 2.5 family.

14


2./. Agentic retrieval increases response time but remains
computationally tractable

To evaluate the computational impact of agentic reasoning, we measured and compared per-
question response times between zero-shot prompting and agentic RAG across all models using
the RadioRAG benchmark. As shown in Table 4, agentic retrieval introduced a substantial latency
overhead across all model groups, with the average response time increasing from 54 + 28
seconds under zero-shot prompting to 324 + 270 seconds under agentic inference, equivalent to
a 6.71 increase.

As shown in Figure 4, this increase varied considerably by model group. Small-scale
models (7-8B parameters), including Qwen 2.5-7B, Qwen3-8B, Llama3-Med42-8B, Llama3-
Med42-8B, and Ministral-8B, showed a 6.04 average increase, with individual models ranging
from modest (2.06* for Qwen3-8B) to substantial (35.98* for Qwen 2.5-7B). Mini models (3-4B
parameters), such as Gemma-3-4B-it, MedGemma-4B-it, and Qwen 2.5-3B, exhibited the highest
relative increase, averaging 11.10x, with Qwen2.5-3B peaking at 18.59x. In contrast, mid-sized
models (~70B parameters), including DeepSeek-R1-70B, Llama-3.3-70B, Qwen 2.5-70B, and
Llama3-Med42-70B, had a more moderate increase of 2.93x. This reflects a balance between
computational capacity and the overhead introduced by iterative reasoning. For example,
DeepSeek-R1-70B showed only a 1.87 increase. The large-model group (120—250B), including
Qwen 3-235B, Mistral Large, and Llama4 Scout 16E, had the largest absolute latency, with a
group average increase of 13.27x. Qwen3-235B showed the most pronounced jump, from 97
seconds to 1703 seconds per question. Despite high computational costs, these models showed
only minimal diagnostic improvement with agentic reasoning, emphasizing a potential efficiency—
performance trade-off. Notably, the DeepSeek mixture of experts*? (MoE) group (DeepSeek-R1
and DeepSeek-V3) exhibited relatively efficient scaling under agentic reasoning, with an average
increase of 4.19x, suggesting that sparsely activated architectures may offer runtime advantages
in multi-step retrieval tasks. Similarly, the Gemma-27B group (Gemma-3-27B-it and MedGemma-
27B-text-it) demonstrated a low variance and consistent response time increase of 2.82x,
indicating reliable timing behavior under agentic workflows.

Despite these increases, the absolute response times remained within feasible limits for
many clinical applications. Furthermore, because evaluations were conducted under identical
system conditions, the relative timing metrics provide a robust measure of computational scaling.
These findings suggest that while the agentic RAG introduces additional latency, its time cost may
be acceptable, especially in mid-sized and sparse-activation models depending on deployment
requirements and accuracy demands.

15


Table 4: Response time comparison between zero-shot and the agentic method across model groups. Average
per-question response times (n = 104) are reported in seconds as mean + standard deviation for both individual models
and aggregated model groups. A fixed overhead of 10,554.64 seconds per model, corresponding to context generation,
was evenly distributed across all questions, contributing approximately 101.5 seconds per question. For time analysis,
models were grouped based on parameter scale and architectural characteristics into six categories: the DeepSeek
mixture of experts (MoE) group, the large model group (120—250B), the medium-scale group (~70B), the Gemma-27B
group, the small model group (7—8B), and the mini model group (3-4B). “Absolute difference” denotes the increase in
average response time per question introduced by the agentic method, and “Relative increase” refers to the ratio of
mean agentic time to mean zero-shot time per group. Final statistics are computed at the group level.

Time

Model / group name
Zero-shot (s) Agentic (s) Absolute difference (s) Relative increase (times)

DeepSeek-V3 group 98.55 + 53.58 412.7 + 156.7 314.2 + 141.6

Large (120-250B) group _|63.7 + 29.4 845.1 + 744.7 781.4 + 715.2

Llama4 Scout 16E 49.6 + 24.6 462.3 + 190.2 412.6 + 169.7

Mistral Large 43.9 + 23.9 369.7 + 142.0 325.8 + 126.0

Qwen 3-235B 97.5 + 54.6 1703.3 + 787.6 1605.8 + 744.0

Medium (= 70B) group 78.7 + 51.4 230.58 + 44.8 151.8 + 34.3

DeepSeek R1-70B 151.3 + 83.4 282.8 + 95.0 131.3 + 68.3

Llama3-Med42-70B 42.2 + 22.4 177.0 + 39.5 134.8 + 27.9

Llama3.3-70B 78.5 + 43.6 216.7 + 60.7 138.2 + 34.7

Qwen 2.5-70B 42.6 + 22.2 245.7 + 76.8 203.1 + 58.5

Gemma 27B group 75.8 + 38.2 214.1 + 54.9 138.3 + 16.7

Gemma-3-27B-it 48.8 + 28.6 175.3 + 37.4 126.5 + 26.2

MedGemma-27B-text-it 102.8 + 56.1 253.0 + 75.2 150.1 + 38.4

Small (7 - 8B) group 22.0 + 39.9 132.9 + 33.9 110.9 + 9.3

Llama3-Med42-8B 1440.7 108.0 + 3.7 106.6 + 3.3

Llama3.3-8B 8444.0 116.3 47.6 107.9446

Ministral-8B 3.742.2 124.9 + 11.8 121.24 10.4

Qwen 2.5-70B 3.4416 122.84 11.4 119.4 + 10.4

Qwen 3-8B 93.2 + 53.4 192.3 + 49.8 99.1 + 33.9

Mini (3 —- 4B) group 11.44 5.4 126.3 + 6.3 114.9 + 8.4

Gemma-3-4B-it 17.5479 127.7 + 13.1 110.2 + 7.0

MedGemma-4B-it 9.6 + 5.4 119.4499 109.8 + 9.1

Qwen 2.5-3B 7.143.7 131.7 + 13.7 124.6 + 11.0

Average 53.7 + 28.4 324.4 + 270.2 271.24 257.3

16


2.8. Agentic retrieval provides human-interpretable context and
enables performance gains for expert radiologists

To better understand the source of diagnostic improvements conferred by the agentic framework,
we conducted an additional experiment involving a board-certified radiologist (TTN) with seven
years of experience in diagnostic and interventional radiology. As in previous evaluations, the
expert first answered all 104 RadioRAG questions unaided, i.e., without access to external
references or retrieval assistance, achieving an accuracy of 51% + 5 [95% Cl: 41, 62] (53/104).
This baseline performance was significantly lower than that of 16 out of 24 evaluated LLMs in
their zero-shot mode (P s 0.017 for all), and not significantly different from 7 models, including
GPT-3.5-turbo, Llama3.3-8B, Qwen 2.5-7B, Ministral-8B, MedGemma-4B-it, Gemma-3-4B-it, and
Qwen 2.5-3B. Only Qwen 2.5-0.5B, the smallest model tested, performed significantly inferior to
the radiologist (37% + 5 [95% Cl: 27, 46]; P = 0.044).

To isolate the contribution of retrieval independent of generative reasoning, we repeated
the experiment with the same radiologist using the contextual reports retrieved by the agentic
system, that is, the same Radiopaedia content supplied to the LLMs. With access to this
structured evidence, the radiologist’s accuracy increased to 68% + 5 [95% Cl: 60, 77] (71/104), a
significant improvement over the unaided baseline (P = 0.010). This finding demonstrates that the
agentic system successfully retrieves clinically meaningful and decision-relevant information,
which can support human diagnostic accuracy even in the absence of language model synthesis.

When comparing the radiologist’s context-assisted performance to that of the LLMs, only
1 out of 24 models significantly outperformed the radiologist under zero-shot conditions (03;
P=0.018). In contrast, when compared to LLM performance under the full agentic framework,
only 2 models, i.e., DeepSeek-V3 (P = 0.016) and 03 (P = 0.012) achieved statistically significant
improvements over the context-assisted radiologist.

3. Discussion

In this study, we introduced an agentic RAG framework designed to enhance the performance,
factual grounding, and clinical reliability of LLMs in radiology QA tasks. To the best of our
knowledge, this is the first application of an agentic retrieval method in radiology, and our large-
scale evaluation across 24 diverse LLMs, including different architectures, parameter scales,
training paradigms, and clinical fine-tuning, represents one of the most comprehensive
comparative analysis of its kind to date**. Our findings indicate that agentic retrieval can improve
diagnostic accuracy relative to conventional zero-shot prompting and _ traditional RAG
approaches, especially in small- to mid-sized models, while also reducing hallucinated outputs.
However, the benefits of agentic retrieval were not uniformly observed across all models or
scenarios, underscoring the need for careful consideration of model scale and characteristics
when deploying retrieval-based systems.

17


A central finding of this study is that the effectiveness of retrieval strategies strongly
depends on model scale. While traditional single-step online RAG***?', and generally non-
agentic RAG"®'74546 approaches have previously been shown to primarily benefit smaller models
(<8 billion parameters) with diminishing returns at larger scales'®'®2', our agentic framework
expanded performance improvements into the mid-sized model range (approximately 17—150
billion parameters). Mid-sized models such as GPT-3.5-turbo, Mistral Large, and Llama3.3-70B
have sufficient reasoning capabilities to follow structured logic but frequently struggle to
independently identify and incorporate relevant external clinical evidence. By decomposing
complex clinical questions into structured subtasks and iteratively retrieving targeted evidence,
the agentic approach consistently improved accuracy across these mid-sized models, gains that
conventional RAG did not achieve in this important segment. Similarly, smaller models also
benefited from structured retrieval, overcoming some limitations associated with fewer
parameters and less comprehensive pretraining. However, the magnitude of improvements varied
between individual small-scale models, likely reflecting differences in architectural design,
instruction tuning, and pretraining data. These results suggest that while agentic retrieval can
broadly enhance performance across smaller and mid-sized models, model-specific optimizations
may be required to fully capitalize on its potential.

In contrast, the largest evaluated models (more than 200 billion parameters), such as
GPT-4-turbo, 03, DeepSeek-R1, and Qwen 3-235B exhibited minimal to no gains from either
conventional or agentic retrieval methods. These models achieved high performance with zero-
shot inference alone, suggesting that their extensive pretraining on large-scale and potentially
clinically relevant data already equipped them with substantial internal reasoning capabilities and
domain-specific knowledge. While retrieval augmentation offered limited incremental accuracy
benefits at this scale, it may still provide value in clinical practice by enhancing transparency,
auditability, and alignment with established documentation standards. Future studies should
explore whether agentic retrieval can improve interpretability and traceability of decisions made
by these high-capacity models, even when accuracy alone does not increase significantly.

To further examine the relationship between model scale and retrieval benefit, we
conducted a controlled scaling analysis using the Qwen 2.5 model family. This approach, which
held architecture and training constant, revealed a strong positive relationship between model
size and diagnostic accuracy across all tested inference strategies*”*®. Nevertheless, the optimal
retrieval approach varied: traditional single-step RAG offered the greatest advantage for smaller
models, whereas agentic retrieval consistently enhanced mid-sized model performance. These
results highlight the importance of aligning retrieval strategies with the intrinsic reasoning capacity
of individual models, emphasizing tailored rather than universal implementation of retrieval
augmentation.

A key consideration in clinical applications is whether domain-specific fine-tuning reduces
the necessity or utility of external retrieval. Clinically specialized LLMs, such as variants of
MedGemma and Llama3-Med42, are increasingly available and presumed to contain embedded
medical knowledge sufficient for diagnostic tasks®. However, our results show that even these
fine-tuned models consistently benefited from agentic retrieval, whereas traditional single-step
RAG provided little additional improvement. The additive benefit of agentic retrieval in fine-tuned
models suggests that structured, case-specific external evidence complements internal

18


knowledge by providing context-sensitive, up-to-date information. Thus, agentic retrieval and
clinical fine-tuning should be viewed as complementary strategies, jointly optimizing model
performance and reliability rather than being seen as mutually exclusive.

Beyond accuracy, our analysis demonstrated that agentic retrieval improved factual
grounding®"* and reduced hallucinations in model outputs. By systematically associating
diagnostic responses with specific retrieved content from Radiopaedia.org'®, the framework
promoted evidence-based reasoning, which is critical in safety-sensitive applications like
radiology. Although clinically relevant evidence was retrieved in less than half of the evaluated
cases, most models successfully leveraged this content to produce factually correct responses
when it was available. Larger and clinically tuned models demonstrated robustness by correctly
responding even when retrieved evidence was irrelevant or insufficient, likely relying on internal
knowledge’®. However, such internally derived answers, while accurate, lack explicit grounding in
external sources, raising potential concerns for interpretability and clinical accountability*’.
Smaller models were less resilient when retrieval failed, highlighting their greater reliance on
structured external support. Consequently, ensuring high-quality retrieval remains paramount,
especially for deployment scenarios where transparency and traceability of decisions are
required.

The increased diagnostic reliability introduced by agentic retrieval came at a
computational cost. Response times significantly increased compared to zero-shot inference due
to iterative query refinement, structured evidence gathering, and multi-agent coordination. This
latency varied substantially by model size and architecture, with smaller models experiencing the
largest relative increases, and mid-sized or sparsely activated architectures demonstrating
comparatively moderate overhead. Very large models, although capable of achieving high
accuracy without retrieval, experienced substantial absolute latency increases without
commensurate accuracy gains. Future work should therefore explore optimization strategies to
manage computational overhead, such as selective retrieval triggering, parallel evidence
pipelines, or methods to distill agentic reasoning into more efficient inference paths.

Furthermore, agentic retrieval demonstrated value as a decision-support tool for human
experts. Providing a board-certified radiologist with the same retrieved context as the agentic
system substantially improved their diagnostic accuracy compared to unaided performance. This
finding illustrates that the agentic retrieval process successfully identified and presented clinically
meaningful, decision-relevant evidence that directly supported expert reasoning. The limited
number of LLMs significantly outperforming the context-assisted radiologist further underscores
the complementary strengths of human expertise and agentically retrieved information. Thus,
agentic retrieval may serve dual purposes in clinical environments, simultaneously enhancing
LLM performance and providing interpretable, actionable evidence to clinicians.

Our study has several important limitations. First, our evaluation relied exclusively on
Radiopaedia.org, a trusted but singular radiology knowledge source. Dependence on a single
data provider can restrict retrieval coverage and may not represent the full breadth of available
radiological information. Incorporating multiple authoritative sources, structured knowledge
bases, or clinical ontologies could improve the generalizability and relevance of retrieved content.
Second, although rigorous, our benchmark dataset comprised a relatively limited number of

19


expert-curated radiology questions (n=104). While these questions spanned diverse
subspecialties and were carefully constructed, larger and more varied datasets encompassing
broader clinical scenarios, different imaging modalities, and increased diagnostic complexity are
required to fully assess the robustness and effectiveness of agentic retrieval frameworks. Third,
the agentic retrieval process incurs significant computational overhead, substantially increasing
response times compared to conventional zero-shot prompting and traditional single-step RAG.
Although response durations remained within feasible limits for non-emergent clinical use cases,
the practicality of the proposed method in time-sensitive settings (e.g., acute diagnostic
workflows) remains uncertain. Future research should explore optimization techniques, such as
parallelization or selective agent activation, to mitigate latency without sacrificing diagnostic
accuracy or reasoning quality. Fourth, our evaluation relied on retrospective, static radiology QA
items drawn from the recently published RSNA-RadioQA and ExtendedQA datasets. These
datasets were curated and released shortly before our study, lowering the likelihood that included
items appeared in model pretraining. However, despite their clinical rigor, such benchmark-style
questions do not fully capture the complexity of real-world radiology workflows, which often
involve evolving diagnostic contexts, iterative communication, and integration of multimodal
imaging data. Our results therefore reflect performance in a controlled QA setting rather than in
live clinical environments. Future work should evaluate agentic retrieval under prospective,
clinically embedded conditions, such as within reporting workflows or decision support systems
to better understand its utility in practice. Fifth, despite evaluating a broad range of LLM
architectures, parameter scales, and training paradigms, we observed substantial variability in the
diagnostic gains attributable to agentic retrieval across individual models. This likely reflects a
combination of factors, including architectural differences, instruction tuning approaches, and
pretraining data composition, as well as implementation-specific elements such as prompt design
and agent orchestration. Because the agentic pipeline relies on structured prompting and task
decomposition, its performance may be sensitive to changes in phrasing, retrieval heuristics, or
agent coordination. Future work should systematically investigate both model-level and
implementation-level sources of variability to develop more robust, generalizable retrieval
strategies tailored to different model configurations.

This study presents a proof-of-concept for an agentic retrieval framework capable of
enhancing diagnostic accuracy, factual reliability, and clinical interpretability of LLMs in radiology
QA tasks. Our extensive, large-scale analysis of 24 diverse models highlights the complex
relationships between retrieval strategy, model scale, and clinical fine-tuning. While agentic
retrieval shows clear promise, particularly for mid-sized and clinically optimized models, future
research is essential to refine retrieval mechanisms, mitigate computational overhead, and
validate these systems across broader clinical contexts. As generative Al continues to integrate
into medical practice, frameworks emphasizing transparency, evidence-based reasoning, and
human-aligned interpretability, such as the agentic approach introduced here, will become
increasingly critical for trustworthy and effective clinical decision support.

20


4. Materials and Methods

4.1. Ethics statement

The methods were performed in accordance with relevant guidelines and regulations. The data
utilized in this research was sourced from previously published studies. As the study did not
involve human subjects or patients, it was exempt from institutional review board approval and
did not require informed consent.

4.2. Dataset

This study utilized a combination of two carefully curated datasets specifically designed to
evaluate the performance of agentic LLMs in retrieval-augmented radiology QA.

We utilized two previously published datasets from the RadioRAG study"®: the RSNA-
RadioQA" and ExtendedQA"* datasets. The RSNA-RadioQA dataset consists of 80 radiology
questions derived from peer-reviewed cases available in the Radiological Society of North
America (RSNA) Case Collection. This dataset covers 18 radiologic subspecialties, including
breast imaging, chest radiology, gastrointestinal imaging, musculoskeletal imaging,
neuroradiology, and pediatric radiology, among others. Each subspecialty contains at least five
questions, carefully crafted from clinical histories and imaging descriptions provided in the original
RSNA case documentation. Differential diagnoses explicitly listed by original case authors were
excluded to avoid biasing model responses. Images were intentionally excluded. Detailed
characteristics, including patient demographics and subspecialty distributions, have been
previously published and are publicly accessible. The ExtendedQA dataset consists of 24 unique,
radiology-specific questions initially developed and validated by board-certified radiologists with
substantial diagnostic radiology experience (5—14 years). These questions reflect realistic clinical
diagnostic scenarios not previously available online or included in Known LLM training datasets.
The final RadioRAG dataset used in this study subsequently contains 104 questions combining
both RSNA-RadioQA and ExtendedQA.

4.2.1. Dataset postprocessing

To ensure consistent evaluation across all models and inference strategies, we applied structured
preprocessing to the original RadioRAG dataset, particularly the ExtendedQA portion (n=24),
which was initially formatted as open-ended questions.

All questions from the RSNA-RadioQA dataset (n=80) were left unchanged. However, for
the ExtendedQA subset, each question was first converted into a multiple-choice format while
preserving the original stem and correct answer. To standardize the evaluation across both
RSNA-RadioQA and ExtendedQA, we then generated three high-quality distractor options for

21


every question in the dataset (n = 104), resulting in a total of four answer choices per item.
Distractors were generated using OpenAl’s GPT-40 and 03 models, selected for their ability to
produce clinically plausible and contextually challenging alternatives. Prompts were designed to
elicit difficult distractors, including common misconceptions, closely related entities, or synonyms
of the correct answer. This ensured that diagnostic complexity was maintained across all
questions. A representative prompt used for distractor generation was:

“I have a dataset of radiology questions that are currently open-ended, each with a correct
answer provided. | want to transform these into multiple-choice questions (MCQs) by
generating four answer options per question (one correct answer + three distractors). The
distractors should be plausible and the level of difficulty must be high. If possible, include
distractors that are synonyms, closely related concepts, or common misconceptions
related to the correct answer.”

Supplementary Table 1 summarizes the characteristics of the datasets used in this study.
The original RSNA-RadioQA questions are publicly available through their original publication’®.

4.3. Experimental Design

4.3.1. System architecture

The experimental design centers on an agentic retrieval and reasoning framework adapted from
LangChain’s Open Deep Research pipeline, specifically tailored for radiology QA tasks. As
illustrated in Figure 1, the pipeline employs a structured, multi-agent workflow designed to
produce comprehensive, evidence-based diagnostic reports for each multiple-choice question.
The reasoning and content-generation process is powered by OpenAl’s GPT-40-mini model,
selected for its proficiency in complex reasoning tasks, robust instruction-following, and effective
tool utilization. The architecture consists of two specialized agents: (i) a supervisor agent and (ii)
a research agent, coordinated through a stateful directed graph framework. State management
within this directed graph framework ensures that all steps in the workflow remain consistent and
coordinated. The system maintains a shared memory state, recording the research plan, retrieved
evidence, completed drafts, and all agent interactions, enabling structured progression from
planning through final synthesis.

4.3.2. Agentic preprocessing

To enable structured, multi-step reasoning in the agentic retrieval framework, we implemented a
preprocessing step focused on diagnostic abstraction. For each question in the RadioRAG
dataset, we used the Mistral Large model to generate a concise, comma-separated summary of
key clinical concepts. This step was designed to extract the essential diagnostic elements of each
question while filtering out rhetorical structure, instructional phrasing (e.g., “What is the most likely

22


diagnosis?”), and other non-clinical language. These keyword summaries served exclusively as
internal inputs to guide the agentic system’s retrieval process and were not shown to the LLMs
as part of the actual question content. The intent was to ensure retrieval was driven by the clinical
essence of the question rather than superficial linguistic cues. The prompt used for keyword
extraction was:

“Extract and summarize the key Clinical details from the following radiology question.
Provide a concise, comma-separated summary of keywords and key phrases in one
sentence only.

Question: {question_text}.

Summary.”

4.3.3. Agent roles and responsibilities

The workflow is coordinated primarily by two agents, each with distinct responsibilities: (i)
supervisor agent and (ii) research agent. The supervisor acts as the central orchestrator of the
pipeline. Upon receiving a question, the supervisor reviews the diagnostic keywords and multiple-
choice options, then formulates a structured research plan dividing the task into clearly defined
sections, one for each diagnostic option. This agent assigns tasks to individual research agents,
each responsible for exploring a single diagnostic choice. Throughout the process, the supervisor
ensures strict neutrality, focusing solely on evidence gathering rather than advocating for any
particular option. After research agents complete their tasks, the supervisor synthesizes their
outputs into a final report, utilizing specialized tools to generate an objective introduction and
conclusion.

Each research agent independently conducts an in-depth analysis focused on one
diagnostic option. Beginning with a clear directive from the supervisor, the research agent
employs a structured retrieval strategy to obtain relevant evidence. This involves an initial focused
query using only essential terms from the diagnostic option, followed by contextual queries
combining these terms with clinical features from the question stem (e.g., imaging findings or
patient demographics). If retrieval results are inadequate, the agent adaptively refines queries by
simplifying terms or substituting synonyms. In cases where sufficient evidence is not available
after four attempts, the agent explicitly documents this limitation. All retrieval tasks utilize
Radiopaedia.org exclusively, ensuring clinical accuracy and reliability. After completing retrieval,
the research agent synthesizes findings into a structured report segment, explicitly highlighting
both supporting and contradicting evidence. Each segment includes clearly formatted citations
linking directly to source materials, ensuring transparency and verifiability.

4.3.4. Retrieval and writing tools

To facilitate structured retrieval and writing processes, the pipeline utilizes a suite of specialized
computational tools dynamically selected based on specific task requirements: (i) search tool, (ii)
report structuring tools, and (iii) content generation tool. In the following, details of each tool is
explained.

23


The retrieval mechanism is powered by a custom-built search tool leveraging a locally
hosted instance of SearXNG, a privacy-oriented meta-search engine deployed within a
containerized Docker environment. This setup ensures consistent and reproducible search
results. To maintain quality and clinical reliability, the search tool restricts results exclusively to
content from Radiopaedia.org through a two-layer filtering process: first by appending a
“site:radiopaedia.org” clause to all queries, and subsequently by performing an explicit domain
check on all retrieved results. Raw results are deduplicated and formatted into markdown bundles
suitable for seamless integration into subsequent reasoning steps.

The supervisor agent employs specific tools to structure the diagnostic report
systematically. An initial Sections tool is used to outline the report into distinct diagnostic sections,
aligning precisely with the multiple-choice options. Additional specialized tools generate
standardized Introduction and Conclusion sections: the Introduction tool summarizes essential
Clinical details from the question, and the Conclusion tool objectively synthesizes findings from all
diagnostic sections, emphasizing comparative diagnostic considerations without bias.

The research agent utilizes a dedicated Section writing tool to construct standardized
report segments. Each segment begins with a concise synthesis of retrieved evidence, followed
by interpretive summaries clearly identifying points supporting and contradicting each diagnostic
choice. Citations are integrated inline, referencing specific Radiopaedia'® URLs for traceability.

4.3.5. Report assembly and persistence

Upon completion of individual research segments, the supervisor agent compiles the final
diagnostic report, verifying the completeness and quality of all sections. The resulting structured
report, including introduction, detailed analysis of diagnostic options, and conclusion, is then
immediately persisted in a robust manner. Reports are streamed incrementally into newline-
delimited JSON (NDJSON) format, preventing data loss in case of interruptions. This storage
method supports efficient resumption by checking previously completed entries, thus avoiding
redundant processing. After processing all questions within a given batch, individual NDJSON
entries are consolidated into a single comprehensive JSON file, facilitating downstream analysis
and evaluation.

4.4. Baseline comparison systems

Each model was evaluated under three configurations: (i) zero-shot prompting (conventional QA),
(ii) traditional online RAG", and (iii) our proposed agentic retrieval framework.

4.4.1. Baseline 1: Zero-shot prompting pipeline

In the zero-shot prompting baseline, models received no external retrieval assistance or context.
Instead, each model was presented solely with the multiple-choice questions from the RadioRAG

24


dataset (question stem and four diagnostic options) and prompted to select the correct answer
based entirely on their pre-trained knowledge. Models generated their responses autonomously
without iterative feedback, reasoning prompts, or additional information.

The exact standardized prompt used for this configuration is provided below:

“You are a highly knowledgeable medical expert. Below is a multiple-choice radiology
question. Read the question carefully. Provide the correct answer by selecting the most
appropriate option from A, B, C, or D.

Question:

{question}

Options:
foptions}”

4.4.2. Baseline 2: Traditional online RAG pipeline

The traditional online RAG baseline was implemented following a state-of-the-art non-agentic
retrieval framework previously developed for radiology question answering by Arasteh et al’®. The
system employs GPT-3.5-turbo to automatically extract up to five representative radiology
keywords from each question, optimized experimentally to balance retrieval quality and efficiency.
These keywords were used to retrieve relevant articles from Radiopaedia.org, with each article
segmented into overlapping chunks of 1,000 tokens. Chunks were then converted into vector
embeddings (OpenAl's text-embedding-ada-002) and stored in a temporary vector database.
Subsequently, the embedded original question was compared against this database to retrieve
the top three matching text chunks based on cosine similarity. These retrieved chunks served as
external context provided to each LLM alongside the original multiple-choice question. Models
were then instructed to answer concisely based solely on this context, explicitly stating if the
answer was unknown.

The exact standardized prompt used for this configuration is provided below:

“You are a highly knowledgeable medical expert. Below is a multiple-choice radiology
question accompanied by relevant context (report). First, read the report, and then the
question carefully. Use the retrieved context to answer the question by selecting the most
appropriate option from A, B, C, or D. Otherwise, if you don't know the answer, just say
that you don't know.

Report:

{report}

Question:
{question}

Options:
foptions}”

25


4.5. Evaluation

SW, JS, TTN, and STA performed model evaluations. We assessed both small and large-scale
LLMs using responses generated between July 1-27, 2025. For each of the 104 questions in the
RadioRAG benchmark dataset, models were integrated into a unified evaluation pipeline to
ensure consistent testing conditions across all settings. The evaluation included 24 LLMs:
Ministral-8B, Mistral Large, Llama3.3-8B°"%°, Llama3.3-70B%’**, | Llama3-Med42-8B*,
Llama3-Med42-70B*, Llama4 Scout 16E%, DeepSeek R1-70B*%, DeepSeek-R1°%,
DeepSeek-V3*°, Qwen 2.5-0.5B**, Qwen 2.5-3B*, Qwen 2.5-7B*, Qwen 2.5-14B°%, Qwen
2.5-70B*?, Qwen 3-8B*°, Qwen 3-235B*°, GPT-3.5-turbo, GPT-4-turbo®, 03, MedGemma-4B-it™,
MedGemma-27B-text-it**, Gemma-3-4B-it*"4*, and Gemma-3-27B-it*!**. These models span a
broad range of parameter scales (from 0.5B to over 670B), training paradigms (instruction-tuned,
reasoning-optimized, clinically aligned, and general-purpose), and access models (open-source,
open-weights, or proprietary). They also reflect architectural diversity, including dense
transformers and MoE*’ systems. Full model specifications, including size, category, accessibility,
knowledge cutoff date, context length, and developer are provided in Table 1.

4.5.1. Accuracy assessment

Accuracy was determined by comparing each LLM's response to the correct option. We used
Mistral Large as an automated adjudicator for this process. For each multiple-choice question,
both the LLM's response and the correct answer (including its corresponding letter and option)
were provided to Mistral Large via a standardized prompt. Mistral Large was instructed to respond
"Yes" if the correct answer was present in the model's response, either explicitly or as a clear
component of the explanation, even if the phrasing differed. Otherwise, it was instructed to
respond "No." A "Yes" was scored as 1 (correct), and a"No" was scored as 0 (incorrect), ensuring
a consistent and unbiased measure of diagnostic accuracy.

The exact standardized prompt used for this configuration is provided below:

“You are a highly knowledgeable medical expert. Determine whether the Correct Answer
appears within the LLMs response, fully or as a clear part of the explanation, even if the
wording differs. Respond with ‘Yes’ if the Correct Answer can be found in the LLMs
response; otherwise respond with ‘No’.

LLMs response:
{llms_response}

Correct Answer:
{correct_answer}”

26


4.5.2. Factuality assessment

To evaluate the factual reliability of model outputs under the agentic retrieval framework, we
conducted a targeted hallucination analysis across all 104 questions in the RadioRAG
benchmark’®. This analysis aimed to differentiate model errors due to flawed reasoning from those
caused by insufficient or irrelevant evidence, and to assess the extent to which final answers were
grounded in the retrieved context.

Each agentic response was reviewed by a board-certified radiologist (TTN) with seven
years of experience in diagnostic and interventional radiology. For every question, the following
three criteria were assessed: (i) whether the retrieved Radiopaedia context was clinically relevant
to the question, (ii) whether the model's final answer was consistent with that context, and (iii)
whether the final answer was factually correct.

Context was classified as clinically relevant only if it contained no incorrect or off-topic
content with respect to the diagnostic question. This strict definition ensured that relevance was
not based on superficial keyword overlap but on the actual clinical utility of the content. Retrievals
were deemed relevant only when the retrieved material included appropriate imaging findings,
clinical clues, or differential diagnoses applicable to the question stem.

Hallucinations were defined as cases in which the model produced an incorrect answer
despite being provided with clinically relevant context. These represent failures of reasoning or
synthesis rather than of retrieval. Given the high-stakes nature of radiologic diagnosis, identifying
such errors is essential for understanding model reliability and safety.

We also documented instances where models answered questions correctly despite being
supplied with irrelevant or unhelpful context. These “correct despite irrelevant context” cases
reflect scenarios in which the model relied on internal knowledge rather than external grounding.
While not classified as hallucinations, these responses raise questions about the transparency,
traceability, and consistency of model behavior in the absence of meaningful retrieval.

4.5.3. Time analysis

To evaluate the computational cost associated with agentic reasoning, we measured per-question
response times for both zero-shot prompting and the agentic retrieval framework using the 104-
question RadioRAG benchmark. Timing logs were collected from structured output directories for
each model. A fixed initialization overhead of 10,554.6 seconds per model, arising from the
context construction phase unique to agentic inference, was distributed uniformly across all
questions, resulting in an adjusted time increase of approximately 101.5 seconds per question.

To ensure robust comparison and mitigate the influence of extreme values, outlier
durations were handled using the Tukey method®°. Specifically, any response time that exceeded
the typical upper range, defined as values greater than the third quartile by more than 1.5 times
the interquartile range, was considered an outlier and replaced with the mean of the remaining
non-outlier values for that model and inference strategy. For each model, we computed the mean

27


and standard deviation of response times under both conditions. Additionally, we calculated the
absolute difference in average response time per question and the relative increase, defined as
the ratio of mean agentic response time to mean zero-shot response time.

To contextualize timing behavior across a heterogeneous model set, we grouped models
according to both parameter scale and architectural characteristics. This grouping approach
reflected the practical computational load of each model more accurately than parameter count
alone. Six distinct groups were defined: (i) the DeepSeek MoE group, including DeepSeek-R1
and DeepSeek-V3; (ii) the large model group (120-250 billion parameters), including Qwen 3-
235B, Mistral Large, and Llama4 Scout 16E; (iii) the medium-scale group (~70B), comprising
DeepSeek R1-70B, Llama3.3-70B, Qwen2.5-70B, and Llama3-Med42-70B; (iv) the Gemma-27B
group, containing Gemma-3-27B-it and MedGemma-27B-text-it; (v) the small model group (7—
8B), including Qwen 2.5-70B, Qwen3-8B, Llama3-Med42-8B, Llama3.3-8B, and Ministral-8B; and
(vi) the mini model group (3—4B), consisting of Gemma-3-4B-it, MedGemma-4B-it, and Qwen 2.5-
3B. Group-level averages and standard deviations were calculated across constituent models
and are reported in Table 4.

All timing evaluations were performed under identical system conditions to ensure fair
comparisons. While absolute response times may vary with hardware and load, the relative
increases provide a stable and interpretable metric for assessing the computational implications
of agentic retrieval.

4.5.4. Human evaluation

To benchmark LLM performance against domain expertise, we conducted a human evaluation
involving a board-certified radiologist (TTN) with seven years of experience in diagnostic and
interventional radiology. The evaluation followed a two-phase design to mirror the LLM
configurations.

In the first phase, the radiologist answered all 104 questions from the RadioRAG
benchmark without any external assistance, analogous to zero-shot prompting. The expert was
blinded to the LLM responses, dataset construction process, and reference standard answers.
Responses were recorded as final, and no additional time or information resources were permitted
during this phase.

In the second phase, we aimed to isolate the contribution of the agentic retrieval
component, independent of generative reasoning. For this, the same radiologist was provided
with the contextual evidence retrieved by the agentic system for each question, the same
Radiopaedia excerpts that were used as inputs for LLM agentic inference. The radiologist
answered the same 104 questions again, this time using the retrieved context as decision support,
without access to the original question-answer pairs or their previous responses. The format and
presentation of the contextual evidence were identical to what the LLMs received during agentic
inference, ensuring comparability.

This design enabled us to disentangle the effects of information retrieval from language
model reasoning, by comparing unaided radiologist performance, radiologist performance with

28


context, and agentic LLM outputs under standardized conditions. Accuracy was computed using
the same evaluation criteria applied to LLMs. Statistical comparisons between human and model
responses were performed using McNemar’s test on paired question-level outcomes. Confidence
intervals and p-values were adjusted for multiple comparisons using the false discovery rate.

4.6. Statistical analysis

Statistical analysis was performed using Python v3.11 with SciPy v1.10, NumPy v1.25.2, and
statsmodels v0.14.5 packages. For each dataset, bootstrapping with 1,000 redraws was used to
estimate means, standard deviations, and 95% confidence intervals (Cl)*". A strictly paired design
ensured identical redraws across conditions’. To assess statistical significance of pairwise
method comparisons across all LLMs, exact McNemar's test® (based on the binomial distribution)
was applied to each model individually. Resulting p-values were corrected for multiple
comparisons using the false discovery rate, with a significance threshold of 0.05. For group-level
comparisons between inference strategies (e.g., zero-shot vs. agentic RAG), paired two tailed t-
tests were used to compare average accuracy across models. To explore the relationship
between model size and performance, Pearson correlation coefficients were computed between
parameter counts and accuracy values within the Qwen 2.5 model family, separately for each
inference strategy.

4.7. Data availability

All data are available via either the original RadioRAG publication’®.

4.8. Code availability

All source code, configurations, and parameters used in this work are publicly available. The
agentic RAG pipeline, developed in Python 3.11, is available at:
https://github.com/sopajeta/agentic-rag. Our implementation relies on several key frameworks
and tools. We used LangChain Open Deep Research (https://github.com/langchain-ai/deep-
research) for experimental agent modules, LangChain v0.3.25 (https://github.com/langchain-
ai/langchain) for orchestration and agent management, and LangGraph_ v0.4.1
(https://github.com/langchain-ai/langgraph) to support multi-step control flow and_ task
decomposition. Model access and embedding generation were handled via the OpenAl Python
SDK _v1.77.0 — (https://platform.openai.com). The SearxNG metasearch — engine
(https://github.com/searxng/searxng) was also deployed via Docker  v25.0.2
(https://www.docker.com) and used for online web retrieval.

29


The traditional online RAG pipeline is hosted at
https://github.com/tayebiarasteh/RadioRAG, which relies on the LangChain v0.1.0, Chroma
(https://www.trychroma.com) for vector storage, and the OpenAl API v1.12 for embeddings.

All locally deployed language models sourced from Hugging Face, were assessed and used
between July 1-27, 2025, and are explicitly listed below, with corresponding URLs:

e Qwen 2.5-0.5B: https://huggingface.co/Qwen/Qwen2.5-0.5B

e Qwen 2.5-3B: https://huggingface.co/Qwen/Qwen2.5-3B

e Qwen 2.5-7B: https://huggingface.co/Qwen/Qwen2.5-7B

e Qwen 2.5-14B: https://huggingface.co/Qwen/Qwen2.5-14B

e Qwen 2.5-70B: https://huggingface.co/Qwen/Qwen2.5-72B

e Qwen 3-8B: https://huggingface.co/Qwen/Qwen3-8B

e Qwen 3-235B: https://huggingface.co/Qwen/Qwen3-235B-A22B

e Llama 3.3-8B: https://huggingface.co/meta-llama/Meta-Llama-3-8B

e Llama 3.3-70B: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct

e Llama 3-Med42-70B: https://huggingface.co/m42-health/Llama3-Med42-70B

e Llama 3-Med42-8B: https://huggingface.co/m42-health/Llama3-Med42-8B

e Llama4 Scout 16E: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E

e Mistral Large: https://huggingface.co/mistralai/Mistral-Large-Instruct-2407

e Ministral 8B: https://nhuggingface.co/mistralai/Ministral-8B-Instruct-2410

e Gemma-3-4B-it: https://huggingface.co/google/gemma-3-4b-it

e Gemma-3-27B-it: https://huggingface.co/google/gemma-3-27b-it

e Medgemma-4B-it: https://huggingface.co/google/medgemma-4b-it

e Medgemma-27B-text-it: https://nhuggingface.co/google/nedgemma-27/b-text-it

e DeepSeek-V3: https://huggingface.co/deepseek-ai/DeepSeek-V3

e DeepSeek-R1: https://nhuggingface.co/deepseek-ai/DeepSeek-R1

e DeepSeek-R1-70B: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B

All LLMs were served using vLLM v0.9.0 (https://github.com/vilm-project/vilm) with tensor
parallelism set to the number of GPUs inside the node, except for models under 3 billion
parameters, which were served without tensor parallelism.

4.9. Hardware

For the majority of experiments, particularly those involving standard LLMs, the computations
were performed on GPU nodes equipped with Nvidia H100 and H200 accelerators. The H100
configuration consisted of four Nvidia H100 GPUs, each providing 94 GB of HBM2e memory and
operating at a 500 W powerlimit. These GPUs were paired with two AMD EPYC 9554 “Genoa”
processors based on the Zen 4 architecture, each offering 64 high-performance cores running at
3.1 GHz. The H200 configuration featured four Nvidia H200 GPUs, each offering 141 GB of
high-bandwidth memory also at 500 W, coupled to the same dual AMD EPYC 9554 processor
configuration. This combination of high-end Nvidia accelerators from NHR@FAU’s Helma Cluster

30


(https://doc.nhr.fau.de/clusters/helma/) provided the necessary computational capabilities for
inferencing the majority of the LLMs used during our experiments.

Experiments involving extremely large-scale architectures, such as the DeepSeek R1 or
V3 model and other similarly demanding workloads, were executed on nodes equipped with
AMD’s MI300-series accelerators. In these cases, the MI300X configuration was utilized, which
combined a dual-socket AMD EPYC 9474F platform with a total of 96 CPU cores and 2304 GiB
of DDR5-5600 system memory, together with eight AMD Instinct MI300X accelerators. Each
MI300X GPU offered 192 GiB of memory, enabling inference runs that required massive
parameter counts and exceptional memory capacity (Deepseek R1 with 671 billion parameters).
Additional experimentation also leveraged AMD Instinct MI300A nodes that integrate 24-core
CPUs with unified on-package memory, with a total of 512 GiB shared across four accelerators.
The hardware used in our experiments included a local machine with an Intel Pentium CPU with
2 cores and 8 GB Memory for consuming API endpoints.

5. Additional information

5.1. Acknowledgements

This research is supported by BayernKl, the central infrastructure for the State of Bavaria to
advance academic Al research. The authors gratefully acknowledge the HPC resources provided
by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-
Alexander-Universitat Erlangen-Nurnberg. NHR funding is provided by federal and Bavarian state
authorities. NHR@FAU hardware is partially funded by the Deutsche Forschungsgemeinschaft
(DFG) — 440719683. DT was supported by grants from the DFG (NE 2136/3-1, LI3893/6-1, TR
1700/7-1) and is supported by the German Federal Ministry of Education (TRANSFORM LIVER,
031L0312A; SWAG, 01KD2215B) and the European Union’s Horizon Europe and innovation
programme (ODELIA [Open Consortium for Decentralized Medical Artificial Intelligence],
101057091). KB received grants from the European Union (101079894), Bayern Innovativ,
German Federal Ministry of Education and Research, Max Kade Foundation, and Wilhelm-Sander
Foundation.

5.2. Author contributions

The formal analysis was conducted by SW, JS, and STA. The original draft was written by STA,
JS, and SW and edited by STA. JS developed the codes for analysis and pipeline; SW configured
and maintained the LLM-serving infrastructure. The experiments were performed by SW and JS.
The statistical analyses were performed by SW, JS, and STA. DT, TTN, KB, LA, MR, and STA
provided clinical expertise. SW, JS, DT, ML, TTN, KB, MR, HK, GW, AM, and STA provided
technical expertise. The study was defined by STA. All authors read the manuscript and agreed
to the submission of this paper.

31


5.3. Competing interests

SW is partially employed by DATEV eG, Germany. DT received honoraria for lectures by Bayer,
GE, Roche, AstraZeneca, and Philips and holds shares in StratifAl GmbH, Germany, and in
Synagen GmbH, Germany. ML is employed by Generali Deutschland Services GmbH, Germany
and is an editorial board at European Radiology Experimental. KB and LA are trainee editorial
boards at Radiology: Artificial Intelligence. AM is an associate editor at IEEE Transactions on
Medical Imaging. STA is an editorial board at Communications Medicine and European Radiology
Experimental, and a trainee editorial board at Radiology: Artificial Intelligence. The other authors
do not have any competing interests to disclose.

References

1. Akinci D’Antonoli, T. et a/. Large language models in radiology: fundamentals, applications,
ethical considerations, risks, and future directions. Diagnostic and Interventional Radiology
(2023) doi:10.4274/dir.2023.232417.

2. Buess, L., Keicher, M., Navab, N., Maier, A. & Tayebi Arasteh, S. From large language
models to multimodal Al: A scoping review on the potential of generative Al in medicine.
Preprint at https://doi.org/10.48550/arXiv.2502.09242 (2025).

3. Tayebi Arasteh, S. et al. The Treasure Trove Hidden in Plain Sight: The Utility of GPT-4 in
Chest Radiograph Evaluation. Radiology 313, e233441 (2024).

4. Clusmann, J. et al. The future landscape of large language models in medicine. Commun
Med 3, 141 (2023).

5. Thirunavukarasu, A. J. et al. Large language models in medicine. Nat Med 29, 1930-1940
(2023).

6. Singhal, K. et a/. Large language models encode clinical knowledge. Nature 620, 172-180
(2023).

7. Arora, A. & Arora, A. The promise of large language models in health care. Lancet 401, 641
(2023).

8. OpenAl. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).

9. Fink, M. A. et al. Potential of ChatGPT and GPT-4 for Data Mining of Free-Text CT Reports
on Lung Cancer. Radiology 308, €231362 (2023).

10. Adams, L. C. et al. Leveraging GPT-4 for Post Hoc Transformation of Free-text Radiology
Reports into Structured Reporting: A Multilingual Feasibility Study. Radiology 307, e230725
(2023).

11. Kottlors, J. et al. Feasibility of Differential Diagnosis Based on Imaging Patterns Using a
Large Language Model. Radiology 308, e231167 (2023).

12. Schmidt, R. A. et al. Generative Large Language Models for Detection of Speech
Recognition Errors in Radiology Reports. Radiology: Artificial Intelligence 6, e230205
(2024).

13. Lewis, P. et a/. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. in
Advances in Neural Information Processing Systems (eds. Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M. F. & Lin, H.) vol. 33 9459-9474 (Curran Associates, Inc., 2020).

32


14.
15.
16.
17.
18.
19.
20.

21.

22.

23.

24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
oe Sellergren, A. et al. MedGemma Technical Report. arXiv preprint arXiv:2507.05201 (2025).
36.
37.
38.

39.
40.

Alkaissi, H. & McFarlane, S. I. Artificial Hallucinations in ChatGPT: Implications in Scientific
Writing. Cureus 15, €35179 (2023).

Ji, Z. et al. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55,
1-38 (2023).

Zakka, C. et al. Almanac — Retrieval-Augmented Language Models for Clinical Medicine.
NEJM AI 1, (2024).

Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking Retrieval-Augmented Generation for
Medicine. Preprint at http://arxiv.org/abs/2402.13178 (2024).

Tayebi Arasteh, S. et al. RadioRAG: Online Retrieval-Augmented Generation for Radiology
Question Answering. Radiology: Artificial Intelligence 7, €240476 (2025).

Radiopaedia Australia Pty Ltd ACN 133 562 722. Radiopaedia.

Brown, T. B. et a/. Language models are few-shot learners. in Proceedings of the 34th
International Conference on Neural Information Processing Systems vol. 159 1877-1901
(2020).

Fink, A., Rau, A., Reisert, M., Bamberg, F. & Russe, M. F. Retrieval-Augmented Generation
with Large Language Models in Radiology: From Theory to Practice. Radiology: Artificial
Intelligence 7, (2025).

Tayebi Arasteh, S. et a/. Large language models streamline automated machine learning for
clinical studies. Nat Commun 15, 1603 (2024).

Ferber, D. et al. Development and validation of an autonomous artificial intelligence agent
for clinical decision-making in oncology. Nature Cancer (2025)
doi:https://doi.org/10.1038/s43018-025-00991-6.

Wang, L. et al. A survey on large language model based autonomous agents. Front.
Comput. Sci. 18, (2024).

Zhou, H.-Y. et al. MedVersa: A Generalist Foundation Model for Medical Image
Interpretation. Preprint at https://doi.org/10.48550/ARXIV.2405.07988 (2024).

Schick, T. et a/. Toolformer: Language models can teach themselves to use tools. Advances
in Neural Information Processing Systems 36, 68539-68551 (2023).

Yao, S. et al. React: Synergizing reasoning and acting in language models. in /nternational
Conference on Learning Representations (ICLR) (2023).

Truhn, D., Reis-Filho, J. S. & Kather, J. N. Large language models should be used as
scientific reasoning engines, not knowledge databases. Nat Med 29, 2983-2984 (2023).
Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems 35, 24824-24837 (2022).
Karunanayake, N. Next-generation agentic Al for transforming healthcare. Informatics and
Health 2, 73-83 (2025).

Khattab, O. et al. Dspy: Compiling declarative language model calls into self-improving
pipelines. arXiv preprint arXiv:2310.03714 (2023).

Kogak, B. & Mese, |. Al agents in radiology: toward autonomous and adaptive intelligence.
dir (2025) doi:10.4274/dir.2025.253470.

Bai, J. et al. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).

Christophe, C., Kanithi, P. K., Raha, T., Khan, S. & Pimentel, M. A. Med42-v2: A Suite of
Clinical LLMs. Preprint at https://doi.org/10.48550/arXiv.2408.06142 (2024).
DeepSeek-Al et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via
Reinforcement Learning. Preprint at https://doi.org/10.48550/arXiv.2501.12948 (2025).
Touvron, H. et a/. LLaMA: Open and Efficient Foundation Language Models. Preprint at
http://arxiv.org/abs/2302.13971 (2023).

Grattafiori, A. et al. The Llama 3 Herd of Models. Preprint at
https://doi.org/10.48550/arXiv.2407.21783 (2024).

Liu, A. et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024).
Yang, A. et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025).

33


41.

42.
43.

4A,

45.

46.

47.

48.

49.

50.
51.

52.

53.

Team, G. et al. Gemma: Open models based on gemini research and technology. arXiv
preprint arXiv:2403.08295 (2024).

Team, G. et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786 (2025).
Jacobs, R. A., Jordan, M. I., Nowlan, S. J. & Hinton, G. E. Adaptive Mixtures of Local
Experts. Neural Computation 3, 79-87 (1991).

Bakhshandeh, S. Benchmarking medical large language models. Nature Reviews
Bioengineering 1, 543-543 (2023).

Wang, C. et al. Potential for GPT Technology to Optimize Future Clinical Decision-Making
Using Retrieval-Augmented Generation. Ann Biomed Eng 52, 1115-1118 (2024).
Kresevic, S. et al. Optimization of hepatological clinical guidelines interpretation by large
language models: a retrieval augmented generation-based framework. npj Digit. Med. 7,
102 (2024).

Hoffmann, J. et a/. Training compute-optimal large language models. arXiv preprint
arXiv:2203. 15556 (2022).

Kaplan, J. et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361
(2020).

Gilbert, S., Kather, J. N. & Hogan, A. Augmented non-hallucinating large language models
as medical information curators. npj Digit. Med. 7, 100 (2024).

Tukey, J. W. Exploratory Data Analysis. vol. 2 (Springer, 1977).

Konietschke, F. & Pauly, M. Bootstrapping and permuting paired t-test type statistics. Stat
Comput 24, 283-296 (2014).

Khader, F. et al. Artificial Intelligence for Clinical Interpretation of Bedside Chest
Radiographs. Radiology 307, e220510 (2022).

McNemar, Q. Note on the sampling error of the difference between correlated proportions or
percentages. Psychometrika 12, 153-157 (1947).

34


Supplementary Table 1: Characteristics of the RadioRAG dataset used in this study. The RadioRAG dataset
combines RSNA-RadioQA and ExtendedQA, as introduced in the original RadioRAG study. Patient demographic
information (age and sex) is based solely on the RSNA-RadioQA subset due to missing metadata in ExtendedQA.
Each question may be assigned to multiple radiology subspecialties. *Age and sex statistics reflect only the RSNA-
RadioQA subset. “Youngest patient was 2 days old. SD: Standard deviation; N/A: Not available.

Value

RadioRAG dataset

Patient age [years]*
Median
Mean + SD
Range

44
44421
(0**, 80)

Patient sex [n (%)]*
Total
Female
Male

Number of questions per subspecialty [n (%)]

80 (100%)
37 (46%)
43 (54%)

Total

104 (100%)

Breast Imaging

10 (10%)

Cardiac

10 (10%)

Chest

20 (19%)

CT

35 (34%)

Emergency Radiology

9 (9%)

Gastrointestinal

18 (17%)

Genitourinary

9 (9%)

Head and Neck

10 (10%)

MRI

27 (26%)

Molecular Imaging

11 (11%)

Musculoskeletal

20 (19%)

Neuroradiology

11 (11%)

Nuclear Medicine

13 (12%)

Oncologic Imaging

16 (15%)

Pediatric

8 (8%)

Radiation Oncology

9 (9%)

Ultrasound

10 (10%)

Vascular Imaging

35

16 (15%)


Supplementary Table 2: Agentic responses and retrieved sources for GPT-3.5-turbo on two RSNA-RadioQA
questions. Full answers generated by GPT-3.5-turbo using the agentic framework for the first two questions of the
RSNA-RadioQA dataset, along with the most relevant sources retrieved from Radiopaedia. Responses illustrate the
model’s reasoning and evidence grounding within the agentic pipeline.

Question identifier LLM’s (GPT-3.5-turbo) answer Most relevant sources

A: BI-RADS 4 - Suspicious abnormality requiring biopsy: BI-RADS 4 lesions are characterized by

suspicious findings with an intermediate risk of malignancy. The presence of rim calcifications

within a fat-containing mass raises concern for malignancy. Biopsy is generally recommended for

BI-RADS 4 lesions, especially if calcifications are suspicious. B: BI-RADS 2 - Rim Calcifications: ps://radiopaedia.org/articles/breast-imaging-

Rim calcifications in fat-containing masses are typically benign and classified as BI-RADS 2. These | reporting-and-data-system-bi-rads-assessment-
calcifications are often associated with benign processes like fat necrosis or oil cysts. The imaging ategory-4, https://radiopaedia.org/cases/suspicious-
features of a round, circumscribed fat-containing mass with rim calcifications align with a benign reast-calcifications-bi-rads-4,

diagnosis. C: BI-RADS 3 - Probably benign finding: BI-RADS 3 category is designated for probably ps://radiopaedia.org/articles/fat-containing-breast-
benign lesions with a low likelihood of malignancy. Fat-containing lesions with rim calcifications | lesions, https://radiopaedia.org/articles/breast-

may fall into this category if they demonstrate typical benign features. Short-interval follow-up is alcifications,

recommended for BI-RADS 3 lesions to monitor for any interval changes. D: Oil cyst secondary to ps://radiopaedia.org/cases/screening-

fat necrosis: Oil cysts are benign lesions resulting from fat necrosis, often following trauma or ammography-rim-calcifications,

surgery. These cysts typically present as fat-containing masses with rim calcifications, consistent ps://radiopaedia.org/articles/breast-imaging-

with the imaging findings described. The absence of a clear history of trauma or surgery does not | reporting-and-data-system-bi-rads-assessment-
exclude the possibility of an oil cyst. Conclude with a Final Answer: Based on the detailed analysis ategory-2, https://radiopaedia.org/cases/breast-

of the imaging findings and the differential diagnosis provided in the report, the most likely diagnosis alcification-bi-rads-2,
for the 66-year-old female with a palpable right breast lump at 10 o'clock, characterized by a round, ps://radiopaedia.org/articles/breast-imaging-
circumscribed fat-containing mass with rim calcifications on mammography, is: D: Oil cyst | reporting-and-data-system-bi-rads-assessment-
secondary to fat necrosis. Reasoning: The presence of a round, circumscribed fat-containing mass ategory-3, https://radiopaedia.org/articles/fat-
with rim calcifications in the absence of a clear history of trauma or surgery aligns with the typical | necrosis-breast-2,
imaging features of an oil cyst secondary to fat necrosis. Oil cysts are benign lesions associated ps://radiopaedia.org/articles/breast-imaging-
with fat necrosis and are characterized by the presence of fat within the lesion and rim calcifications porting-and-data-system-bi-rads-2,

on mammography. This diagnosis is further supported by the benign nature of rim calcifications in ps://radiopaedia.org/articles/oil-cyst-breast
fat-containing masses, typically classified as BI-RADS 2. Therefore, considering the imaging

findings and clinical context, the most likely explanation for the palpable abnormality in this case is

an oil cyst secondary to fat necrosis.

RSNA-RadioQA-Q11

A: Low-grade fibrosarcoma: Low-grade fibrosarcoma is a rare malignant mesenchymal tumor

characterized by spindle cell morphology with a herringbone pattern. Immunohistochemical

markers for fibrosarcoma often include vimentin positivity, but specific markers can vary. Beta-

catenin nuclear staining is typically absent in fibrosarcomas, which helps differentiate them from

other entities. The absence of nuclear beta-catenin staining in the presented case suggests low-

grade fibrosarcoma is less likely. B: Phyllodes tumor: Phyllodes tumors are rare fibroepithelial

breast tumors with a biphasic pattern of epithelial and spindle cell stromal components. Beta-

catenin staining may show nuclear positivity in some cases, reflecting Wnt pathway activation. The

clinical presentation of a rapidly growing breast mass in adult women aligns with the patient's | https://radiopaedia.org/articles/fibrosarcoma-of-the-

characteristics. The presence of spindle cells with nuclear beta-catenin staining in the presented | breast, https://radiopaedia.org/cases/breast-sarcoma-

case supports the possibility of a phyllodes tumor. C: Metaplastic carcinoma: Metaplastic | 2?lang=gb, https://radiopaedia.org/articles/phyllodes-

carcinoma is a rare primary breast malignancy with both epithelial and mesenchymal | tumour-1, https://radiopaedia.org/articles/metaplastic-
RSNA-RadioQA-Q2 components.Beta-catenin expression patterns may show aberrant nuclear localization, associated | breast-carcinoma,

with Wnt signaling pathway activation. The patient's history of prior radiation exposure aligns with | https://radiopaedia.org/cases/metaplastic-carcinoma-

a potential risk factor for metaplastic carcinoma.\n- The spindle cell morphology and nuclear beta- | breast, https://radiopaedia.org/articles/fibromatosis-

catenin staining in the presented case could be consistent with metaplastic carcinoma. D: Breast | of-the-breast, https://radiopaedia.org/cases/desmoid-

fibromatosis: Breast fibromatosis is a rare benign but locally aggressive tumor characterized by | fibromatosis-of-the-breast

spindle cell proliferation and strong nuclear beta-catenin staining.Clinical association with prior

radiation exposure matches the patient's history. The spindle cell morphology and nuclear beta-

catenin staining in the presented case are in line with breast fibromatosis. Final Answer: D: Breast

fibromatosis. Summary of Reasoning: Considering the patient's high risk of breast malignancy due

to BRIP1 mutation and prior radiation exposure, the presence of spindle cells with nuclear beta-

catenin staining in the breast mass suggests a diagnosis of breast fibromatosis. This rare tumor

entity aligns with the clinical presentation, histopathological findings, and the patient's medical

history, making it the most likely diagnosis in this case.

36


Supplementary Table 3: Zero-shot responses from GPT-3.5-turbo on the first 20 questions of the RSNA-
RadioQA dataset. Model-generated answers are shown without retrieval augmentation or agentic reasoning.
Responses reflect zero-shot inference using only the question text as input.

Question identifier

RSNA-RadioQA-Q11

RSNA-RadioQA-Q2

RSNA-RadioQA-Q3

RSNA-RadioQA-Q4

RSNA-RadioQA-Q5

RSNA-RadioQA-Q6

RSNA-RadioQA-Q7

RSNA-RadioQA-Q8

RSNA-RadioQA-Q9

RSNA-RadioQA-Q10

RSNA-RadioQA-Q11

RSNA-RadioQA-Q12

RSNA-RadioQA-Q13

RSNA-RadioQA-Q14

RSNA-RadioQA-Q15

RSNA-RadioQA-Q16

RSNA-RadioQA-Q17

RSNA-RadioQA-Q18

RSNA-RadioQA-Q19

RSNA-RadioQA-Q20

The mos

The mos

The mos

The mos
involvem

LLM‘s (GPT-3.5-turbo) answer

likely diagnosis is a benign breast lesion, such as a lipoma or fibroadenoma, with associated calcifications.
likely diagnosis is desmoid tumor (also known as aggressive fibromatosis).
likely diagnosis is Cat Scratch Disease (CSD) with suppurative granulomatous lymphadenitis.

likely diagnosis is bilateral invasive ductal carcinoma of the breast, ER positive, PR positive, and HER2 negative, with no lymph node
ent.

The most likely diagnosis is pulmonary rhabdomyosarcoma.

The mos

The mos

likely diagnosis is severe COVID-19 pneumonia.

likely diagnosis is pulmonary lymphangioleiomyomatosis (LAM), a characteristic manifestation of Tuberous Sclerosis Complex (TSC).

The most likely diagnosis is a patellar fracture.

The mos

likely diagnosis is diverticulitis with associated pericolic inflammation and phlegmon formation.

The mos

likely diagnosis is Kienb6éck's disease.

The most likely diagnosis is a benign simple cyst of the liver.
The most likely diagnosis is Giant Cell Tumor of Tendon Sheath (GCTTS).

The mos

The mos

The mos
cysts on

likely diagnosis is Alveolar soft part sarcoma (ASPS).
likely diagnosis is patellar tendon avulsion fracture.

likely diagnosis is benign complicated cysts, given the resolution of the mass with aspiration and the benign nature of the identified
imaging.

The most likely diagnosis is a retroperitoneal teratoma.

37