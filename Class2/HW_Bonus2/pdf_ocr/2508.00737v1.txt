How LLMs are Shaping the Future of Virtual Reality

Stieda Ozkaya™, Santiago Berrezueta~Guzman™, Stefan Wagner

“Technical University of Munich, Heilbronn, Germany

Abstract

The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immer-

sive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection

of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions,

a accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer-reviewed studies published between 2018

& and 2025, we identify key application domains—ranging from emotionally intelligent NPCs and procedurally generated storytelling

CN to Al-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence,

including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight

rs that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment

requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper

concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source
development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.

Keywords: Accessibility, Affective Computing, AI Game Mastering, Ethical AI, Immersive Games, Large Language Models
(LLMs), Memory Management, Multimodal Interaction, NPC Interaction, Personalized Gameplay, Procedural Storytelling,

S)
a. Real-Time Systems, Virtual Reality (VR).
N
Y,
1. Introduction
—
> The integration of Large Language Models (LLMs) into Vir-
cn tual Reality (VR) games represents a transformative step in the
[~ evolution of interactive digital environments (1) [2]. LLMs are
© neural networks trained on extensive text data to produce lan-
i) guage that resembles human speech. They have progressed
0© quickly in their abilities and uses, evolving from text-generating
© tools to agents engaging in real-time dialogue, narrative de-
sign, and adaptive learning [3] 4]. In parallel, VR games have
. - evolved from simulations to immersive worlds that leverage
. = spatial computing, haptic feedback, and embodied interaction
S< [5]. The convergence of these technologies offers unprece-
s| dented opportunities to enhance interactivity, narrative depth,
© emotional engagement, and accessibility in digital games [(6][7].
The core focus of gaming and simulation development on
immersive experiences has generated rising interest in artifi-
cial intelligence (AI) applications, especially LLMs for cre-
ating dynamic and human-like gameplay [8] [9]. The explo-
ration of LLMs as creative tools for virtual reality experiences
has emerged because these models enable complex non-player
character (NPC) dialogues and generate storylines and envi-
ronments that adapt to player choices [ii]. The mod-
els’ context-sensitive language capabilities enable personalized
gameplay that becomes more accessible and inclusive, thus
expanding educational and training possibilities and entertain-
ment options [12] [13].
This research paper explores the developing relationship be-
tween LLMs and VR games to determine their applications for

Preprint submitted to Pre-print

improving core immersive gameplay elements. This research
investigates the following key questions:

- RQ1. How do LLMs contribute to more emotionally intelli-
gent and lifelike NPC interactions?

- RQ2. In what ways can they support procedural storytelling
and adaptive narratives?

- RQ3. How do they affect personalization, accessibility, and
user experience in immersive environments?

- RQ4. What challenges and limitations -technical, ethical, and
practical—must be addressed while achieving their full poten-
tial?

- RQS5. How can future research leverage emerging trends in
multimodal AI, reinforcement learning, affective computing,
and open-source tools to build scalable, ethically responsible,
and emotionally attuned VR systems?

To answer these questions, we conduct a comprehensive liter-
ature review that examines current research and systems across
six application domains: (1) dynamic NPC interactions and
emotional intelligence, (2) procedural storytelling and narrative
generation, (3) intelligent game mastering and adaptive control
systems, (4) personalized player experience, (5) accessibility,
inclusivity, and usability, and (6) challenges and limitations in-
cluding ethical concerns and deployment barriers. Addition-
ally, we analyzed the current state of the art before defining the
main opportunities and constraints forming the future direction
of LLM-based VR gaming.

This paper contributes to the expanding field of intelligent
digital worlds by evaluating the combined impact of LLMs and
VR on gameplay and critically reviewing their technological

August 4, 2025


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

convergence. It aims to support researchers, developers, and de-
signers seeking to build more engaging, equitable, and respon-
sive VR environments through LLMs’ responsible and creative
use. Fi gure! provides an overview of this review’s organization
and thematic flow.

Paper Structure

' | Large Language Models: An

sgh | Literature Search Strategy |

| Virtual Reality Games: An |

Overview | Categorization of Literature |;

(ite Intersection of LLMs and VR) |
' Games '

(v) Challenges and Limitations '

[ Computational and Performance ]
Constraints j

| Dynamic NPC Interactions and '
' Emotional Intelligence

Procedural Storytelling and Narrative ) | Ethical and Safety Considerations |

Generation

User Experience and Immersion '
Issues ‘

| Intelligent Game Masters for VR and '
Adaptive Systems '

" . Scalability and Deployment
Personalized Player Experience

Accessibility, Inclusivity, and Usability |

} | Advances in Al for More Realistic VR
H Gaming

' | Integration with Other Technologies }

\ | Ethical Al Development for Games

Open-Source and Industry Trends

Figure 1: Overview of the paper structure and organization.

2. Background

Understanding the integration of Large Language Models
(LLMs) into Virtual Reality (VR) games requires a founda-
tional overview of both technologies and their evolution.

2.1. Large Language Models (LLMs )—Overview

LLMs have experienced considerable advancement in re-
cent years, moving from theoretical ideas to more advanced
and scalable architectures like OpenAI’s Generative Pre-trained
Transformers (GPT) series, Meta’s Large Language Model
Meta AI (LLaMA), and Google’s PaLM. The development of
large-scale transformer-based architectures has facilitated sig-
nificant progress in generating text, creating dialogue systems,
and enabling procedural content generation [3].

LLMs began with early autoregressive models, which fo-
cused on predicting the next word in a sentence based on the
words that came before. At first, this approach was a small

How LLMs are Shaping the Future of Virtual Reality

area of research in Natural Language Processing (NLP). How-
ever, it gained major attention after the release of GPT-2 in
2019, where it showed that transformer models trained on mas-
sive text datasets could generate high-quality, coherent lan-
guage—and that their output could be shaped using carefully
designed prompts [14] [15] [T6}.

OpenAI’s GPT models have been leading in developing
LLMs, driving major progress in natural language process-
ing [17]. GPT-3, with 175 billion parameters, impressed re-
searchers with its ability to generate fluent and meaningful text,
which led to its use in chatbots and interactive AI tools [18].
Later versions—GPT-3.5 and GPT-4—further improved these
abilities by using Reinforcement Learning from Human Feed-
back (RLHF), which helped the models give more accurate and
helpful responses [4] {16} [T9}.

In addition to OpenAlI’s work, other powerful transformer-
based models have been developed. Bidirectional Encoder
Representations from Transformers (BERT) introduced bidirec-
tional training, which made it better at understanding context
and improved tasks like text classification and natural language
inference. LLaMA was trained only on publicly available data,
making it more accessible for open-source use. While BERT
focused on deep understanding of text, LLaMA aimed to of-
fer smaller, more efficient models without losing performance
compared to other top models [20] [21] [22].

As LLMs have grown in size and capability, new multimodal
models like GPT-4V and Large Language and Vision As-
sistant (LLaVA) have expanded their use beyond just text.
These models can now understand images, generate speech, and
support interactive storytelling. They are also being improved
to use memory and computing power more efficiently while be-
coming more accurate and flexible.

2.2. Virtual Reality Games—Overview

Virtual Reality (VR) gaming has come a long way, moving
from simple simulations to highly immersive and interactive
experiences. This progress has been driven by hardware, soft-
ware, and AI advances, especially in areas like NLP and content
generation.

The beginnings of VR can be traced back to 1968, when
Ivan Sutherland and his student Bob Sproull created the first
computer-based VR system, known as the ’Sword of Damo-
cles”. It used head tracking to display a basic 3D wireframe
view that changed with the user’s perspective. While it wasn’t
a game, it introduced key ideas, like perspective-based inter-
action, that still form the foundation of modern VR gaming
[25] |).

The 3D Internet allows users to move around and interact
with digital objects in space, rather than just clicking on flat
screens. A well-known example is Second Life {26}, a virtual
world where people can socialize and explore using avatars.
It combines AI, 3D headsets, motion sensors, and even holo-
graphic displays to create more immersive experiences [27].

After early VR experiments in the 1980s, the release of
consumer-friendly devices like the Oculus Rift, HTC Vive, and
PlayStation VR changed the gaming world. These headsets


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

gave players access to realistic virtual worlds with features like
motion tracking, haptic feedback (touch sensations), 3D visu-
als, and the use of photogrammetry [28].

As VR technology improved, games became more realistic,
featuring better physics, stronger hardware, smarter characters,
and stories that change based on player choices. Modern VR
games now include AlI-powered characters, voice interaction,
and multiple communication methods, making the experience
feel more natural and immersive, like talking to real people.

VR gaming hardware includes two main parts: output de-
vices and input devices. Output devices, like head-mounted
displays (HMDs), show 3D visuals and provide a wide field
of view to make the experience realistic. These can be mobile
(like Google Cardboard or Samsung GearVR) or wired (like
Oculus Rift, HTC Vive, and PlayStation VR), and often include
motion sensors and sometimes eye tracking for better perfor-
mance. To simulate touch, devices like vests, gloves, and full-
body suits provide haptic feedback, letting players feel things
like force, wind, or temperature. All these tools work together
using motion sensors and tracking systems to deliver accurate
and responsive gameplay [5].

Despite its rapid development, the VR gaming sector still
faces key challenges, including high production costs, limited
content, motion sickness, and hardware accessibility. The im-
mersive potential of VR is well recognized, but the expense
of headsets and the need for high-performance computers con-
tinue to hinder widespread adoption [5]. Additionally, some
users experience nausea or disorientation during extended play,
often due to latency or mismatched sensory input [29]. Devel-
oping VR content also demands specialized tools and expertise,
creating barriers for smaller studios. These factors highlight
that technical and usability issues must be addressed for VR
gaming to scale sustainably.

2.3. The Intersection of LLMs and VR Games

The convergence of LLMs and VR technologies represents
a significant advancement in developing interactive digital en-
vironments, particularly in entertainment-based and serious
games [B30]. The convergence of these technologies enables de-
velopers to create innovative immersive experiences, dialogue
systems, and educational simulations [I] [2].

Initial applications of LLMs in VR have focused on improv-
ing user interaction and supporting developers. One of the ear-
liest documented integrations involved helping novice develop-
ers create VR content. Tools like ChatGPT assist with code
suggestions, debugging, and explaining concepts within devel-
opment environments such as Unity, making the VR develop-
ment process more accessible [6].

Beyond development support, LLMs are increasingly used
in real-time VR environments to generate narrative content,
enable dynamic interactions, and simulate intelligent NPCs.
These conversational agents are especially valuable in educa-
tional and training applications, where they provide adaptive
feedback and act as virtual tutors [7/31].

A key use of LLMs in VR is in Al-driven dialogue systems.
Unlike scripted interactions, LLMs support unscripted, context-
aware conversations that enhance immersion. For instance, the

How LLMs are Shaping the Future of Virtual Reality

*LeamingverseVR” platform uses generative AI to cre-
ate NPCs with distinct personalities and backgrounds, enabling
learners to engage in personalized, natural dialogue while ex-
ploring content at their own pace.

Various VR game genres have integrated LLM-based as-
sistants to guide players through challenges using voice in-
teraction [35]. These systems offer real-time
problem-solving and adapt to player input, enhancing engage-
ment through natural, human-like communication.

The applications of LLMs in VR span several key domains,
including NPC interaction, game mastering, accessibility, per-
sonalization, and ethics. Figure [2| provides a visual overview
of these areas, many of which are explored in depth through-
out this paper to highlight current implementations and future
opportunities.

| Personalized Help Agents Personalized Help Agents |

Contextual [  Someual Memo |

inl rs
[  Someual Memo | I [ tntetigent Game Masters. | [ tntetigent Game Masters. | Masters

| Dynamic NPC Personalized [ Prrerrees | &
Converisations [ Prrerrees |
Emotionally Aware ls Culture-Aware Dialogue -_Culture-Aware Dialogue |
Interactions
| Memory-Consistent NPCs Accessibility Support | Accessiity Support |

}——+{_Grpainy erg Meral | and Moral
= Design

— NPC Reactions
Text-to-Speech and Speech-
to-Text

[Procedural Quest ——_ \
<n in VR <— }

[ emigemae Aware
[ emigemae tellin,

Nonverbal Interactions
(gaze, gestures, lip sync)

[__Mutinguat npcs | [__Mutinguat npcs |

[User immersion Support| Immersion [User immersion Support|

Scene ee for
ee Impaired
Autonomous Dungeon
Mastering

Personalized Player
Feedback

Figure 2: Application areas of Large Language Models in Virtual Reality games

3. Methodology

This study employed a structured literature review to exam-
ine how Large Language Models (LLMs) are applied in Virtual
Reality (VR) games. The goal was to identify current applica-
tions, implementation strategies, and key technical and ethical
challenges.

3.1. Literature Search Strategy

A comprehensive search was carried out across five leading
academic databases: IEEE Xplore, ACM Digital Library, Sco-
pus, Web of Science, and Google Scholar. These sources were
chosen for their extensive coverage of peer-reviewed research
in computer science, artificial intelligence (AI), and immersive
technologies.

Various search terms were used in different Boolean com-
binations (AND, OR) to capture a broad and relevant set of
studies. To improve precision, we used Boolean logic to com-
bine general terms (e.g., “virtual reality”) with model-specific


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

terms (e.g., “GPT-4’, “conversational agent”), and excluded
non-relevant acronym matches. Wherever supported by the
database, double quotation marks (e.g., virtual reality”, *NPC
dialogue”) were used to ensure exact phrase matching and re-
duce noise in search outputs.

These search terms targeted the intersection of LLMs and
VR across multiple application areas such as NPC interaction,
procedural storytelling, accessibility, personalization, and sys-
tem performance. Table[T]summarizes the main categories and
example search terms used during the database queries.

Clear inclusion criteria were established to guide the selec-
tion of relevant studies. These criteria prioritized recency, rele-
vance to LLM and VR integration, and practical application in
gaming or adjacent immersive contexts. Table [2] presents the
conditions that studies had to meet to be considered in the final
review.

Alongside the inclusion conditions, exclusion criteria were
defined to eliminate irrelevant or low-quality sources. These
criteria ensured that only studies with substantial technical con-
tributions and contextual alignment were analyzed. TableB]out-
lines the reasons for omitting certain works from the review.

While the primary focus of the research review was on recent
publications, some earlier works were also included, particu-
larly those published before 2015. These were not used mainly
to analyze’ Key Applications” or Challenges and Limitations”
parts, but served as foundational sources to explain core con-
cepts and historical context in the Background section.

The initial database queries returned a total of 528 records
across five academic databases. After removing duplicates, 422
papers remained. Title-based relevance filtering further reduced
the pool to 250 studies, which were then screened based on their
abstracts. Of these, 89 papers were selected for full-text review
due to their relevance to LLM integration in VR or transferable
insights into virtual game design. Ultimately, 62 papers were
included in the final literature review.

Due to the limited number of studies focused exclusively on
“LLMs in VR games,” the inclusion criteria were purposefully
extended to papers addressing related topics, such as LLMs in
non-VR games or LLM-powered interactions in VR simula-
tions, which were analyzed to extract applicable insights. A
snowballing method was also employed, wherein frequently
cited and conceptually central studies were traced backward
from reference sections.

The distribution of reviewed papers by publication year re-
flects a sharp increase in research interest, especially after 2023,
peaking in 2024 and 2025 (see Figure [3).

Following the full-text analysis of 62 selected studies, a the-
matic categorization was conducted to structure the findings
and identify primary research directions. The classification
framework was based on the scope of each study and the recur-
ring concepts in the literature. Studies were grouped according
to their primary application domains, key findings, and the chal-
lenges they addressed. Two main thematic categories emerged:

e Key Applications of LLMs in VR Games

e Challenges and Limitations in Implementation

How LLMs are Shaping the Future of Virtual Reality

2020
2025

2021

2022

2023
2024

Figure 3: Distribution of reviewed papers by publication year.

We divided these categories into subcategories based on
common research objectives, technical approaches, and exper-
imental setups. The classification is aligned with the structure
of the literature review section in this paper, which includes:

1. Dynamic NPC Interactions and Emotional Intelli-
gence: Studies that implement LLMs to enhance the re-
alism and responsiveness of NPC dialogue and behavior.

2. Procedural Storytelling and Narrative Generation:
Works on generating adaptive, personalized narratives us-
ing LLMs, often in role-playing games or branching story
environments.

3. Intelligent Game Masters and Adaptive Systems: Re-
search using LLMs for autonomous scene control, impro-
visational gameplay, and dynamic environment manage-
ment.

4. Personalized Player Experiences: Papers discussing sys-
tems that tailor content, difficulty, or narrative tone based
on player preferences and interaction history.

5. Accessibility and Inclusivity: Studies that leverage
LLMs for real-time translation, multimodal interaction,
and interface personalization to support diverse player
groups.

6. Computational and Performance Constraints: Re-
search that addresses latency, memory management, and
computational costs in real-time LLM deployment within
VR games.

7. Ethical and Safety Considerations: Papers exploring
content moderation, bias mitigation, and privacy in AI-
driven VR applications.

8. User Experience and Immersion Issues: Studies ex-
amining the perceptual quality of interactions with LLM-
powered NPCs, focusing on believability, consistency, and
emotional engagement.

9. Scalability and Deployment Barriers: Contributions
discussing challenges in bringing experimental LLM+VR
systems into real-world, multi-user, or commercial set-
tings.


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

How LLMs are Shaping the Future of Virtual Reality

Table 1: Search Terms and Categories Used in the Literature Review

Search Term

Example Keywords and Phrases

LLMs in VR Games

“LLMs in VR Games”, “GPT in VR Games”, “NPC Dialogues in Virtual Reality”,
“LLM-based conversational agents in VR”

AI Storytelling

29 66

“Large Language Models in Gaming”,
ratives in immersive environments”

Procedural Storytelling with AI’, “Al-generated nar-

NPCs and Game Masters

“AlI-driven NPC interactions in VR”, “Intelligent Game Masters in VR”, “LLMs as Game
Masters in VR”, “Al-controlled characters in virtual environments”

Player Personalization

“Personalized Player Experience with LLMs”, “Adaptive narrative systems in VR”, “Behavior-
driven dialogue generation”

Performance Issues

99 66

“LLM latency in VR games’, “Real-time response in Al-driven gameplay”,
overhead in immersive environments”

Computational

Memory and Ethics

“LLM memory management for virtual agents”, “Ethical issues in VR AT’, “Long-term inter-
action in AI systems”

Bias and Safety

“Bias in LLM-based NPCs”, “Trust and safety in conversational agents’, “Content moderation
in AJ-driven games”

Immersion “Immersion and believability in Al-driven VR”, “User experience with AI NPCs”, “User ex-
periences in VR”
Deployment “Deployment challenges for LLMs in VR”, “Scalability of LLMs in VR games”, “Integration
of LLMs into game engines”
Table 2: Inclusion Criteria for Selected Literature
Criterion Description

Time Frame

Studies published between 2018 and 2025, covering the period in which transformer-based LLMs emerged
and were first explored in immersive technologies

Publication Type

Peer-reviewed journal articles, conference papers, or technical reports with publicly available full texts

Topical Relevance

Focused on the integration of LLMs into VR environments, with a primary emphasis on gaming contexts

Related Applica-
tions

Papers addressing adjacent VR domains such as education, training, or accessibility using LLMs, provided
they offer transferable insights for VR gaming

Table 3: Exclusion Criteria for Literature Screening

Criterion Description
Language Studies not written in English, to ensure consistency and accessibility during the review process
Scope Publications unrelated to either LLMs or VR/immersive environments, or papers where the two technologies

were discussed independently without meaningful integration

Depth of Contribu-
tion

Conceptual or commentary papers without technical implementation, empirical evaluation, or practical de-
sign frameworks involving LLMs in VR



S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

Table 4: Summary of Literature Screening and Selection Process

Stage Papers
Total papers retrieved from databases | 528
After duplicate removal 422
After title-based filtering 250
Abstract-screened papers 250
Full-text papers reviewed 89
Final papers included in review 62

This taxonomy provides a structured synthesis of current re-
search and helps identify emerging opportunities and gaps in
integrating LLMs with VR games. It also enables comparative
evaluation of different technical and design strategies.

Figure [4] illustrates the approximate number of papers asso-
ciated with each key application area to visualize the distribu-
tion of reviewed studies across these categories. Note that in-
dividual studies may span multiple categories if they contribute
meaningfully to more than one domain.

Dynamic NPC Interactions and Emotional Intelligence

Procedural Storytelling and Narrative Generation

Intelligent Game Masters for VR and Adaptive Systems

Personalized Player Experience

Accessibility, Inclusivity, and Usability

Application Area

Computational and Performance Constraints

Ethical and Safety Considerations

User Experience and Immersion Issues

Scalability and Deployment

°
nN
5

6
Number of Papers

Cs
B
°
B
R

Figure 4: Categorization of reviewed papers based on key application areas.
Note that one paper may be relevant to multiple categories.

4. Key Applications of LLMs in VR Games

As the convergence of Large Language Models (LLMs) and
Virtual Reality (VR) advances, various impactful use cases have
emerged across the VR gaming landscape.

4.1. Dynamic NPC Interactions and Emotional Intelligence

Recent advancements in LLMs have enabled the develop-
ment of more responsive and emotionally intelligent NPCs
within VR environments. Modern NPCs move away from fixed,
pre-programmed behaviors because they now use LLMs to gen-
erate dynamic conversational abilities, emotional expression,
and adaptive interaction strategies [8]. This section combines

How LLMs are Shaping the Future of Virtual Reality

essential research findings to show how LLMs improve char-
acter behavior and dialogue delivery in virtual reality environ-
ments.

Emotionally Expressive NPCs with LLMs. The use of LLMs
such as GPT-3.5 and GPT-4 to give NPCs facial expressions,
gestures, and emotionally human-like dialogue is on the rise.

Normoyle et al., used GPT-3.5 to create facial expres-
sions, body movements, and lip-syncing for game characters
based on what was being said. They used the Facial Action
Coding System (FACS) and Laban Movement Analysis (LMA)
to guide the animations. Their study was done in a 3D point-
and-click game, not real-time VR. One limitation was that
small changes in prompts could cause inconsistent animations,
a known issue with LLMs. Despite this, the study shows that
LLMs can automatically help generate emotional character be-
havior, saving time and making interactions feel more lifelike.

Building on this, Marincioni et al. studied how LLMs
could assign emotions like Happy, Sad, Angry, or Neutral to
NPCs in a mystery game, and how these emotions affected
players. Interestingly, players often reacted positively even to
negative emotions, such as gratitude toward angry NPCs. This
reveals how emotionally expressive NPCs can create complex
psychological responses. The study shows that giving NPCs
emotional depth using LLMs can greatly enhance immersion
and shape the overall gameplay experience.

Personality and Conversational Naturalism. Consistency in
NPC personality is essential for believable and immersive in-
teractions. Hasani and Udjaja proposed an early frame-
work combining generative dialogue, emotional cues, and mul-
timodal interaction to support personality-consistent, context-
aware responses. Building on this, Zhu et al., found that
users retained more information and felt more immersed when
engaging with human-like avatars than abstract ones. Similarly,
Tonini’s international study showed that voice-driven AI
NPCs enhanced user experience through emotionally engaging
and polite communication, though issues like latency and lim-
ited memory reduced sustained immersion.

Memory, Consistency, and Long-Term Interaction. Ensuring
consistent dialogue over time remains a key challenge in AI-
driven NPC design. Zheng et al., proposed a dual-memory
system (MemoryRepository) that mimics human-like forgetting
and summarization, allowing NPCs to recall both recent and
long-term interactions. Tested with models like GPT-4, GPT-
3.5, and ChatGLM, the system improved dialogue continuity,
engagement, and immersion. In a related approach, Jahangiri
et al., focused on optimizing performance by combining
LLMs with Pursuit Learning Automata (PLA). Their hybrid
system enabled faster responses and dynamically adjusted di-
alogue tone to match player preferences, balancing emotional
richness with real-time scalability.

Multimodal and Nonverbal Interactions. LLMs also support
multimodal NPC interactions by combining voice, gaze, and
gesture, making characters more lifelike and responsive. Play-
ers tend to prefer NPCs that recognize physical gestures, such


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

as waving or nodding, and provide real-time feedback through
cues like lip-syncing and state lights. Yin and Xiao
analyzed 47 VR games and found that physical actions signifi-
cantly enhance immersion. Players expected NPCs to respond
to proximity, gestures, and eye contact, making them feel more
aware and reactive. Maslych et al. further emphasized
the role of feedback cues like state lights, gaze, and facial ex-
pressions during conversations, which increased users’ trust and
engagement. Even simple indicators during system response
times, such as loading bars, reassured users that the NPC was
actively processing their input. Sissler [9] developed an open-
source Unity framework using GPT-3.5 that integrates voice,
gestures, and animated facial expressions. The study showed
that synchronized multimodal responses improved NPC believ-
ability and helped players feel heard. These findings highlight
that real-time multimodal feedback is key to creating immersive
and socially engaging LLM-driven NPCs in VR.

Across the reviewed studies, LLMs such as GPT-3.5 and
GPT-4 are widely adopted to create emotionally expressive, so-
cially aware NPCs capable of real-time dialogue. While many
systems simulate facial expressions, gestures, and vocal affect,
most were tested outside of fully immersive VR settings. Re-
search consistently highlights the importance of consistent per-
sonality, emotional depth, and long-term memory in maintain-
ing user immersion. However, limitations remain in sustain-
ing coherence over extended interactions and minimizing la-
tency in real-time environments. The integration of multimodal
cues—voice, gaze, gesture—has proven especially effective in
enhancing believability and player engagement.

4.2. Procedural Storytelling and Narrative Generation

LLMs are changing how games tell stories by automatically
creating quests, dialogues, scenes, and storylines that adjust to
player actions and preferences. This flexibility shapes players’
experiences, making gameplay more immersive and personal-
ized. Recent research has explored ways LLMs support story-
telling, such as generating new story paths, building dynamic
quests, and adapting to the game’s context. These studies show
that LLMs can improve interactive storytelling, though chal-
lenges with consistency and coherence remain.

One study used GPT-4-powered NPCs in an interactive fic-
tion game where players could speak freely instead of choos-
ing from pre-written options. This led to unexpected story-
lines and character relationships that the designers had not
planned—players who liked exploring enjoyed this freedom to
create complex and personal narratives. However, the system
sometimes repeated itself or gave inconsistent replies due to
memory limitations [11]. Another project, PANGeA, combined
LLMs with branching logic to create quests and dialogue in a
turn-based RPG. The game world changed based on player de-
cisions, leading to unique and replayable stories. While this ap-
proach gave players more variety, it sometimes produced plot
inconsistencies, especially during long play sessions [44].

Procedural Quest and Dialogue Generation. LLMs have been
widely explored for generating quests and dialogues in role-
playing games (RPGs). One study fine-tuned GPT-2 using 978

How LLMs are Shaping the Future of Virtual Reality

quests from existing games, resulting in a model called Quest-
GPT-2. This system produced more varied and creative quests
than traditional retrieval-based methods, and human evaluators
found that about 20% of the generated quests were usable with-
out significant edits. However, the model struggled with coher-
ence, often creating quests with unclear goals or inconsistent
character relationships, especially in multi-step storylines [45].

Another approach used knowledge graphs alongside LLMs
to improve coherence and relevance. The system combined in-
formation about characters, history, and player choices to gen-
erate quests and dialogues aligned better with the game world.
Human reviewers rated these quests higher for fluency and log-
ical consistency than standard LLM output. Still, the method
faced challenges with memory retention and maintaining story
consistency over longer play sessions [46].

Further development came through a persona-based frame-
work that used LLMs to generate consistent character dia-
logues across different scenes. This system used “persona
cards” to define character traits and “scene cards” to give con-
text, which helped LLMs maintain each NPC’s personality over
time. Combining prompt engineering and fine-tuning, even
smaller LLMs (with around 7 billion parameters) could produce
high-quality, personality-rich dialogues [47] [48].

Scene, Context, and Environment-Aware Storytelling. Recent
studies have focused on how LLMs can generate narratives and
dialogue that adapt to in-game environments and context. One
notable system, SceneCraft, used GPT-4 to create interactive
scenes and cutscenes by combining predefined templates with
probabilistic variation. Developers could define scene struc-
tures, and the system would expand them into coherent story
events. While the generated scenes were engaging and con-
sistent in individual instances, maintaining character and world
consistency across multiple scenes remained a challenge [49].

Radez and Bohak introduced a system that enabled
NPCs to generate dialogue based on their awareness of the
game environment. Using panoramic image capture and seman-
tic segmentation, NPCs could reference nearby objects and spa-
tial relationships, creating more believable and immersive inter-
actions. Players appreciated the added realism, but the system’s
high computational demands limited its application in real-time
VR settings.

Li et al., proposed a schema-based prompting method
for GPT-4 Turbo agents to handle spatial interactions in VR,
such as pointing, grabbing, or navigating scenes. Their sys-
tem generated dialogue based on environmental cues, object
properties, and user actions. The agents were tested in various
role-play scenarios and showed practical spatial reasoning and
responsiveness. However, they sometimes hallucinated object
references and struggled in more complex environments.

Earlier work by HamAlainen et al., demonstrated a sys-
tem that adapted NPC dialogue in Fallout 4 based on gameplay
variables like health or quest progress. Instead of generating
new lines, the system rephrased existing ones to better fit the
player’s current state. While effective for personalization, it
lacked memory of past interactions and could not support dy-
namic long-term conversations.


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

The reviewed studies demonstrate that LLMs can signifi-
cantly enhance procedural storytelling by generating dynamic
quests, dialogues, and scene-aware narratives. Techniques such
as fine-tuning, prompt engineering, and knowledge graph in-
tegration help maintain coherence and character consistency,
though challenges persist with memory retention and logical
continuity across extended sessions. Systems like SceneCraft
and schema-based prompting show promise in generating
context-aware scenes and spatially grounded dialogue, yet of-
ten face scalability limitations in real-time VR settings. Over-
all, while LLMs expand narrative flexibility and personaliza-
tion, consistent world-building and long-term dialogue coher-
ence remain open challenges for future development.

4.3. Intelligent Game Masters for VR and Adaptive Systems

Recent advancements in LLMs have made it possible to cre-
ate AI Dungeon Masters (DMs) capable of managing player-
driven narratives and improvising gameplay in real time. Sev-
eral studies show that AI DMs can take over key storytelling
responsibilities typically handled by human game masters, en-
hancing the role-playing experience. For instance, ChatGPT
has been explored as a DM for tabletop role-playing games
(TTRPGs) like Dungeons & Dragons. It was able to gener-
ate coherent narratives and respond to player input dynami-
cally. However, the study also noted limitations, such as de-
layed responses and limited emotional engagement, which af-
fected player immersion [53].

To support novice game masters, Kelly et al. expanded a tool
called Shoelace by adding LLM-based dialogue suggestions
and information retrieval. This helped users manage scenes and
improvise more effectively, especially beginners [54].

Another study focused on improving AI DMs by integrating
function calling into LLMs. In the context of Jim Henson’s
Labyrinth: The Adventure Game, the system used two types
of functions: one for simulating dice rolls and another for up-
dating the game state. Combining both functions led to more
consistent and engaging storytelling, as the AI could better fol-
low game rules and handle random events [55].

Research has also looked into how the personality of AI game
masters affects players. Findings show that players respond
more positively to friendly and cooperative AI DMs, suggest-
ing that the tone and demeanor of the AI can influence both
gameplay and player emotions [56].

Adaptive and Interactive AI Systems in VR Games. LLMs are
now used in tabletop games and as adaptive assistants and world
managers in dynamic virtual environments. One early example
involved a GPT-based voice assistant in a low-cost VR escape
room, which provided hints and story cues based on the game’s
context. While this improved gameplay through adaptive re-
sponses, it faced challenges such as response delays and limited
real-time flexibility [57].

Beyond desktop and cloud-based systems, some studies ex-
plored lightweight mobile VR applications. For example, Khan
et al. created a multiplayer VR carrom game where players
competed against an AI opponent using Bluetooth controls and

How LLMs are Shaping the Future of Virtual Reality

first-person vision. This demonstrated an early attempt to in-
tegrate Al-driven decision-making into mobile VR platforms
[58}.

In more complex scenarios, LLMs have been used to con-
trol multi-agent teams in adversarial search-and-rescue games.
These AI agents outperformed traditional strategic planning and
opponent modeling models by using advanced prompting meth-
ods such as Zero-shot Chain-of-Thought (CoT) and iterative
cue-based learning [59].

The LLMR framework furthers this by offering a modular
system for managing interactive virtual worlds. It uses multiple
GPT-based modules to handle scene understanding, task plan-
ning, and debugging. This setup enables real-time 3D scene
creation with fewer errors and greater coherence than using
a single LLM alone [60]. Together, these studies show how
LLMs can support adaptive, responsive, and intelligent control
of VR game environments.

LLMs are increasingly leveraged to function as intelligent
game masters and adaptive agents in VR, capable of facilitating
improvisational storytelling, rule-based decision-making, and
multi-agent coordination. Studies show that ChatGPT and sim-
ilar models can effectively manage narrative flow and simulate
dynamic events, though response delays and limited emotional
depth still affect immersion. Integrations with function calling,
tone customization, and scene management tools like Shoelace
and LLMR have improved coherence and flexibility. However,
most applications remain experimental or limited to lightweight
systems, highlighting the need for further optimization for real-
time, large-scale VR environments.

4.4. Personalized Player Experience

LLMs offer new ways to personalize gameplay in VR by
enabling adaptive dialogue, emotional feedback, and context-
aware storytelling. Personalization now goes beyond adjusting
difficulty or settings—it involves creating Al-driven agents that
can understand, remember, and respond to players in socially
intelligent and emotionally engaging ways. While some aspects
have been discussed earlier, this section focuses on broader
strategies such as creativity support, player modeling, and emo-
tionally tailored narratives.

One primary use of LLMs in personalization is enabling nat-
ural conversations between players and virtual agents. Stud-
ies have shown that players remember more and engage longer
when interacting with LLM-powered avatars, especially when
the agent remembers previous interactions and maintains a
human-like personality [38] [33].

A promising direction involves using LLMs to support player
creativity. Lin et al., developed a VR brainstorming sys-
tem where ChatGPT-powered NPCs acted as creative partners.
These assistants offered voice suggestions, summarized discus-
sions, and retrieved relevant information in real time. The sys-
tem encouraged divergent thinking and collaborative idea gen-
eration by understanding the ongoing conversation, turning the
Al into a co-creator rather than just a tool.

Tucek explored emotionally personalized storytelling,
where NPCs adapted to each player’s social identity, emotional


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

state, and choices. These emotionally aware agents used LLMs
to generate real-time dialogue aligned with the player’s per-
spective, aiming to foster empathy and deeper narrative engage-
ment.

Other studies focused on personalizing VR experiences
through familiarity. Guo et al., found that players re-
sponded better to NPCs that looked or sounded familiar. In ex-
ergames, avatars that reflected users’ preferences improved en-
joyment and performance, particularly for more self-conscious
players. These results show that even simple visual or audi-
tory customization can enhance user experience in measurable
ways.

LLMs are enabling highly personalized VR experiences by
supporting emotionally aware, conversationally adaptive, and
context-sensitive virtual agents. Studies highlight that memory
retention, personality continuity, and co-creative dialogue en-
hance user engagement and satisfaction. Creative assistance,
emotionally aligned storytelling, and familiarity-based cus-
tomization have been shown to foster empathy, enjoyment, and
performance. However, most implementations remain small-
scale or experimental, and sustaining long-term personalization
in complex, dynamic environments remains a challenge for fu-
ture work.

4.5. Accessibility, Inclusivity, and Usability

LLMs are increasingly used in VR environments to improve
accessibility, inclusivity, and usability. Their integration sup-
ports the development of adaptive systems that address diverse
user needs, including individuals with disabilities, language dif-
ferences, or limited technological experience. This section ex-
plores how LLMs enhance VR through multimodal assistance,
personalized dialogue, and culturally sensitive design.

Multimodal and Sensory Accessibility. One key advantage of
LLMs is their ability to generate natural language explanations
for users with sensory limitations. Multimodal models like
GPT-4V enable scene descriptions via text-to-speech, helping
visually impaired users navigate virtual spaces. For example,
EnVisionVR interprets 360-degree scenes to provide real-time
audio feedback on spatial layouts and object locations [64].

LLMs also support inclusive design by enabling dynamic,
user-adaptive interactions. Bozkir et al., argue that LLM-
powered NPCs can adjust to different user needs through
prompt engineering and fine-tuning, offering more personalized
and equitable experiences than static, pre-scripted agents.

VR Games for Mental Health and Well-being. Baghaei et al.
conducted a design-driven study exploring how individu-
alized virtual reality GVR) environments could enhance men-
tal health outcomes, particularly among young people aged
18-25. Drawing on prior work by Falconer et al. [66],
they implemented a VR experience aimed at increasing self-
compassion as a pathway to alleviating depressive symptoms.
Participants could personalize key aspects of the virtual envi-
ronment, including the avatar, therapeutic setting, and avatar

How LLMs are Shaping the Future of Virtual Reality

behaviors. The study found that such personalized experi-
ences were perceived as more meaningful, emotionally en-
gaging, and safer than standardized VR therapy. Personaliza-
tion—especially when tailored to the user’s identity, emotional
state, and goals—was shown to enhance users’ motivation and
sense of connection. These findings support the potential of
iVR to provide scalable, user-centered mental health interven-
tions.

Baghaei et al. conducted a scoping review of 34 stud-
ies that used VR to treat depression and anxiety. Their findings
indicate that the majority of included studies reported positive
therapeutic outcomes when VR was used as part of a treat-
ment strategy. Notably, nine of these studies applied cogni-
tive behavioral therapy (CBT) within or alongside VR envi-
ronments, all of which reported a reduction in symptoms. The
review highlighted that VR-based CBT was not only effective
but also practical for clinicians, allowing for standardized de-
livery, repeatability, and increased patient engagement. The au-
thors concluded that VR shows strong potential for structured
mental health interventions, especially when it leverages im-
mersive interaction and controlled exposure through techniques
like VRET (Virtual Exposure Therapy).

Chitale et al. [68] presented a scoping review focused on
the use of both video games and VR for assessing anxiety and
depression. Out of 4566 records initially screened, 10 studies
were included, split evenly between VR and videogame-based
approaches. An important trend noted in the findings was that
studies on anxiety predominantly used VR, while those on de-
pression leaned toward traditional video games. A few studies
incorporated machine learning techniques, and only two were
clinical trials. Most studies yielded encouraging outcomes, sug-
gesting that both modalities could be useful tools for assess-
ment. However, the authors stressed the limited availability of
high-quality clinical evidence and recommended closer collab-
oration with mental health professionals to ensure safety and
privacy in future development.

Given their adaptability, conversational fluency, and capac-
ity for emotionally responsive interaction, LLMs hold strong
potential to enhance these therapeutic VR experiences, particu-
larly by supporting personalized narratives, mood-aware guid-
ance, and dynamic user engagement in mental health contexts

Social and Educational Inclusion. Beyond accessibility, LLMs
have shown potential in fostering inclusion for individuals with
diverse cognitive and learning needs. Liet al. implemented
LLM-based chatbots within VR job interview simulations de-
signed for autistic users. These virtual agents offered person-
alized, voice-based feedback in low-pressure, repeatable envi-
ronments. The structured yet flexible format helped users build
communication skills while maintaining a sense of psychologi-
cal safety, an essential aspect for neurodivergent learners navi-
gating real-world scenarios.

Similarly, Voultsiou et al. [12] explored the use of LLM-
powered assistants in VR learning environments tailored to stu-
dents with special educational needs, including autism. Their
findings indicate that AI-driven guidance enhanced learner en-


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

gagement and comprehension, especially when combined with
multimodal inputs like visual cues or simplified language.
However, they also observed that the current systems often
lack sufficient depth in personalization and struggle to main-
tain long-term contextual awareness, which limits their effec-
tiveness across extended educational sessions.

Additional studies on usability show that integrating natural
input modalities such as hand tracking further improves inter-
action quality. Geetha et al. and Krupka et al. empha-
size that users—particularly those unfamiliar with game con-
trollers—benefit from gesture-based systems that provide intu-
itive, real-time feedback. These affordances make VR more
approachable for a broader range of users, from children with
learning difficulties to older adults or those with motor impair-
ments.

Cultural and Contextual Usability. Cultural usability in VR
is gaining attention as a means to make virtual environments
more relatable and engaging for diverse user groups. LLMs,
with their capacity for dynamic language generation and con-
textual adaptation, are increasingly being used to enhance cul-
tural relevance in VR narratives. Lau et al., [34] explored this
in a Scottish curling game, where NPCs used culturally ap-
propriate language and expressions. Participants reported that
the familiar tone and regional references significantly improved
their sense of presence and emotional connection to the ex-
perience, demonstrating that localized dialogue—powered by
LLMs—can heighten user engagement in culturally specific
scenarios.

Similarly, Subandi et al. developed a VR shopping sim-
ulation designed to preserve and promote Indonesian textile
heritage. Users were guided by LLM-enabled NPCs through
the traditional Sasirangan fabric-making process. The agents
not only narrated historical context but also responded to ques-
tions, allowing for interactive exploration. This use of LLMs
for cultural storytelling helped users engage with intangible cul-
tural knowledge in a personalized and immersive manner, sug-
gesting new possibilities for cultural preservation and education
through interactive AI.

In broader educational and heritage applications, LLMs have
been used to power intelligent virtual tutors capable of deliv-
ering contextualized instruction. For instance, Ayre et al.,
created a GPT-4-based assistant for a virtual chemistry lab. This
tutor provided step-by-step instructions and real-time support
tailored to users’ actions, effectively acting as a dynamic guide.
Users reported increased understanding and autonomy, attribut-
ing it to the tutor’s ability to interpret the learning context and
offer personalized feedback.

Together, these studies show how LLMs enhance cultural and
contextual usability in VR by offering adaptive, linguistically
nuanced, and locally grounded interactions. This not only im-
proves accessibility for diverse populations but also enriches
the educational and emotional value of VR content.

LLMs improve accessibility, inclusivity, and usability in VR
through adaptive multimodal and context-aware interactions.
Users with sensory limitations benefit from LLMs because they
provide real-time audio guidance, adaptive dialogue, and intu-

10

How LLMs are Shaping the Future of Virtual Reality

itive interfaces that enhance VR navigation and responsiveness.
The application of LLMs shows great promise for therapeutic
interventions because they create emotionally responsive and
personalized VR scenarios that benefit patients undergoing anx-
iety, depression, and PTSD treatments.

The implementation of LLM-based assistants in educational
and social training environments has enhanced communication
abilities and learning outcomes and user confidence for autism
and cognitive difference users, while gesture-based inputs make
the system more accessible to new users. The implementation
of LLMs with cultural and contextual adaptations through lo-
calized narratives and intelligent tutoring systems demonstrates
their ability to enhance user engagement in heritage, educa-
tional, and commercial VR experiences. Future research needs
to resolve essential challenges, which include deep personal-
ization capabilities, sustained memory retention, and real-time
system performance limitations.

5. Challenges and Limitations

While LLMs offer transformative possibilities for VR games,
their integration introduces significant technical, ethical, and
usability challenges.

5.1. Computational and Performance Constraints

Although many studies explore how language models can en-
hance interactive systems, most have not yet been tested in real
immersive VR settings. Instead, evaluations are often done on
desktop platforms, where performance issues like latency, mo-
tion tracking, and multimodal input are less demanding. This
creates a gap in our understanding of how LLMs behave un-
der real-time, resource-intensive VR conditions, where delays
or instability can negatively impact user experience.

Several studies report that using LLMs in VR requires sub-
stantial computational resources. For example, Maslych et al.
found that even with local deployment and optimizations
like automatic speech recognition (ASR), text-to-speech (TTS),
and behavior-state modeling, response times averaged 3.2 sec-
onds—too slow for real-time interaction. Running LLMs lo-
cally instead of via cloud APIs helps reduce delay, while
behavior-state modeling, which defines agent states like listen-
ing or speaking, supports more synchronized interactions. Still,
these methods don’t fully solve latency issues in VR.

Jahangiri and Rahmani observed longer delays—over 20
seconds—in LLM-based NPC systems. They combined LLMs
with Pursuit Learning Automata (PLA) to address this, creating
a hybrid setup that reduced response time to under one second.
While promising, this approach still requires careful tuning and
is difficult to generalize across different VR environments.

Memory limitations are another critical barrier. As Zheng et
al. point out, LLMs struggle to maintain consistent con-
versations over time. Their MemoryRepository system mim-
ics human memory by summarizing past interactions, helping
sustain dialogue coherence. However, this adds processing de-
mands, which may not scale well in complex or multi-character
VR scenarios.


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

Making NPCs aware of their environment adds more com-
plexity. Radez and Bohak used image capture and semantic
segmentation to let NPCs reference objects and spaces around
them. While this improves realism, the real-time processing it
requires is complex to achieve on typical consumer VR hard-
ware, making widespread use difficult without sacrificing per-
formance.

Sissler [9] demonstrated improved NPC dialogue using GPT-
3.5 in Unity, but delays from REST API calls still reduced im-
mersion. The study recommends switching to stream-based
architectures for faster response. It also highlights the need
for expert prompt engineering to achieve natural conversations.
Significantly, while LLMs enhance language-based interac-
tions, key NPC behaviors—like movement and planning—still
depend on traditional scripting, limiting full autonomy.

LLMs offer rich linguistic and expressive capabilities for
VR, but their integration into real-time immersive environments
faces significant technical challenges, including latency, mem-
ory constraints, and computational overhead. Even with lo-
cal deployment and optimizations, current systems often fall
short of the responsiveness needed for seamless interaction,
with some reporting delays up to 20 seconds. Approaches
such as hybrid architectures, memory repositories, behavior-
state modeling, and stream-based communication offer partial
improvements, yet scalability remains limited. To bridge the
gap between expressive AI and immersive VR design, future
work should focus on lightweight models, edge computing, and
tighter integration with traditional game logic.

5.2. Ethical and Safety Considerations

Integrating LLMs into VR games has led to rapid progress
in interactive storytelling, emotional expression, and intelligent
gameplay. However, as these systems become more adaptive
and human-like, a critical question arises: Can we trust AI
agents that learn and respond to us in real time? While the po-
tential for immersive and personalized experiences is exciting,
it also brings serious ethical and safety concerns, particularly in
VR environments where users may build emotional bonds with
Al characters [74].

One primary concern is privacy. VR systems collect detailed
biometric and behavioral data, such as gaze, voice, movement
patterns, and emotional cues. Unlike traditional web apps,
this data is continuous and fine-grained. Garrido et al., [/75!
showed that just a few minutes of telemetry data, like eye track-
ing and EEG signals, can reveal private information such as a
user’s gender, income level, or emotional state. These findings
emphasize the need for stricter safeguards around data use in
immersive settings. One design solution might be to imple-
ment consent-aware logging mechanisms, which inform users
of what data is being collected and allow them to enable or dis-
able specific data tracking modalities.

In addition to privacy, LLM-generated content raises risks
of bias and misinformation. Yang et al., found that GPT-
based agents in mixed-initiative gameplay (MIG) can produce
biased or misleading stories, especially problematic in educa-
tional or therapeutic games. When LLMs are used without
proper moderation, they can unintentionally reinforce harmful

11

How LLMs are Shaping the Future of Virtual Reality

stereotypes or distort learning outcomes. Proactive bias miti-
gation can be addressed at the prompt level through controlled
prompt engineering and content filtering techniques tailored to
sensitive domains such as education or therapy.

Waghale et al., also warn that LLMs can introduce un-
fairness into gameplay, especially in multiplayer or competitive
environments. Bias in training data or algorithm design may
lead to advantages or disadvantages for specific player groups.
Procedural content generation using big data can unintention-
ally reinforce cultural or gender stereotypes. At the same time,
using sensitive data to personalize gameplay raises significant
privacy concerns.

Another challenge is the emotional impact of AI-driven em-
pathy systems. Tucek showed that emotionally respon-
sive digital characters behave unpredictably or generate inap-
propriate content; they can harm user trust or reinforce negative
perceptions, especially when the goal is to foster empathy to-
ward marginalized communities. To reduce user confusion or
mistrust, transparent AI feedback systems can be used—for in-
stance, by showing visual indicators when an NPC is adapting
its behavior in real time.

Tanksale |'7] adds that LLMs used in immersive Web3D envi-
ronments pose additional risks when combining real-time per-
sonalization with procedural generation. Without oversight,
these systems may create biased or culturally insensitive con-
tent, especially when trained on unfiltered internet data.

Finally, as Damianova emphasizes, ethical considera-
tions must be built into the design process, not added after
deployment. Developers should take responsibility for ethical
practices by integrating fairness, inclusivity, and safety princi-
ples throughout the design cycle.

These studies highlight the urgent need for responsible AI
design, content moderation, and strong privacy protections in
VR. Without clear ethical safeguards, the line between help-
ful personalization and harmful manipulation becomes danger-
ously thin.

As LLMs bring emotional intelligence and real-time respon-
siveness into VR games, they also introduce significant ethical
and safety risks. Studies consistently show that privacy con-
cerns, algorithmic bias, and unpredictable emotional impacts
are not hypothetical—they are already emerging in practice.
While adaptive AI systems enhance personalization, they also
risk reinforcing harmful stereotypes or manipulating user be-
havior without clear consent. Addressing these concerns re-
quires embedding fairness, transparency, and safety protocols
into every stage of design and deployment, particularly as im-
mersive AI interactions grow more lifelike and emotionally per-
suasive.

5.3. User Experience and Immersion Issues

Creating virtual characters that feel truly lifelike remains one
of the biggest challenges in VR game design, especially when
using LLMs for NPC dialogue. At the same time, these mod-
els can generate fluent and responsive language, which alone
does not guarantee engaging or believable interactions in im-
mersive environments. Research shows that the biggest obsta-


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

cles to user experience are unnatural reactions, inconsistent per-
sonality, limited conversational structure, and memory lapses
over time.

Maslych et al., [43] conducted a pilot study revealing low
realism scores (3.12 out of 7) for LLM-driven avatars in task-
based VR scenarios. The main issues were minimal animations,
limited to basic lip-sync and head movement, which broke im-
mersion. Participants noted that adding facial expressions, idle
behaviors, and body motion could improve believability. Visual
feedback cues, such as state lights and loading indicators, also
played a key role in maintaining user trust by signaling that the
avatar was actively listening or processing input.

Tonini’s international study on voice-based VR gameplay
highlighted similar issues. While players appreciated LLM-
powered NPCs’ emotional tone and responsiveness, they also
found conversations repetitive and sometimes generic. The lack
of dialogue variety and slow reaction times made interactions
feel scripted rather than natural. Players enjoyed the freedom
of open voice interaction, but the underlying AI often failed to
sustain flexible, emotionally rich conversations over time [33].

A mixed-reality study showed that human-like avatars signif-
icantly improved memory retention and immersion compared to
symbolic or abstract characters [38]. This suggests that avatar
design, specifically realism, expressiveness, and embodiment,
is critical for building emotional user connections. However,
delivering this level of engagement requires more than flu-
ent speech. Multimodal feedback, personality modeling, and
memory-aware systems must work together to create believable
and responsive virtual characters [78].

Narrative consistency is another primary concern, especially
in emergent gameplay. Peng et al., showed that while play-
ers can freely co-create stories with LLM-driven characters, this
often leads to fragmented or inconsistent plotlines. When sys-
tems fail to remember player actions or goals, the story can feel
disjointed and lose its emotional impact.

These findings show that while LLM-based NPCs can deliver
moments of intense immersion, they still struggle with sustain-
ing realistic, emotionally coherent, and context-aware interac-
tions. To address this, future work must improve animation,
clarity of feedback, narrative memory, and long-term emotional
engagement.

Another significant barrier to immersive experience in VR is
cybersickness—a form of motion-induced discomfort that af-
fects many users, particularly during fast-paced or unstructured
gameplay. It is usually like a physiological response marked as
nausea, disorientation, or dizziness. To address this issue, the
literature offers a variety of techniques designed to detect and
reduce the severity and frequency of cybersickness symptoms
[79].

Physiological signal analysis has shown great promise for
cybersickness detection. Islam et al. proposed a deep
learning-based method that uses heart rate, breathing rate, heart
rate variability, and galvanic skin response to automatically de-
tect and predict cybersickness severity. Their simplified CNN-
LSTM model achieved 97.44% accuracy for current state detec-
tion and 87.38% for predicting future symptoms, outperforming
traditional classifiers. This method provides a robust, real-time

12

How LLMs are Shaping the Future of Virtual Reality

solution by leveraging subtle physiological changes that corre-
late with user-reported discomfort.

In addition to physiological signal analysis approaches, Mon-
teiro et al. demonstrated that trajectory compression rate
can also be used as a marker to identify cybersickness during
VR gameplay. The authors found a clear correlation between
variations in compression rate and users’ Discomfort Scores, in-
dicating that changes in movement patterns—such as increased
rotation or erratic navigation—are linked to higher levels of
sickness. A simple neural network model using compression
rate and its variation as input was able to accurately predict
whether discomfort would increase or decrease over time.

Furthermore, Wang et al. presented a novel method for
predicting simulator sickness (SS) in real time using only in-
game character movement and eye-tracking data, without the
need for expensive or external physiological sensors. The au-
thors trained a long short-term memory (LSTM) neural network
on data collected from three VR games and achieved an SS pre-
diction accuracy of 83.4% for players with high sensitivity to
SS. Their findings support the hypothesis that intense character
motion and negative eye movement patterns are strong indica-
tors of SS in VR environments.

While LLM-powered NPCs enhance user interaction through
fluent dialogue and emotional tone, they often fall short in de-
livering sustained immersion due to limited animation, repeti-
tive responses, and poor narrative memory. Studies show that
visual feedback, avatar realism, and consistent personality cues
are essential for believable interactions. However, fragmented
storytelling and generic conversations remain common, espe-
cially over time. Achieving deeper engagement will require in-
tegrating expressive multimodal feedback, persistent memory,
and emotionally aware behavior into LLM-driven virtual char-
acters.

5.4. Scalability and Deployment

While many LLM-based prototypes in VR show promising
capabilities, scaling them for real-world, large-scale applica-
tions remains a significant challenge. Transitioning from con-
trolled lab settings to multiplayer or persistent virtual environ-
ments requires more than model performance—it demands ro-
bust infrastructure, cost-effective deployment, and compatibil-
ity with consumer hardware. As VR games become more com-
plex and interactive, these demands intensify.

One of the main bottlenecks is the high computational cost
of real-time LLM inference, especially in multi-agent settings
where several NPCs must perceive, reason, and respond simul-
taneously. Techniques like retrieval-augmented generation and
modular prompting aim to reduce memory load and latency, but
their effectiveness is limited in fast-paced, interactive environ-
ments [43}|60]. Multi-module systems like LLMR, while offer-
ing improved scene understanding, often face execution delays
due to orchestration overhead, including planning, debugging,
and memory updates [60].

Hybrid system designs offer one potential solution. For
example, combining LLMs with Pursuit Learning Automata
(PLA) allows agents to learn user tone preferences and se-
lect pre-generated responses instead of generating them from


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

scratch. This reduces processing load and supports smoother,
long-term dialogue [77| |40]. However, these methods are still
in early stages and have only been tested in limited, single-user
RPG setups, raising questions about scalability to multiplayer
environments.

From a deployment standpoint, technical hurdles also per-
sist. Many LLM-based VR systems rely on cloud APIs, lead-
ing to latency, privacy concerns, and service interruptions—all
of which affect real-time performance. Integrating LLMs into
engines like Unity often requires third-party middleware and
custom tools, increasing development time and limiting cross-
platform compatibility. While frameworks like SceneCraft
showcase the narrative potential of LLMs, they still need op-
timization for real-time, in-engine deployment [49].

Memory and model management are also critical. Persis-
tent agents must track long-term context, manage session mem-
ory, and coordinate with other agents. Current memory sys-
tems, such as those proposed by Buongiorno et al. and Zheng
et al. [39], improve dialogue continuity and increase sys-
tem resource demands. These systems become unsustainable in
larger, ongoing game worlds without efficient memory pruning,
modular retrieval, and compression strategies.

Ultimately, scaling LLMs for real-time VR applications will
require an integrated approach combining efficient local infer-
ence, modular design, optimized memory, and seamless engine
integration. LLM-powered VR games will unlikely move be-
yond experimental prototypes into mainstream, persistent vir-
tual worlds without these developments.

While LLMs demonstrate strong potential in VR game pro-
totypes, their deployment at scale faces major challenges. High
computational demands, latency from cloud APIs, and or-
chestration overhead limit real-time performance, especially in
multi-agent or multiplayer settings. Hybrid systems and mod-
ular memory frameworks offer partial solutions, but they re-
main largely untested in large-scale environments. To transi-
tion from experimental setups to persistent virtual worlds, fu-
ture systems must integrate lightweight inference, robust mem-
ory management, and native engine compatibility for seamless,
scalable deployment. Beyond these LLM-specific challenges,
users are also confronted with cybersickness—a more general
limitation of VR environments, and this physiological discom-
fort can severely impact immersion.

6. Future Directions

As the integration of LLMs into VR games continues to
evolve, several key areas emerge that will shape the next gener-
ation of immersive, intelligent, and ethically responsible digital
experiences. This section outlines promising future directions
based on current trends and gaps identified throughout this re-
view.

6.1. Advances in AI for More Realistic VR Gaming

One of the most promising developments lies in the evolu-
tion of multimodal AI models that integrate language, vision,

13

How LLMs are Shaping the Future of Virtual Reality

and audio understanding to support holistic, context-aware in-
teraction. Systems such as GPT-4V and LLaVA al-
ready demonstrate the capacity to interpret both images and
text, which, when integrated into VR environments, can lead to
NPCs that see” and “hear” alongside players. These systems
could track user gaze, interpret gestures, analyze visual scenes,
and generate emotionally resonant, non-verbal responses in real
time. Future advancements may enable avatars capable of full-
body interaction understanding, dynamic emotion simulation,
and persistent spatial memory across scenes, leading to virtual
characters that feel more genuinely human and socially intelli-
gent.

Furthermore, integrating LLMs with emotion recognition
and affective computing will likely enhance their capacity for
emotionally attuned interactions [84]. By incorporating
data from facial expression tracking, voice tone analysis, and
physiological inputs (e.g., heart rate, EEG), NPCs could dy-
namically respond to player moods, stress levels, or engage-
ment patterns, enabling deeper player-character connections,
particularly in therapeutic or educational applications [85].

6.2. Integration with Other Technologies

Beyond improved realism, future systems will benefit from
the convergence of LLMs with complementary AI technolo-
gies. For instance, reinforcement Learning (RL) can be used
to refine NPC behavior through iterative experience-based op-
timization, allowing characters to adapt over time and learn
from player interactions. With LLMs’ generative flexibility, RL
could support characters who evolve with players’ styles, form-
ing long-term bonds or gameplay strategies [86].

Procedural content generation (PCG) is another area where
LLMs can synergize with rule-based or stochastic systems to
produce personalized worlds, quests, and story arcs in real time.
Instead of replacing traditional PCG algorithms, LLMs can en-
rich them by filling narrative, dialogue, and context-sensitive
decision trees with fluid, naturalistic language.

The Internet of Things (IoT) and wearable technology also
offer integration potential for health-focused or location-aware
VR experiences [83]. For example, LLM-driven virtual
coaches could adapt content or difficulty in real-time based on a
player’s heart rate, body temperature, or environment, enhanc-
ing fitness, therapy, or safety applications. In a fitness game,
if a player’s heart rate exceeds a safe threshold, an LLM as-
sistant could reduce the workout intensity while explaining the
reasoning and suggesting hydration tips.

6.3. Ethical AI Development for Games

With increasing immersion and personalization come rising
ethical stakes. Future systems must embed ethical frameworks
directly into the design pipeline, prioritizing bias mitigation,
content moderation, explainability, and informed consent [88].
This is especially urgent as AI characters become more emo-
tionally responsive and persuasive, particularly among vulner-
able populations such as children or neurodivergent users.

It will be key to building transparent AI systems that can ex-
plain their decisions, avoid reinforcing stereotypes, and flag po-
tentially harmful outputs. This will also involve developing new


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

benchmarking tools and ethical evaluation protocols that assess
emotional impact, fairness, and long-term influence in game-
play, criteria often neglected in traditional usability testing. For
example, a narrative-based VR simulation for conflict resolu-
tion teaching would use AI characters who modify their com-
munication style and cultural references according to player
background and emotional state to provide respectful guidance
that avoids cultural or socioeconomic bias [89].

User data privacy must also be foregrounded. With VR sys-
tems collecting detailed biometric, motion, and interaction data,
LLM-powered applications must ensure secure handling, pre-
cise opt-in mechanisms, and compliance with emerging privacy
regulations [90]. Local processing or edge computing may re-
duce data transfer risks while enabling more responsive real-
time AI.

6.4. Open-Source and Industry Trends

The growing open-source ecosystem is accelerating the de-
mocratization of LLM development and deployment. Mod-
els like LLaMA, Mistral, and Baichuan offer competitive per-
formance and increased transparency, allowing smaller studios
and researchers to experiment with AI-driven game mechanics
without being locked into proprietary APIs. Open frameworks
like LangChain, Hugging Face Transformers, and Unity GPT
integrations also make it easier to develop modular, customiz-
able LLM agents tailored to specific gameplay scenarios [91].

In parallel, industry leaders like Meta, NVIDIA, and Ope-
nAI actively invest in AlI-native game engines and toolkits for
generative NPCs, suggesting that large-scale, real-time LLM
integration will soon become commercially viable. The release
of real-time streaming APIs, quantized model deployment so-
lutions, and multimodal interfaces will further reduce latency
and memory constraints, facilitating scalable use in complex
multi-agent virtual worlds.

Future research collaborations between academia and indus-
try will be essential to ensure that these tools remain innovative,
ethical, and inclusively designed. The open-source and com-
mercial sectors should align around shared principles of trans-
parency, accessibility, and responsible AI deployment.

7. Conclusion

This paper has examined the transformative role of Large
Language Models in reshaping the landscape of Virtual Real-
ity games. Through a comprehensive review of 62 recent stud-
ies, we analyzed the integration of LLMs across key domains:
dynamic NPC interactions, procedural storytelling, intelligent
game mastering, personalized experiences, accessibility design,
and performance-aware deployment.

Our findings indicate that LLMs significantly expand the de-
sign space for VR games, enabling more adaptive, expressive,
and emotionally resonant virtual characters. They support un-
scripted narrative branching, real-time user interaction, and so-
cially intelligent behavior that redefines immersion. Moreover,
LLMs open new pathways for inclusive and accessible VR sys-
tems, offering tailored experiences to diverse user populations,
including those with disabilities or language barriers.

14

How LLMs are Shaping the Future of Virtual Reality

However, significant challenges remain. Real-time respon-
siveness, memory management, and latency limit large-scale
deployment in complex VR scenarios. Ethical concerns—such
as bias, privacy, and emotional manipulation—are amplified by
the immersive nature of these experiences and require care-
ful consideration throughout the design and testing processes.
Scalability also presents a significant barrier as models must
be optimized for resource-constrained hardware and sustained
multi-agent interaction.

Future research should focus on advancing multimodal and
hybrid AI systems, integrating reinforcement learning, affective
computing, and spatial awareness. Developers and researchers
must prioritize ethical AI development by embedding fairness,
transparency, and safety into game mechanics and content gen-
eration pipelines. Collaboration across open-source commu-
nities and industry partners will be essential to make intelli-
gent, inclusive, and creative VR experiences more accessible
and impactful. By combining technical innovation with re-
sponsible design, LLMs have the potential to fundamentally
reshape how we build, experience, and interact within virtual
worlds—ushering in a new era of intelligent, human-centered
digital play.

Acknowledgment

This research was financially supported by the TUM Cam-
pus Heilbronn Incentive Fund 2024 of the Technical University
of Munich, TUM Campus Heilbronn. We gratefully acknowl-
edge their support, which provided the essential resources and
opportunities to conduct this study.

References

[1] K. Plupattanakit, P. Suntichaikul, P. Taveekitworachai, R. Thawonmas,
J. White, K. Sookhanaphibarn, W. Choensawat,
in: 2024 IEEE 13th
Global Conference on Consumer Electronics (GCCE), Institute of Electri-
cal and Electronics Engineers (IEEE), 2024, pp. 257-258./doi:10.1109/|
URL https ://doi.org/10.1108/G0CH62371.2024. 10760474
R. Wei, K. Li, J. Lan,
in: 2024 13th International Conference

on Educational and Information Technology (ICEIT), Institute of Elec-

trical and Electronics Engineers (IEEE), 2024, pp. 1-6. doi:10.1109/
ICEIT61397.2024.10540942

URL https: //doi.org/10.1109/ICEIT61397.2024.10540942

S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Am-

atriain, J. Gao, Large language models: A survey, arXiv preprint

arXiv:2402.06196 (2024).

URL|https:: //arxiv.org/abs/2402.06196

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
D. Amodei, in: Proceedings
of the 34th International Conference on Neural Information Processing
Systems (NeurIPS 2020), NeurIPS ’20, Curran Associates Inc., Red
Hook, NY, USA, 2020, pp. 159:1-159:25.

URL https: //proceedings.neurips.cc/paper_files/paper/
2020/file/1457c0d6bfcb4967418bf b8ac142f64a-Paper.pdf

[2]

[3]

[4]


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

[5]

[6]

[7]

[8]

[9]

[10]

(11)

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

C. Anthes, R. J. Garcia-Herndndez, M. Wiedemann, D. Kranzlmiiller,

State of the art of virtual reality technology) in: 2016 IEEE Aerospace
Conference, IEEE, 2016, pp. 1-19. doi: 10.1109/AERO.2016.7500674

URL https ://ieeexplore.iece.org/document/7500674
A. Alkhayat, B. Ciranni, R. S. Tumuluri, R. S. Tulasi, Leveraging large
language models for enhanced vr development: Insights and challenges,
in: 2024 IEEE Gaming, Entertainment, and Media Conference (GEM),
Institute of Electrical and Electronics Engineers (IEEE), 2024, pp. 1-6.
V. Tanksale, Leveraging large language models for web3d: Applications,
challenges, and future directions, in: 2023 International Conference on
Computational Science and Computational Intelligence (CSCD, Institute
of Electrical and Electronics Engineers (IEEE), 2023, pp. 254-259. |doi : |
P, Sweetser,
in: Proceedings of the 6th ACM Conference on Conver-
sational User Interfaces, CUI ’24, Association for Computing Machinery,
New York, NY, USA, 2024.
URL|https:://doi.org/10.1145/3640794.3665582
ACM Games 2 (3) (aug 2024).
URL|https://doi.org/10.1145/8662003
A. Normoyle, J. Sedoc, F. Durupinar,
in: 2024 IEEE Confer-

ence on Virtual Reality and 3D User Interfaces Abstracts and Workshops
(VRW), Institute of Electrical and Electronics Engineers (IEEE), 2024,

pp. 632-635. doi: 10.1109/VRW62533.2024.00124

URL https: //doi.org/10.1109/VRW62533.2024.00124
X. Peng, J. Quaye, S. Rao, W. Xu, P. Botchway, C. Brockett, N. Jojic,

G. DesGarennes, K. Lobb, M. Xu, J. Leandro, C. Jin, B. Dolan, |Player-|
in: 2024 IBEE Confer
ence on Games (CoG), Institute of Electrical and Electronics Engineers
(IEEE), 2024, pp. 1-8.
URL https: //doi.org/10.1109/CoG60054.2024. 10645607
E. Voultsiou, L. Moussiades,

applications in special education: Opportunities, challenges, and fu-
Education and Information Technologies (2025).
10.1007/s10639-025- 13550-4
URL https: //doi.org/10.1007/810639-025- 18550-4
E. Bozkir, S. Ozdel, K. H. C. Lau, M. Wang, H. Gao, E. Kasneci,
bedding large language models into extended reality: Opportunities and
the ACM Conversational User Interfaces (CUI ’24), CUI ’24, Associ-
ation for Computing Machinery, New York, NY, USA, 2024, pp. 1-7.
URL https: //doi.org/10.1145/8640704, 3665563

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever,Language|
Open Blog 1 (8) (2019).
URL

language_models_are_unsupervised_multitask_learners.pdf
R. Gallotta, G. Todd, M. Zammit, S. Earle, A. Liapis, J. Togelius,

G. N. Yannakakis, Large language models and games: A survey and
roadmap, IEEE Transactions on Games (2024) 1-1§doi : 10.1109/|
F. Du, X.-J. Ma, J.-R. Yang, Y. Liu, C.-R. Luo, X.-B. Wang, H.-O. Jiang,
X. fing survey of Im datases: From autoregressive model toa cha
Journal of Computer Science and Technology 39 (3) (2024) 542-566.
URL https: //doi.org/10.1007/s11300-024-8767-8

Z. Wang, Z. Chu, T. V. Doan, S. Ni, M. Yang, W. Zhang,
development, and principles of large language models: An introduc-
AI and EthicsPublished: 14 October 2024 (2024).
10.1007/s43681-024-00583-7
URL https ://doi.org/10.1007/843681-024-00583-7
M. Zong, B. Krishnamachari, (2022).
2212.00857

URL https: //arxiv.org/abs/2212.00857

J. Hauser, D. Kondor, J. Reddish, M. Benam, E. Cioni, F. Villa,
J. S. Bennett, D. Hoyer, P. Francois, P. Turchin, R. M. del Rio-

Chanona, |Large language models' expert-level global history knowledge

benchmark (hist-IIm), in: A. Globerson, L. Mackey, D. Belgrave,
A. Fan, U. Paquet, J. Tomczak, C. Zhang (Eds.), Advances in

15

How LLMs are Shaping the Future of Virtual Reality

Neural Information Processing Systems, Vol. 37, Curran Asso-
ciates, Inc., 2024, pp. 32336-32369, dataset and results available at

URL
J. Devlin, M.-W. Chang, K. Lee, K. Toutanova,
J. Burstein, C. Doran, T. Solorio (Eds.), Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), Association for Computational Linguistics, Minneapolis,

Minnesota, 2019, pp. 4171-4186. doi: 10.18653/v1/N19- 1423
URL https: //aclanthology.org/N19-1423/

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,

A. Joulin, E. Grave, G. Lample, Llama: Open and efficient foundation
language models . arXiv:2302.13971
URL https: //arxiv.org/abs/2302.13971

A. Singh, Exploring language models: A comprehensive survey and
analysis, in: 2023 International Conference on Research Methodolo-
gies in Knowledge Management, Artificial Intelligence and Telecom-

munication Engineering (RMKMATE), 2023, pp. 1-4. doi:10.1109/

A 9243.20 03694

[23] O. etal.,,Gpt-4 technical report openAI Technical Report (2024). arXiv: |
2303.08774
URL https: //arxiv.org/abs/2303.08774

H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, in: Proceedings
of the 37th International Conference on Neural Information Processing
Systems, NIPS ’23, Curran Associates Inc., Red Hook, NY, USA, 2023.
D. D. Adhikary, A. Maheta, Origin of gaming in virtual reality, Inter-
national Journal of Recent Trends in Engineering & Research (IJRTER)
3 (4) (2017) 154-160.

Q. Zhu, T. Wang, Y. Jia, Second life: A new platform for education,
in: 2007 First IEEE International Symposium on Information Technolo-
gies and Applications in Education, 2007, pp. 201-204.
GeeksforGeeks, Introduction of 3d __ internet,
accessed: 2025-03-24 (n.d.).

S. Berrezueta-Guzman, A. Koshelev, S. Wagner, From reality to virtual
worlds: The role of photogrammetry in game development, arXiv preprint
arXiv:2505.16951 (2025).

I. J. LaViola Ir,
SIGCHI Bulletin 32 (1) (2000) 47-56.
URL https ://doi.org/10.1145/83329.833344

N. Damianova, S. Berrezueta-Guzman, Serious games supported by vir-
tual reality—literature review, IEEE Access 13 (2025) 38548-38561.
Y. Song, K. Wa, J. Ding,
Computers & Education: X Reality 4
(2024) 100069. doi: 10.1016/j.cexr.2024.100069
URL
$2949678024000199
A. Rychert, M. L. Ganuza, M. N. Selzer, Integrating gpt as an as-
sistant for low-cost virtual reality escape-room games, IEEE Com-
puter Graphics and Applications 44 (4) (2024) 14-19. [doi:10.1109/7)
L. Tonini, “talk to me, hal”: A study of player experience and interaction
in a voice interaction vr game featuring ai-driven non-player characters,
Master’s thesis, University of Twente, Enschede, The Netherlands, super-
visors: Dr. Max A. Friehs, Prof. Dr. Sven Zebel (2024).

K. H. C. Lau, E. Bozkir, H. Gao, E. Kasneci,
URL
B. Bateni, J. Whitehead,
in: Proceedings of the 19th

International Conference on the Foundations of Digital Games, FDG
°24, Association for Computing Machinery, New York, NY, USA, 2024.

[20]

[21]

[22]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]


S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

URL https: //doi.org/10.1145/8649921,3650013

A. Marincioni, M. Miltiadous, K. Zacharia, R. Heemskerk, G. Doukeris,
in: 2024 IEEE
Conference on Games (CoG), Institute of Electrical and Electronics Engi-
neers (IEEE), 2024, pp. 1-6.
URL|ht tps: //doi.org/10.1109/CoG60054.2024. 10645631)
in: 2021 Ist International Conference on Com-

puter Science and Artificial Intelligence ICCSAD), Institute of Electrical
and Electronics Engineers (IEEE), Jakarta, Indonesia, 2021, pp. 418-421.
URL
J. Zhu, R. Kumaran, C. Xu, T. Hollerer,
in: 2023 IEEE Interna-

tional Symposium on Mixed and Augmented Reality (ISMAR), Insti-
tute of Electrical and Electronics Engineers (IEEE), 2023, pp. 751-760.
URL https: //doi.org/10.1109/TSMARB9233.2023.00090
S. Zheng, K. He, L. Yang, J. Xiong, IEEE
Access 12 (2024) 62581-62596.
URL https ://doi.org/10.1109/ACCESS.2024.8893485
M. M. Jahangiri, P. Rahmani,

in: 2024 International Congress on Human-Computer Interac-
tion, Optimization and Robotic Applications (HORA), Institute of Elec-
trical and Electronics Engineers (IEEE), 2024, pp. 1-6.
URL https: //doi.org/10.1109/HORAG1326.2024, 10550450
Proc. ACM Hum.-Comput. In
teract. 8 (CHI PLAY) (oct 2024).
URL https: //doi.org/10.1145/3677098

E. Krupka, K. Karmon, N. Bloom, D. Freedman, I. Gurvich, A. Hurvitz,
I. Leichter, Y. Smolin, Y. Tzairi, A. Vinnikov, A. Bar-Hillel,
in: Proceedings of the 2017 CHI Conference on Human Fac-
tors in Computing Systems, CHI ’17, Association for Computing Ma-
chinery, New York, NY, USA, 2017, pp. 1887-1898.
3025453.3025508

URL https: //doi.org/10.1146/3025463 3025608

M. Maslych, C. Pumarada, A. Ghasemaghaei, J. J. LaViola Jr.,
from ans Ilm capabilities to multiple conversational avatars in a vr

y (2025). arXiv: 2501.00168
nttps: //arxiv.org/abs/2501.00168

. ee L. Klinkert, Z. Zhaung, T. Chawla, C. Clark, Pangea:
Procedural artificial narrative using generative ai for turn-based, role-
playing video games, Vol. 20, 2024, p. 156 — 166. doi:10.1609/
S. Vartinen, P. Haimilainen, C. Guckelsberger, |Generating role-playing]
IEEE Transactions on Games
16 (1) (2024) 127-139.
URL https: //doi.org/10.1109/T¢.2022.3228480
T. Ashby, B. K. Webb, G. Knapp, J. Searle, N. Fulda,
in: Proceedings of the 2023 CHI

Conference on Human Factors in Computing Systems, CHI ’23, Asso-
ciation for Computing Machinery, New York, NY, USA, 2023.
10.1145/3544548.3581441

URL https: //doi.org/10.1145/3544548.3581441
L. Bingli, D. V. Vargas, |Towards immersive computational story-

telling: Card-framework for enhanced persona-driven dialogues) IEEE
Transactions on Games (2024) 1-13Early Access. doi:10.1109/

URL https: //doi.org/10.1108/16.2004 3466808

A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan,
D. Wang, D. Yan, F. Yang, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong,
H. Zhao, H. Xu, H. Sun, H. Zhang, H. Liu, J. Ji, J. Xie, J. Dai, K. Fang,
L. Su, L. Song, L. Liu, L. Ru, L. Ma, M. Wang, M. Liu, M. Lin, N. Nie,

P. Guo, R. Sun, T. Zhang, T. Li, T. Li, W. Cheng, W. Chen, X. Zeng,
X. Wang, X. Chen, X. Men, X. Yu, X. Pan, Y. Shen, Y. Wang, Y. Li,

[49]

[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

[59]

[60]

How LLMs are Shaping the Future of Virtual Reality

Y. Jiang, Y. Gao, Y. Zhang, Z. Zhou, Z. Wu,
.
URL https : //arxiv.org/abs/2309.10305
V. Kumaran, J. Rowe, B. Mott, J. Lester,

in: Proceedings of the Nineteenth AAAI Conference on Artificial In-
telligence and Interactive Digital Entertainment, AIIDE ’23, AAAI Press,
2023.

URL
G. RadeZ, C. Bohak,

the Human-Computer Interaction Slovenia 2024 (HCI-SI 2024), CEUR
Workshop Proceedings, 2024, ljubljana, Slovenia.

URL https ://ceur-ws.org/Vol-3866/paper2.paf|
Z. Li, H. Zhang, C. Peng, R. Peiris,
in: 2025 IEEE Conference
on Virtual Reality and 3D User Interfaces (VR), Institute of Electri-
cal and Electronics Engineers (IEEE), 2025, pp. 1-11.
URL https ://doi.org/10.1109/VR59515.2025.00025
in: Proceedings of the 14th International Confer-

ence on the Foundations of Digital Games, FDG °19, Association for
Computing Machinery, New York, NY, USA, 2019. doi:10.1145/
3337722.3341865

URL https: //doi.org/10.1145/3337722.3341865
T. Triyason, |Exploring the potential of chatgpt as a dungeon master in
dungeons & dragons tabletop game) in: Proceedings of the 13th In-

ternational Conference on Advances in Information Technology, IAIT
°23, Association for Computing Machinery, New York, NY, USA, 2023.
URL|https: //doi.org/10.1145/3628454,3628457
J. Kelly, M. Mateas, N. Wardrip-Fruin,
in: Proceedings of the 18th

International Conference on the Foundations of Digital Games, FDG ’23,
Association for Computing Machinery, New York, NY, USA, 2023./doi:)
URL
J. Song, A. Zhu, C. Callison-Burch,

X. You, P. Taveekitworachai, S. Chen, M. C. Gursesli, X. Li, Y. Xia,

R. Thawonmas, Dungeons, dragons, and emotions: A preliminary study
of player sentiment in llm-driven ttrpgs| in: Proceedings of the 19th

International Conference on the Foundations of Digital Games, FDG
°24, Association for Computing Machinery, New York, NY, USA, 2024.
URL https: //doi.org/10.1145/8649921, 3656901
A. Rychert, M. L. Ganuza, M. N. Selzer,

sistant for low-cost virtual reality escape-room games| IEEE Com-
puter Graphics and Applications 44 (4) (2024) 14-25. doi:10.1109/

MCG.2024.3426314

URL https: //doi.org/10.1109/MCG.2024.3426314
M. Z. Khan, F. Hassan, M. Usman, U. Ansari, S. Noor, | Virtual reality
in multiplayer carrom game with artificial intelligence) in: 2018 12th

International Conference on Mathematics, Actuarial Science, Computer
Science and Statistics (MACS), Institute of Electrical and Electronics En-

gineers (IEEE), 2018, pp. 1-5.
URL https: //doi.org/10.1109/MACS.2018.8628394
H. Li, P. Yi, D. Wei, W. Bai, Seck-and-take games of heterogeneous agent
in: 2024 China Automation Congress
(CAC), Institute of Electrical and Electronics Engineers (IEEE), 2024, pp.
70781084, doi : 10.1109 /CAC63892.2024, 10864895)

URL https: //doi.org/10.1109/CAC63892,2024, 10864895

F. De La Torre, C. M. Fang, H. Huang, A. Banburski-Fahey,
J. Amores Fernandez, J. Lanier,
in: Proceedings of the 2024

CHI Conference on Human Factors in Computing Systems, CHI ’24, As-
sociation for Computing Machinery, New York, NY, USA, 2024.



S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

10.1145/3613904.3642579

URL https: //doi.org/10.1145/3613904.3642579
X.-K. Lin, C.-W. Shiu, N.-Y. Pai, Application of chatgpt-integrated npcs
to enhance virtual reality brainstorming in: 2024 IEEE 13th Global Con-

ference on Consumer Electronics (GCCE), Institute of Electrical and
Electronics Engineers (IEEE), 2024, pp. 1271-1272. doi:10.1109/
GCCE62371.2024.10760995

URL https: //doi.org/10.1109/GCCE62371.2024.10760995
T. Tucek, Enhancing empathy through personalized ai-driven experiences
and conversations with digital humans in video games in: Compan-

ion Proceedings of the 2024 Annual Symposium on Computer-Human
Interaction in Play, CHI PLAY Companion ’24, Association for Com-
puting Machinery, New York, NY, USA, 2024, pp. 446-449.
10.1145/3665463.3678856

URL https: //doi.org/10.1145/3665463.3678856
Z. Guo, W. Xu, J. Zhang, H. Wang, C.-H. Lo, H.-N. Liang,

ing me?: Exploring the impact of audience familiarity on player perfor-

[61]

[62]

[63]

mance, experience, and exertion in virtual reality exergames) in: 2023

IEEE International Symposium on Mixed and Augmented Reality (IS-
MAR), Institute of Electrical and Electronics Engineers (IEEE), 2023,

pp. 622-631. doi: 10.1109/1SMAR59233.2023.00077
URL https: //doi.org/10.1109/ISMAR59233.2023.00077

J. Chen, R. P. Galindo Esparza, V. Garaj, P. O. Kristensson, J. Dudley,
Envisionvr: A scene interpretation tool for visual accessibility in virtual

arXiv preprint (2025). arXiv :2502.03564

URL https: //arxiv.org/abs/2502.03564
N. Baghaei, L. Stemmet, A. Hlasnik, K. Emanov, S. Hach, J. A. Naslund,

M. Billinghurst, I. Khaliq, H.-N. Liang, Time to get personal: Indi-
in: Extended Abstracts of
the 2020 CHI Conference on Human Factors in Computing Systems,
Association for Computing Machinery, 2020, pp. 1-9.
3334480.3382932

URL https: //doi.org/10.1145/3834480 3382932

C. J. Falconer, A. Rovira, J. A. King, P. Gilbert, A. Antley, P. Fearon,
N. Ralph, M. Slater, C. R. Brewin,

virtual reality and its effects on patients with depression) BJPsych Open
2 (1) (2016) 74-80, pMID: 27703757, PMCID: PMC4995586.

10.1192/bjpo.bp.115.002147

URL https: //doi.org/10.1192/bjpo.bp.115.002147

[64]

[65]

[66]

[67]

N. Baghaei, V. Chitale, A. Hlasnik, L. Stemmet, H.-N. Liang, R. Porter,
Virtual reality for supporting the treatment of depression and anxiety:

URL neeps://d0i.org/10-2196/2968i)

V. Chitale, N. Baghaei, D. Playne, H.-N. Liang, Y. Zhao, A. Erensoy,
Y. Ahmad,
Games for Health Journal
11 (6) (2022) 341-354. doi: 10.1089/g4h.2021.0227
URL https: //doi.org/10.1089/g4h.2021.0227
S. Berrezueta-Guzman, W. Chen, S. Wagner, A therapeutic role-
playing vr game for children with intellectual disabilities, arXiv preprint
arXiv:2507.19114 (2025).

Z. Li, P. P. Babar, M. Barry, R. L. Peiris,

in job communication skills, in: Extended Abstracts of the 2024 CHI
Conference on Human Factors in Computing Systems, CHI EA ’24, As-

sociation for Computing Machinery, New York, NY, USA, 2024, pp. 1-7.
URL https: //doi.org/10.1145/8613005.3651996
S. Geetha, G. Aditya, C. M. Reddy, G. Nischith,
in: 2024 TEE Invern
tional Conference on Electronics, Computing and Communication Tech-
nologies (CONECCT), Institute of Electrical and Electronics Engineers

(IEEE), 2024, pp. 1-6. |doi : 10.1109/CONECCT62155.2024.10677239

URL https: //doi.org/10.1109/CONECCT62155.2024.10677239
Subandi, A. A. Syahidi, S. Zakiah, K. Kiyokawa, A. Riyadi, M. H. Noor,

Sasirangan cloth recognition and shopping experience simulation based
in: 2022 8th
International HCI and UX Conference in Indonesia (CHIuXiD), Institute
of Electrical and Electronics Engineers (IEEE), 2022, pp. 53-58.
URL https: //doi.org/10.1109/CHTuXiD57244 2022. 10009705

JMIR Mental Health 8 (9) (2021) e29681.

[68]

[69]

[70]

[71]

[72]

17

[73]

[74]

[75]

[76]

[77]

[78]

[79]

[80]

[81]

[82]

[83]

[84]

[85]

[86]

How LLMs are Shaping the Future of Virtual Reality

Implementation of an Artificial Intelligence (AI) Instructional Sup-

port System in a Virtual Reality (VR) Thermal-Fluids Laboratory
Vol. Volume 8: Engineering Education of ASME International Me-

chanical Engineering Congress and Exposition.

//asmedigitalcollection.asme.org/IMECE/proceedings-
pdf / IMECE2023/87653/V008T09A029/7239730/v008t09a029-
imece2023-112683.pdf | doi:10.1115/IMECE2023- 112683

URL https: //doi.org/10.1115/IMECE2023- 112683
O. Sobchyshak, S. Berrezueta-Guzman, S. Wagner, Pushing the bound-

aries of immersion and storytelling: A technical review of unreal engine,
arXiv preprint arXiv:2507.08142 (2025).

G. Munilla Garrido, V. Nair, D. Song,|Sok: Data privacy in virtual reality]
Proceedings on Privacy Enhancing Technologies 2024 (1) (2023) 21-40.
URL
D. Yang, E. Kleinman, C. Harteveld,

A. Waghale, N. Potdukhe, R. Rewatkar, Ai in gaming: From sim-
ple algorithms to complex agents) in: 2024 2nd DMIHER Inter-

national Conference on Artificial Intelligence in Healthcare, Educa-

tion and Industry (IDICAIEI), IEEE, 2024, pp. 1-5. doi:10.1109/
IDICATEI61867.2024.10842756

URL https: //doi.org/10.1109/IDIGATET61867.2024. 10842756
L.A. Brito, J. S. Dollis, F. B. Farber, P. S. F. B. Ribeiro, R. T. Sousa, A. R.
Galvio Filho,
lim-riven approaches for viral reality(2025),
URL /arsavorg/ab/ 2805 16467,
S. Ang, J. Quarles,
Virtual Reality 4 (2023). doi: 10.3389/frvir.2023.1027552
R. Islam, Y. Lee, M. Jaloli, I. Muhammad, D. Zhu, P. Rad, Y. Huang,

J. Quarles, Automatic detection and prediction of cybersickness severity

using deep neural networks from user’s physiological signals, in: Pro-
ceedings of the 2020 IEEE International Symposium on Mixed and Aug-

mented Reality (ISMAR), IEEE, 2020, pp. 400-411. doi:10.1109/
URL https ://doi.org/10.1109/TSMARB0242.2020.00066
D. Monteiro, H.-N. Liang, X. Tang, P. Irani,

in: Proceedings of the 2021 IEEE International Symposium on Mixed
and Augmented Reality (SMAR), IEEE, 2021, pp. 138-146.
URL https ://doi.org/10.1109/ISMAR52148.2021,00028)
J. Wang, H.-N. Liang, D. Monteiro, W. Xu, J. Xiao,
Games 15 (2) (2023) 252-261.
URL|https ://doi.org/10.1109/T6.2022.3178539

J. Marin-Morales, J. L. Higuera-Trujillo, A. Greco, J. Guixeres,
C. Llinares, E. P. Scilingo, M. Alcaftiz, G. Valenza,

Scientific Reports 8 (1) (2018) 13657.
URL https: //doi.org/10.1038/s41598-018-32063-4
D. Harris, T. Arthur, M. Wilson, S. Vine,
2023 1th

International Conference on Affective Computing and Intelligent In-
teraction Workshops and Demos (ACIIW) (2023) 1-€doi:10.1109/
ACIIW59127.2023.10388102

URL https : //doi.org/10.1109/ACTIW59127.2023.10388102
G. Pei, Q. Shang, S. Hua, T. Li, J. Jin,
Computers in Human Behavior 152 (C) (2024) 108085.
URL ht tps: //doi.org/10.1016/ .chb.2023. 108085
N. Justesen, P. Bontrager, J. Togelius, S. Risi,
IEEE Transactions on Games 11 (2) (2019) 1-20.
URL https ://doi.org/10.1109/TG.2018.2881352



S. Ozkaya, S Berrezueta-Guzman, S. Wagner.

[87] Z. Ly, J. Lloret Mauri, H. Song, J. Wang, |Digital twins: The confluence|
IEEE Consumer Electronics Magazine 12 (6)
(2023) 27-28.
URL ht tps: //doi.org/10.1109/NCE.2023.3296868

[88] H. Vainio-Pekka, M. O.-O. Agbese, M. Jantunen, V. Vakkuri, T. Mikko-

nen, R. Rousi, P. Abrahamsson, |The role of explainable ai in the research]
ACM Transactions on Interactive Intelligent Systems
13 (4) (2023) 26:1-26:39.
URL https: //doi.org/10.1145/8599974
[89] A. Giaretta Security and privacy in virtual reality: a literature survey] Vir-
tual Reality 29 (10) (2025) 1-32.

18

[90]

cia)

- clo

How LLMs are Shaping the Future of Virtual Reality

RL https: //doi.org/10.1007/s10055-024-01079-9

Natgunanathan, A. Mehmood, Y. Xiang, G. Beliakov, J. Yearwood,

rotection of privacy in biometric data, IEEE Access 4 (2016) 880-892.
0i:10.1109/ACCESS.2016.2535120

RL https ://doi.org/10.1109/ACCESS.2016.2535120

[91] S. Hu, T. Huang, G. Liu, R. R. Kompella, F. Ilhan, S. F Tekin, Y. Xu,

N

U

. Yahn, L. Liu, A survey on large language model-based game agents

arXiv preprint arXiv:2404.02039 (2025).

RL https://arxiv.org/abs/2404.02039