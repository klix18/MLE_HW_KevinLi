arXiv:2508.00766v1 [cs.CV] 1 Aug 2025

Sample-Aware Test-Time Adaptation for Medical
Image-to-Image ‘Translation

Irene Iele!*, Francesco Di Feola!*, Valerio Guarrasi®, Paolo Soda?”
? ? ?

*Unit of Artificial Intelligence and Computer Systems, Department of Engineering,
Universita Campus Bio-Medico di Roma, Via Alvaro del Portillo, 21, Rome, 00128, Italy
’Department of Diagnostics and Intervention, Radiation Physics, Biomedical
Engineering, Umea University, Umeda, 901 87, Sweden

Abstract

Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality con-
version. However, it suffers from limitations in handling out-of-distribution
samples without causing performance degradation. To address this limi-
tation, we propose a novel Test-Time Adaptation (TTA) framework that
dynamically adjusts the translation process based on the characteristics
of each test sample. Our method introduces a Reconstruction Module to
quantify the domain shift and a Dynamic Adaptation Block that selectively
modifies the internal features of a pretrained translation model to mitigate
the shift without compromising the performance on in-distribution samples
that do not require adaptation. We evaluate our approach on two med-
ical image-to-image translation tasks: low-dose CT denoising and 7 to
T> MRI translation, showing consistent improvements over both the base-
line translation model without TTA and prior TTA methods. Our analysis
highlights the limitations of the state-of-the-art that uniformly apply the
adaptation to both out-of-distribution and in-distribution samples, demon-
strating that dynamic, sample-specific adjustment offers a promising path to
improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA

*Corresponding author.

Email addresses: irene.iele@unicampus.it (Irene Iele’),
francesco.feola@umu.se (Francesco Di Feola'), valerio. guarrasi@unicampus. it
(Valerio Guarrasi), p.soda@unicampus.it, paolo.soda@umu.se (Paolo Soda)

'These authors contributed equally to this work.

Preprint submitted to Medical Image Analysis August 4, 2025


Keywords: Test Time Adaptation, Medical Image Translation, Domain
Shift, Medical Imaging

1. Introduction

Image-to-image translation converts an image from a source domain X
to a target domain Y [1], and has found numerous applications across di-
verse fields, particularly in medical imaging [2], supporting different imaging
modalities and key tasks such as noise reduction, image synthesis and super-
resolution. In clinical settings, image-to-image translation may offer signifi-
cant time and cost benefits, as it can reduce the need for repeated imaging
exams on the same patient while minimizing exposure to high-dose radiation.
As a data-driven method, image-to-image translation is sensitive to distri-
bution shift, which arises when the training and testing distributions differ
due to variations in imaging systems, acquisition protocols, or anatomical
structures [3]. Such shifts lead to out-of-distribution (OOD) data, samples
that deviate from the training distribution, ultimately degrading model per-
formance, limiting generalization, and reducing the quality of translated im-
ages. To address this challenge, Test-Time Adaptation (TTA) has emerged
as a strategy to adapt pretrained models to unseen data during inference
without requiring retraining or access to large datasets, improving robust-
ness in real-world scenarios, where institutions often lack the computational
resources or data volume required to train models from scratch. For instance,
TTA methods modify the input data, adjust intermediate feature represen-
tations, or update the model’s weights at inference time [4]. Despite recent
advancements in this field, most existing methods apply adaptation strategies
to all samples during inference [5], whether they are OOD or in-distribution
(ID) samples, that is data drawn from the same distribution as the training
set. However, this is a main limitation because TTA is currently designed to
improve performance on OOD samples: when applied to ID data, it may dis-
rupt the model’s optimal configuration and compromise its generalization |6].
Therefore, striking the right balance between adapting to new data and pre-
serving the original knowledge of the model is crucial to achieve consistent
and reliable performance across diverse data distributions. Moreover, while
most studies have focused on TTA for predictive tasks [7, 8, 9, 10, 11, 12, 13],
its application in the generative domain remains largely unexplored.

On these grounds, we hereby propose a sample-aware TTA method for
medical image-to-image translation that dynamically adapts a pretrained


translation model to OOD data while preserving its performance on ID data.
To this end, we introduce a trainable reconstruction module that quantifies
the degree of domain shift for each test input, and based on the estimated
shift, an adaptation module applies feature-level transformations at multiple
stages of the translation model, adjusting to the specific characteristics of
each test sample. We validate our approach through extensive experiments
on two distinct tasks: Low Dose CT denoising (LDCT) and T; to T2 MRI
translation.
Hence, our contributions can be summarized as follows:

e We propose a sample-aware TTA method that adapts a pre-trained
image-to-image translation model to OOD samples, effectively narrow-
ing the performance gap between ID and OOD samples while preserving
ID performance.

e We design our TTA approach to be dynamic, i.e., to be able to tailor
the adaptation process to each OOD test sample and a strategy that
allows maximizing performance across varying degrees of distribution

shift.

e We conduct extensive experiments to demonstrate the effectiveness of
the proposed approach on two distinct medical image-to-image trans-
lation tasks, highlighting the task-agnostic nature of our method.

The rest of the paper is organized as follows: section 2 reviews related works
on TTA, section 3 presents the methods, section 4 describes the experimental
configuration, section 5 presents and discusses the results, whereas section 6
offers concluding remarks.

2. Related Works

TTA has emerged as a promising strategy to improve model robustness
under distribution shifts [4]. Early efforts in this direction date back to 2011,
when Jain and Learned-Miller [14] investigated the use of Gaussian process
regression to adapt a cascade of classifiers at inference time. Since then,
a variety of strategies have been proposed to adapt deep neural networks
without requiring access to the training data or full model retraining [4, 15].

TTA is particularly relevant in medical imaging, where shifts frequently
stem from differences in acquisition protocols, patient populations, and imag-
ing devices. Despite that, recent studies have primarily focused on using TTA


to mitigate performance degradation in predictive tasks such as medical im-
age classification [7, 8], and segmentation [9, 10, 11, 12, 13]. For example,
Ma et al. [7] proposed a TTA strategy to address label distribution shifts by
training multiple classifiers, each specialized for a class-dominated distribu-
tion. At test time, the outputs of these classifiers are dynamically combined
and adapted to the test label distribution, using a consistency regularization
loss to guide and calibrate their relative contributions. Yang et al. [8] in-
troduced a method that dynamically adjusts the learning rate for each test
sample based on the estimated distribution shift, enabling stable adaptation
across both medical image classification and segmentation tasks.

In the context of medical semantic segmentation, Karani et al. [9] pro-
posed input level adaptation by dynamically optimizing a shallow normal-
ization network during inference. This network projects each test sample
into a normalized space, guided by a denoising autoencoder prior trained to
identify and correct implausible segmentations. Building on this approach,
Valvano et al. [11] replaced the autoencoder prior with a discriminator net-
work, trained to identify and provide corrective feedback to the normalization
network at test time. However, input-level adaptation alone may be insuf-
ficient to fully address complex domain shifts, as it cannot adapt deeper,
task-relevant features. To overcome this limitation, Li et al. [10] extended
TTA to both the input and feature levels, using a translation network and
a multi-task segmentation network guided by an autoencoder-based recon-
struction loss. Another line of research focuses on test-time training using
self-supervision [16]. Instead of adapting intermediate features directly, the
model weights are updated during inference guided by pre-text tasks such as
image inpainting [17] or rotation prediction [18]. Building on this idea, Wen
et al. [12] proposed an approach for CT image segmentation that employs
a Y-shaped architecture based on a U-Net backbone. The model includes a
self-supervised denoising decoder that shares skip connections with the main
segmentation branch. It is jointly trained on segmentation and denoising
tasks, while at inference time, only the encoder’s batch normalization layers
are adapted using the denoising loss. Similarly, Hu et al. [13] proposed a
TTA method that updates only batch normalization parameters during in-
ference, guided by two loss terms: one that promotes confident and diverse
predictions across local image regions, and a contour regularization loss that
encourages smooth boundaries to enhance spatial consistency in the seg-
mentation outputs. While TTA has been extensively studied for predictive
tasks, its application to medical image-to-image translation has, to the best

4


of our knowledge, been explored only in [19]. In this work, similarly to [9],
He et al. [19] proposed a self-supervised TTA approach for T, to T, MRI
translation that performs feature-level adaptation by using shallow adaptor
networks, guided by an autoencoder-based reconstruction loss designed to
capture domain shift. However, the adaptation is applied uniformly across
all test samples, without distinguishing between ID and OOD cases, which is
a key limitation of the current literature. Furthermore, the method employs
a fixed set of adaptors placed at both image and feature levels, inserted at
predetermined locations within the task model, without accounting for the
specific characteristics of each test sample, that can result in unnecessary
transformations or suboptimal adaptation outcomes.

The review of the works presented in this section highlights the growing
interest in TTA across various predictive domains. However, these methods
have seen limited application in generative tasks, revealing a critical gap in
the literature. Moreover, the lack of selective, sample-specific TTA strategies
for image-to-image translation underscores the need for approaches that ac-
tivate adaptation only when necessary, preserving high fidelity and ensuring
generalization across diverse medical imaging scenarios.

3. Methods

Let x € R”*" be a source image and y € R”*” its corresponding target
image, where w and h denote its image width and height, respectively. Image-
to-image translation can be formulated as:

y=T(a) ry (1)

where 7 is a generic task model trained to map the source image « to its
target counterpart y ~ y. At inference, we apply TTA to refine the task
model’s performance on potentially OOD samples. The goal is to reduce
translation error by adapting the model to the individual test sample, such
that:

Ig yl <|y-yl (2)

where y® denotes the synthesized output after applying TTA, and | - | is
a generic error metric (e.g., @; or @2 norm). For the sake of presentation,
Table 1 list the notation used in the manuscript.


Sample Aware Test Time Adaptation

x T y yj,
@ Fi
NO
a YES :

TTA trigger

Aa

y

feedback

r h Dynamic Adaptation Block !
x hy ho nt ” hyo hay
Dynamic '
cbector c g an a _ 74 c 5 ] '
i} ee 4 >
1] Ae Ry | Ay | Ai | AY | AY
' ‘a ¥, y Re a V i
1 hy hy [2 hyo Ai (©!
H 2 H

Figure 1: Schematic workflow of our Test-Time Adaptation approach. (a) TTA phase,
(b) Reconstruction Module, (c) Dynamic Adaptation Block, where a selector determines
whether a feature map should be adapted or kept unchanged.


Symbol Description

A, Input-level adaptor

Aj Set of intermediate adaptors

h; Feature maps of layer i

he Adapted feature maps of layer i

he Concatenation of encoder and decoder feature maps at

level 7, defined as ht. = hi ® hi_,;

hé Reconstruction of hé,

M Number of training steps used to update adaptors dur-
ing TTA

n Number of layers in the task model

T Task model used for Image-to-Image translation task

TY Task model with adaptors inserted at positions defined
by w

x Source image

x input after applying TTA

x input produced by the reconstruction model Ry

y Target image

y Synthetic target image

Yr Reconstructed output produced by the reconstruction
model Ry

y" Synthesized output of the full TTA pipeline

ye Reconstruction of the synthesized output of the full TTA

pipeline, obtained by passing y® through the output re-
construction module R,

Rey Input-domain reconstruction model

R Output-domain reconstruction model

Set all of intermediate reconstruction models, i.e., ex-

cluding input (R,) and output (R,) reconstruction

models

Rj i-th element of the set of intermediate reconstruction
models R, which excludes the input (R,) and output
(Ry) models

Ex Reconstruction error on the adapted input, i.e., |/[a* —
a" ||
Ej Reconstruction error on intermediate feature maps,

computed at level 7

ey Reconstruction error on the output, i-e., ||y — yr]|
T Threshold for triggering adaptation

Q Search space of reconstruction model configurations
2) A candidate configuration in 2

Table 1: Summary of notation used throughout the manuscript sorted in alphabetical
order.


Our TTA framework for medical image-to-image translation is illustrated
in Figure 3. Panel (a) shows that our approach comprises four main compo-
nents: the first is the pretrained task model 7, which remains frozen during
adaptation. The second is the TTA trigger, which selectively activates adap-
tation for OOD samples. The third is the Reconstruction Module (RM),
shown in panel (b), which estimates domain shift at multiple feature levels
and guides the adaptation process. The fourth component is the Dynamic
Adaptation Block (DAB), which transforms input features based on the shift
estimated by the RM (panel (c)). The key idea of our approach is to adapt
only when necessary and in a sample-specific manner. On the one hand,
we retain the fixed parameters of 7 for ID samples, avoiding unnecessary
adaptation. On the other hand, we enable feature-level adaptation for OOD
samples by dynamically selecting the most effective subset of feature adap-
tors.

To this end, we follow a multi-step training process. First, we train the
task model 7; then, we freeze 7 and train the RM on the same training set to
learn feature distributions and quantify reconstruction errors at different lev-
els of 7, serving as a proxy for domain shift during TTA. Following Figure 3
(a), the test image a is first passed to the frozen task model 7, produc-
ing the output image y. We then use the reconstructor R, to compute the
discrepancy between the task model’s output and its reconstruction:

&y=||9— Gl], where Y, = R,(g). (3)

A trigger mechanism then activates TTA when the input image a is iden-
tified as OOD, i.e., when the reconstruction error €, exceeds a threshold T.
Details on how we select the threshold 7 are provided in section 4.2. Once
TTA is triggered, the image a is processed by the DAB module, which uses
the reconstruction error from the RM to perform input-level adaptation, pro-
ducing an adapted input a2*. The adapted input 2° is then passed through
the first layer of 7, producing a feature map h;. The DAB transforms h,
into its adapted counterpart h{, which replaces h; and is propagated to the
next layer of J. This iterative process of adaptation and forward propaga-
tion continues layer by layer, ultimately producing the final adapted output
y”. In the following, section 3.1 and section 3.2 detail the Reconstruction
Module (RM) and the Dynamic Adaptation Block (DAB), respectively.


3.1. Reconstruction Module (RM)

The Reconstruction Module (RM), shown in Figure 3 (b), quantifies do-
main shift by computing reconstruction errors on both intermediate feature
maps within the task model 7 and the generated output. To this end, each
layer of 7 is paired with a dedicated reconstruction network in RM, im-
plemented as a convolutional autoencoder. These networks {Ri} int... 251 \
R, and Ry, are independently trained on the same data used to train 7,
learning the expected distribution of features at their respective levels. Once
trained, they are kept frozen to serve as stable references for detecting distri-
butional discrepancies. The core assumption is that the reconstruction error
measured in inference by each reconstruction network is a proxy for domain
shift, identifying deviations between test-time features and those observed
during training. At the highest resolution level, we use R, and R,, to pro-
duce «* and y?, which are the reconstructions of the adapted input x* and
the task model’s output y*%, respectively. The corresponding reconstruction
errors are defined as:

é = |le* - &"| (4)

ey = 9° — oll (5)
where £° = R,(a*) and y? = R,(y*) are the reconstructions produced by
R, and R,, respectively, and ||-|| denotes the L; distance. At the intermedi-
ate feature levels, we employ the set of reconstruction models {Ri} int... 251 \
where n denotes the total number of layers in 7. At each level 7, we concate-
nate the encoder and decoder features, leveraging the symmetric structure
of T:

he, = hi Gh; (6)

where @ denotes channel-wise concatenation.
The feature map h@, is then passed through the corresponding 7;, yielding
hé, = Ri(hé,), with the corresponding reconstruction error defined as

€, = ||Re, — Ril. (7)

By fusing bottom-up and top-down information, each R;, gains a more com-
plete representation of the feature space at level 7, enhancing its ability to
detect deviations indicative of domain shift.


3.2. Dynamic Adaptation Block (DAB)

This block, illustrated in Figure 3 (c), transforms the input image x
and the features h,,ho,...,hy, using the information provided by the RM
to improve robustness to domain shift. It is composed of trainable compo-
nents, called adaptors, which perform feature-level transformations at mul-
tiple stages of the task model 7, and of a dynamic selector determining
whether a given feature map should be adapted. Let us remember that the
adaptors, which are the only trainable component during TTA, are updated
based on feedback from RM, with each reconstruction module R; paired to
a corresponding adaptor A; in DAB. To identify the most effective subset of
adaptors, we propose a structured search framework that evaluates candidate
combinations of reconstruction models within RM, using their feedback to
guide the adaptation process and estimate domain shift.

Let R = {Ri,..., Ry n= jJ be the set of intermediate reconstruction mod-
els. Note that R explicitly excludes R, and R, from the search process: on
the one hand, FR, is fixed, as it operates directly on the adapted input image
x = A,(a), which always undergoes adaptation; on the other hand, Ry, is
consistently used to monitor the reconstruction error on the adapted output
y-. The search space is defined as:

Q={wCN|wFAD}={{Rif,...,{Ri, Ro},.-.,R} (8)

where each w € (2 represents a valid, non-empty subset of reconstruction
models, considered as a candidate configuration for estimating domain shift
during TTA. Given a search strategy over the space 2, we evaluate each
candidate configuration w € Q using equation 5 to measure the reconstruction
error of R, over a limited number of adaptation steps MM. At each step,
we perform TTA to generate the adapted output y* = 7“(a*), where J“
denotes the task model that performs inference while using the reconstruction
models in w to estimate domain shift and apply the corresponding adaptors.
We then compute the reconstruction error ¢, = ||y* — y?||, where y® is
the adapted output and y* = R,(y*) is the corresponding reconstruction
produced by the reconstruction model R,. The optimal configuration w* is
selected as the candidate that achieves the lowest reconstruction error over
M steps.

In details, we have an input-level adaptor A, and a set of intermediate
adaptors {Ai} int... j; Where the number of intermediate adaptors corre-
sponds to the first half of the architecture, as each A; is applied symmet-
rically to both encoder and decoder layers positioned at depth i and n — 2,

10


respectively. A, applies a sequence of convolutional layers preserving input
image size. A; is implemented as a 1 x 1 convolutional layer that transforms
the feature maps without altering their spatial resolution. A dynamic selec-
tor determines whether a given feature map should be adapted. Based on
its state, the intermediate feature h; is either transformed by the adaptor,
h? = A;(h;), or left unchanged as h? = I(h;) = hj, where I(-) denotes
the identity operator. During TTA, each adaptor is updated independently,
allowing it to learn feature-level transformations based on the local charac-
teristics of each test sample. Each intermediate adaptor A; is applied both
to its corresponding feature h; and to its symmetric counterpart h,_;, lo-
cated on the encoder and decoder branches of 7, respectively. This design
ensures consistent transformation across symmetric layers while minimizing
the overall parameter overhead.

Algorithm 1 Dynamic Search

1: if ey > 7 then

2: ePest £ O09, w* + None
3: for allw € 2 do > Evaluate each configuration in the search space
A: eet + 00 > Initialize best error for the current configuration
5: for i= 1 to M do > Iterate over adaptor update steps
6: y* =T*(«) > Run the inference with the current configuration w
7: eo &y = |ly* — gy? ll, > Evaluate the output reconstruction error
8: ife< epee then
9: eet te > Update best error value for the
10: current configuration w
11: end if
12: end for
13: if epee <ebet then p> If the current best error value is better than
14: the overall best error value
15: ebest cnet Ww Hw > Update best configuration w*
16: end if
17: end for
18: end if
19: return w* > Return best configuration w*

We propose an exhaustive grid search strategy as the core implementa-
tion of our sample-aware TTA framework. As detailed in Algorithm 1, the
method evaluates all possible subsets of reconstruction modules in the config-
uration space 2 to identify the optimal feature-level adaptation for each test

11


sample. Adaptation is triggered only when the sample is detected as OOD,
i.e., when the output reconstruction error €, exceeds a fixed threshold 7. For
each candidate configuration w € 2, the model performs M update steps;
the adapted output y* is reconstructed through R, and the corresponding
reconstruction error is computed. The configuration that achieves the lowest
reconstruction error across all steps is selected as the optimal configuration
w* and used for the final prediction. While computationally intensive, this
exhaustive strategy enables precise, sample-specific adaptation and serves as
the reference implementation throughout this work.

4. Experimental Configuration

This section provides a comprehensive overview of the experimental setup
used to evaluate the performance of our TTA approach. We first describe
the datasets used, including their characteristics and the preprocessing steps
applied. We then outline the experiments carried out, including the state-
of-the-art competitor selected for comparison. We describe implementation
details, covering architectural choices, training configurations, and hyper-
parameter settings. Finally, we introduce the evaluation metrics used to
quantitatively assess the effectiveness of our approach.

4.1. Datasets

We evaluate our approach on two image-to-image translation tasks: LDCT
denoising and 7; to T2 MRI translation, whose main characteristics are sum-
marized in Table 2. For the first task, we use the Mayo Clinic LDCT-and-
Projection Data [20], a public dataset which includes thoracic and abdominal
high-dose CT (HDCT) scans and corresponding LDCT data simulated using
a quarter-dose protocol. From this dataset, we selected 60 patients compris-
ing a total of 17,594 slices. Of these, 10 patients (5,936 slices) were used for
training, while the remaining 50 patients (11,658 slices) were used for testing.
Preprocessing involved converting the raw DICOM files to Hounsfield Units
(HU), selecting a display window centered at —400 HU with a width of 1400
HU, and normalizing all images to the range [-1, 1].

For the second task, we use the BraTS 2018 dataset [21] that contains
clinically pre-operative MRI scans acquired from multiple institutions, al-
ready preprocessed by the authors. We employed a total of 6,528 paired
MRI slices, each consisting of a J)- and a 75-weighted image, amounting to
13,056 images in total. Of these, 5,760 pairs were used for training and 768

12


pairs for testing. We additionally used the [XI dataset [22] as an external test
set. It includes paired 7 and T> brain MRI scans collected from two clinical
sites using different scanners: Hammersmith Hospital (Philips 3T system)
and the Institute of Psychiatry using (Philips 1.5T system). Following [19],
the preprocessing included MNI space registration, white matter peak nor-
malization, and volume resizing. A total of 70 3D scans were selected from
the IXI dataset. From each scan, we extracted 21 axial slices, uniformly
spaced between slice indices 120 and 180, using a 3 mm inter-slice distance.
This procedure resulted in a final dataset of 2,800 2D slices.

It is worth noting that for the LDCT denoising task, no external valida-
tion set is used, as the test split of the Mayo Clinic dataset already offers
sufficient variability to evaluate generalization. With more than 11,000 test
slices, OOD samples occur more frequently, making this setting particularly
favorable for evaluating the effectiveness of sample-aware TTA.

Dataset Domain source Domain target # Instances Train Set Test Set Image Size
Mayo Clinic LDCT [20] LDCT HDCT 17,594 slices 5,936 slices 11,658 slices 512x512
BraTS 2018 [21] T1 MRI T2 MRI 6,528 slices 5,760 slices 768 slices 240 x 240
IXI [22] T1 MRI T2 MRI 2,800 slices — 2,800 slices 240240

Table 2: Summary of the datasets used in this study.

4.2. Experiments

Category Experiment Description

No TTA Baseline task model without TTA.
Competitors He et al. [19] Static TTA approach from He et al. [19]
He et al.” Static TTA approach from He et al. [19] applied at all feature
, levels of the task model T.

Our Approach TTAcria Sample-aware TTA with exhaustive grid search.

Table 3: List of the proposed experiment configurations.

As detailed in Table 3, we evaluate our proposed and three competitors;
furthermore we also investigated alternative search strategies for our TTA
approach, widely discussed in section 5.1. The first competitor, named No
TTA, corresponds to the task model without adaptation and serves as a
baseline for evaluating the benefits of our TTA strategy. The second config-
uration correspond to the approach presented by He et al. [19]. It implements

13


a static TTA method, and, to the best of our knowledge, is the only prior
work addressing TTA in the context of image-to-image translation, as men-
tioned in section 1. The third configuration, named He et al. *, adopts the
same adaptation strategy, integrating it directly into our task model 7 by
inserting reconstruction models at all feature levels and applying them uni-
formly during inference, without any form of sample-specific selection. This
results in a full static adaptation strategy, applied uniformly across the test
set, and serves as an ablation baseline for assessing the contribution of dy-
namic configuration selection. As mentioned in section 1, applying TTA to
ID samples can degrade performance by disrupting the task model’s optimal
configuration learned during training.

To mitigate this, we adopt a gating mechanism that triggers TTA only
when it is likely to be beneficial, using a threshold 7 that is based on the
reconstruction error produced by Ry, on each test sample, as we describe in
section 3.2. We set 7 equal to the 95” percentiles of the reconstruction error
distribution computed over the entire test set. We deem that this choice is
reasonable because samples with high reconstruction error are more likely
to be OOD and, therefore, better candidates for adaptation. On the other
hand, setting the threshold at the 95” percentile is a conservative choice
that limits unnecessary adaptation of ID samples, thereby reducing the risk of
performance degradation caused by overfitting or instability during test-time
updates. This balance helps ensure that TTA is selectively applied to truly
uncertain or anomalous cases, rather than introducing noise into confidently
handled inputs. Furthermore, to investigate the robustness of the 7 value
used we performed a sensitivity analysis that evaluates 7 multiple settings
(ie., 85", 90", and 98" percentiles of R,). The results of this analysis,
provided in Appendix B, confirm that setting 7 to the 95” percentile is the
best choice.

4.3. Implementation details

Our model consists of three main components: the task model, which
performs the translation task and is implemented as a CycleGAN [23]; the re-
construction models, implemented as multi-level convolutional autoencoders;
and the adaptors, implemented as 1 x 1 convolutional layers. For more de-
tails on the model’s components, please refer to Appendix A. We trained
the task models 7 using the Adam Optimizer with an initial learning rate of
2 x 10-4 and a batch size of 8. The training schedule consists of 50 epochs
with a fixed learning rate, followed by 50 epochs during which the learning

14


rate decays linearly to zero, for a total of 100 epochs. This strategy balances
the need for sufficient iterations to capture complex patterns while mitigating
overfitting. The learning rate decay stabilizes convergence in the final stages
of training, and the chosen batch size reflects a trade-off between memory
constraints and training stability.

Each reconstruction model FR was trained independently using Adam op-
timizer with the initial learning rate of 1 x 1073, and a batch size of 8. The
training schedule consists of 20 epochs with a fixed learning rate, followed by
80 epochs during which the learning rate decays linearly to zero, for a total
of 100 epochs. This setup ensures consistency in training dynamics across all
reconstruction models while enabling each of them to specialize on different
features extracted from the task model.

All experiments were conducted on a high-performance computing clus-
ter, equipped with one GPU NVIDIA A100, optimized for large-scale deep
learning workloads.

4.4. Evaluation metrics

We used three well-established image quality assessment metrics to quan-
tify the quality of the generated images. They are the Mean Absolute Error
(MAE), Peak-Signal-to-Noise Ratio (PSNR), and Structural Similarity Index
(SSIM) [24].

The MAE measures the absolute difference between the pixel values of
the generated image y and the target image y:

3

1 —

MAE(g, y) = —

a

ll
°
S

ll
°

where m and n are the number of rows and columns in the images, respec-
tively, and y;,;, Yi; denotes the pixel elements at the 7-th row and j-th column
of y and 4%, respectively. It varies in the range [0,+c00], with a lower MAE
indicating a better match between y and y.

The PSNR compares the maximum intensity in the generated image
(MAX,) with the error between the generated image y and the target image
y given by the mean squared error (MSE):

. MAX;

15


It varies in the range [0,+00], where higher PSNR values indicate better
quality.

The SSIM [25] computes the similarity between two images as a function
of luminance, contrast, and structure:

(2g My + C1) (20 + €2)
(H5 + py + c1)(05 + oF + c2)

SSIM(y, y) = (11)

where jug, {ty are mean intensities of the pixels in y and y, respectively; 0;
and oF are the variances, oj, is the covariance whilst c, and cz are constant
values to avoid numerical instabilities. The (SSIM) varies in the range [0, 1]:

the higher its value, the greater the similarity between the two images.

5. Results and Discussion

This section presents a comprehensive evaluation of our sample-aware
TTA approach through both quantitative and qualitative analyses across two
medical image translation tasks; LDCT denoising and 7, to 7) MRI trans-
lation. We first compare the performance of our method against the com-
petitors described in section 4.2, followed by a detailed analysis of their core
design assumptions and limitations in addressing domain shifts at test time.
Next, we presents a visual comparison of the translated outputs, highlighting
the qualitative improvements achieved by our method in terms of anatomical
fidelity and noise suppression. section 5.1 evaluates alternative search strate-
gies for identifying optimal adaptor configurations, and section 5.2 examines
the computational costs associated with each search strategy, emphasizing
trade-offs between inference cost and adaptation effectiveness.

Table 4 summarizes the experimental results, and is organized into three
sections according to the translation task: LDCT denoising, T-to-T> MRI
translation on the BraTS 2018 dataset, and 7}-to-T, MRI translation on
the IXI dataset. For each task, there are four rows corresponding to the
configurations presented in section 4.2. By column, each metric in table is
divided in two sub-columns A and B. Here, A denotes the full test set,
while B C A includes only the samples identified as OOD by our method.
OOD samples are selected using a selection threshold 7 set to 95” percentile
of the reconstruction error distribution computed over the entire test set,
as detailed in section 4.2. In each section, the best-performing results are
highlighted in green and demonstrate statistically significant differences from

16


the others, satisfying the Wilcoxon test with Bonferroni correction [26]. Al-
though the competitors do not support OOD sample identification, we report
their performance on B to enable a comprehensive comparison under OOD
conditions.

Task Experiment SSIM + MAE | PSNR ¢
A BCA A BCA A BCA
No TTA 693 + .199 647 + .2437 063 + .048 118 + 0717 27.913 + 5.793 18.064 + 2.1647
LDCT Competitors He et al. [19] | .384 +167 552 = 97 | 388 +29 331 +71 | 15.723 1924 16.388 + 11837
Denoising He et al.* 645 #711 729 #2731 | ggg =-069 ggg = 064 | 94.875 $4277 95.557 * 95-6704
Our approach TTAGria .699 * 708 769 * 78° | 060 #4 063 *-8 | 28.464 4% — 29.204 * S187
MRI No TTA 858% 40 866 +t | 037 +-910 039 + -05t | 26.459 +1864 95.941 + 6817
T,-Ty Competitors He et al. [19] | 330 $2! 279 =-031F | 571 #060 591 + 0597 7.23 #899 7.701 + S447
(BraTS He et al.* 803 #9 795 +044 | 979 +098 975 + OMT | 20.498 +2760 19.403 * 16734
2018) | Our approach TTAGuia 855 #2 ggg #080 | 939 +18 967 = 18 | 26.239 = 22% 90.557 #2517
No TTA 692 + .046 700 +.048} 115 + .022 126 + .0187 18.622 + 1.347 17.953 + 1.044}
MRI Competitors He et al. [19] | .289+-%5 271 #47) 481 +3 594 +157 | 9.951 $2018 9.479 + 15367
oo He et al Uk .704 + .045 724 + .0447 103 + .017 .090 + .0187 18.410 +1.996 18.477 + 2.259}
Our approach TTAcria JO) ES BIL = | OREO) = He 19. = 32 Yogi = AO

Table 4: Quantitative comparison of different experiments across different translation
tasks. Metrics are reported for the entire test set A and for the subset B C A. The best
results for each task are highlighted in green. The dagger symbol (') denotes results from
configurations that do not support OOD sample identification but are evaluated on the
subset B Cc A, identified by our approach, to illustrate how these configurations would
perform on the detected OOD samples.

In the LDCT denoising task, TTAg,;g achieves the highest performance
across all three metrics on both the full test set A and the OOD subset
B CA, clearly outperforming all competitors. The performance improve-
ment is evident on B, where domain shift is more severe, which is expected
since TTAg,i;q dynamically tailors the adaptation on a per-sample basis,
rather than applying a fixed adaptation across all test samples. The com-
parison with the static method proposed by He et al.”, further underscores
the importance of the adaptive nature of our approach. While He et al.”
provides modest gains over the task model without adaptation, No TTA, it
lacks the flexibility to adjust to the specific characteristics of individual test
samples. These findings demonstrate that TTAgq not only improves average
denoising but also enhances robustness under domain shift, a critical factor
in clinical scenarios characterized by low-dose protocols and heterogeneous
image quality.

Turning to the results for T\-to-T, MRI translation on the BraT'S 2018
dataset, TTAg,jiq does not provide measurable benefits. Our method achieves
comparable performance to the task model without adaptation (No TTA) on

17


A and even shows a slight performance drop on the OOD subset B. This out-
come is expected, as both the training and test samples are drawn from the
same distribution; since the task model already generalizes effectively, TTA
provides limited benefit and can even degrade performance. These findings
underscore the importance of first assessing the presence of domain shift be-
fore applying TTA, highlighting the limitations of indiscriminate adaptation
in ID scenarios.

To further investigate this hypothesis, we assess the performance of the
task model on the [XI dataset, used here as an external test set. In this more
realistic setting, where the dataset introduces a substantial domain shift due
to differences in scanner types and acquisition protocols, T’T’Ag,;q achieves
consistent performance gains across all metrics, with notable improvement
on the OOD subset B. He et al. [19] achieves the poorest performance, fail-
ing to improve translation quality in either task. This may be due to the
uniform application of adaptation via a static design: feature-level adapta-
tion is carried out through a fixed set of adaptors inserted at all layers of the
task model 7, without accounting for input-specific characteristics or sup-
porting dynamic configuration. Additionally, the reconstruction models are
trained jointly using a single cumulative loss function, which limits their abil-
ity to specialize and reduces their effectiveness in capturing domain shifts.
As a result, the adaptors are less capable of applying appropriate feature
transformations at different levels of the task model 7. These shortcomings
underscore the limitations of a one-size-fits-all adaptation strategy that treats
all test samples equally, regardless of whether they are ID or OOD. He et
al.” achieve moderately better performance by training each reconstruction
model independently, allowing them to capture feature-level variations more
effectively and serve as a stronger proxy for measuring domain shift. How-
ever, the adaptation mechanism remains static and applied uniformly across
all test samples, limiting the adaptors’ ability to tailor feature-level transfor-
mations to input-specific characteristics. Although both He et al. [19] and
He et al.” do not distinguish between ID and OOD samples, we report their
performance on the OOD subset B, as identified by TTAcrq. He et al.” at-
tains suboptimal results on the full test set A, confirming that TTA is most
effective when selectively applied, but it also fails to improve translation per-
formance on the OOD subset B. On the subset B, we further observe that,
even when focusing only on OOD samples, fixed reconstruction strategies
remain less effective than our approach. This comparison reinforces the im-
portance of tailoring the adaptation to the specific distributional properties

18


of each test sample.

Figures 2, 3, and 4 show visual comparisons for LDCT denoising and
T,-to-T2 MRI translation tasks across different experimental configurations.
Each figure follows the same structure: for each dataset, we display the in-
put image, the ground truth reference, and the generated outputs produced
by the competitors (No TTA, He et al. [19], and He et al.*) alongside our
proposed approach, TTAciq. Each panel includes a red zoom-in box high-
lighting a region of interest (ROI) selected to emphasize structural or textural
features most affected by the adaptation strategy.

(a) Low-dose input (b) high-dose reference

(d) He et al. [19] (e) He et al.” (f) TTAcria

Figure 2: Visual comparison of denoised CT slices. Zoomed ROIs highlight key regions
where TTA yields notable improvements in denoising quality.


(a) Ty input (b) T2 reference

(d) He et al. [19] (e) He et al.” (f) TTAGria

Figure 3: Zoom-in comparison across different TTA strategies for the BraTS 2018 dataset.
Zoomed ROIs highlight key regions where TTA does not yield notable improvements in
translation quality.

20


(a) Ty input (b) T2 reference

(d) He et al. [19] (e) He et al.” (f) TTAGria

Figure 4: Zoom-in comparison across different TTA strategies for [XI dataset. Zoomed-in
regions of interest (ROIs) highlight areas where TTA demonstrates notable improvements
in translation quality.

21


Turning our attention to Figure 2, the denoised image using the task
model without adaptation (No TTA, Figure 2 (c)) exhibits residual noise and
structural inconsistencies relative to the high-dose reference (Figure 2(b)).
He et al. {19] (Figure 2 (d)) fails to address these artifacts, while He et
al.” (Figure 2 (e)) yields modest improvements, though it applies a fixed
adaptation uniformly across all samples. In contrast, TTAcia(Figure 2 (f))
produces improved anatomical fidelity, demonstrating the benefit of sample-
specific TTA. These results further stress the importance of tailoring TTA
to individual test cases rather than relying on static, one-size-fits-all TTA
strategies.

Figure 3 shows results for the T)-to-T> MRI translation task on the BraTS
2018 dataset. In this ID setting, the task model without adaptation (No
TTA, Figure 3 (c)) already generates visually consistent outputs. None of
the TTA methods, neither the competitors nor our proposed approach, pro-
vide noticeable improvements and, in some cases, even degrade image quality.
This reinforces the observation that when dealing with ID test samples, adap-
tation may be unnecessary and can potentially be detrimental. In contrast,
turning to Figure 4 shows results for the same translation task on the [XI
dataset. Here, the task model model without adaptation (No TTA, Figure 4
(c)) shows evident structural distortions and reduced contrast. He et al. [19]
(Figure 4 panel d) fails to correct these artifacts, while He et al.” (Figure 4
(e)) achieves only modest improvements. In contrast, TTAcria (Figure 4 (f))
delivers the highest visual quality, recovering sharper anatomical boundaries
and minimizing artifacts. Our thorough analysis confirms the core intu-
ition behind sample-aware TTA: while adaptation offers limited benefit ID
settings, it becomes essential for improving generalization under significant
distribution shifts, provided it is applied selectively and tailored to individual
test samples.

5.1. Alternative Search Strategies for Sample-Aware TTA

To address the computational overhead of TTAcriq, we investigate alter-
native search strategies for dynamic adaptor selection, each differing in how
they explore the configuration space 2. As detailed in Table 5, we evaluate
six representative search strategies, spanning heuristic and probabilistic ap-
proaches, including random searches, named as TTARj9 and TTAgsg, which
sample 10 and 50 configurations, respectively; forward selection TTAps, back-
ward elimination TTAgpg, and bayesian optimization TTAga.

22


Experiment Description

Sample-aware with random search over 10 sampled configu-

TTAR10 rations.

TT Apso Sample-aware TTA with random search over 50 sampled
configurations.

TTAgs Sample-aware TTA with forward selection search.

TTApE Sample-aware TTA with backward elimination search.

TTABA Sample-aware TTA with Bayesian search.

Table 5: Summary of alternative search strategies used for sample-aware TTA.

Table 6 reports the quantitative metrics in the same format as Table 4.
Results on BraTS 2018 are omitted, as prior analysis showed that TTA is
ineffective in this setting, making further exploration unnecessary. The table
is divided into two sections according to the translation task: LDCT de-
noising, and 7, to 75 translation on the [XI dataset. We report the results
obtained by each method on both the entire test set A and the subset of
OOD samples B Cc A. In the LDCT denoising task, TTAc:iq demonstrates
strong overall performance but is not consistently statistically superior to
other search strategies, particularly TTApso, as detailed in Appendix D. In
contrast, greedy algorithms such as TTArs and TTApg exhibit greater per-
formance variability, likely due to their tendency to get stuck in local minima
within the search space. Bayesian optimization, TTAga, performs reasonably
well but does not outperform simpler alternatives, possibly due to the lim-
ited number of trials and the need for careful hyperparameter tuning, which
itself introduces additional complexity. Turning to the 7)-to-T2 MRI trans-
lation task, we observe a similar pattern: while TTAciq maintains strong
performance, its advantage over lighter strategies, such as TTARs9 and even
TTArio, is not always statistically significant, as detailed in Appendix D.
These findings underscore the effectiveness of random search methods, even
with a small number of evaluations, in achieving competitive performance
with significant reduction in computational cost.

23


Task Experiment SSIM +t MAE J PSNR t
A BCA A BCA A BCA
TTAcra 699 + .203 .769 + .289 .060 + .045 063 + .053 28.464 + 5.492 29.204 + 6.137
LDCT TTArso* 699 + .202 764 + .286 061 + .045 066 + .052 28.435+ 5.477 28.625 + 5.899
TTApa 697 £201 734 £275 | ggg +-047 gz +079 | 98 993 +5484 95 745 + 5-450
TTAgria .739 £88771 = 060 | 993 #22 ggg +24 | 20,042 £1998 90.317 #227
MRI TTARi0* 729 + .060 760 + “a 088 + .022 O81 + .022 19.303 + 2:328 19.447 = 1.946
T,-Ty TTArs0* 727 * oS .758 + on 089 +91 082 +2 | 19.676 +1899 19.678 +2275
(IXI) TTArs 716 = 9 749 = 7 | ggg = 100 + 74 | 18.850 = 18384 18.395 * 2019

Table 6: Quantitative comparison of different search strategies for sample-aware TTA.
Metrics are reported for the entire test set A and for the subset B C A. The best results
for each task are highlighted in green, while the second-best are marked in blue. An
asterisk (*) denotes that results were averaged over three runs to reduce variance and
improve stability. The dagger symbol (1) denotes results from configurations that do not
support OOD sample identification but are evaluated on the subset B C A, identified by
our approach, to illustrate how these configurations would perform on the detected OOD
samples.

5.2. Computational analysis

Turning our attention to the computational cost, Table 7 shows the aver-
age inference time and complexity O(-) computed on a single A100 across all
tasks. The analysis accounts for both the adaptor update steps and the con-
figuration selection process involved in each TTA configuration, along with
their corresponding computational complexity. As expected, all alternative
search strategies introduce additional computational overhead compared to
the baseline. Among them, TTAcriq is by far the most computationally
demanding, requiring up to 130 seconds (s) per sample. This is due to
its exhaustive exploration of the configuration space 2, whose complexity
grows exponentially with the number of intermediate layers n—2, resulting
in O(2”~7) operations for each test samples requiring adaptation. To mitigate
this cost, we evaluate alternative strategies that reduce the number of config-
urations explored while maintaining dynamic and sample-specific adaptation.
Random searches, TTARi9 and TTARs0, with Neon fig equal to 10 and 50, re-
spectively, limit the search to 10 or 50 configurations, respectively, achieving
inference times of 12 s and 55s. As expected, their complexity scales lin-
early with the number of sampled configurations, i.e, O(Neonsig), offering

24


a trade-off between speed and performance. TTApsg and TTAgg achieve a
more favorable balance, with inference times around 25 s and 21 s. Their
quadratic complexity O((n — 2)?) reflects the structured, iterative nature of
the search over n—2 intermediate layers. Finally, TTAga offers a principled
compromise, with a fixed budget of Neon fig = 20 trials guided by a proba-
bilistic surrogate model. While its complexity remains linear in the number
of evaluations, the targeted search effectively avoids unnecessary configura-
tions, resulting in inference times of approximately 20 s. While TTAcria
remains the most exhaustive and frequently top-performing strategy, our
statistical analysis reveals that its performance is not always consistently or
significantly superior to lighter alternatives. In particular, approaches such
as TTArso often achieve comparable results without incurring the high com-
putational cost of a grid search. This suggests that, in practice, more efficient
search strategies can provide a favorable trade-off between adaptation quality
and inference time, offering competitive performance with significantly lower
resource demands.

TTA Inference Time (s/sample) O(-)

TTAcria 130 O(2"~?)

TTARio 12 O( Neon fig)
TTARs0 ays) O( Neon fig)
TTApaA 20 O( Neon tig)

Table 7: Average inference time per sample (in seconds), measured across all tasks. The
reported times include both adaptor update and the configuration selection search. The
last column reports the theoretical complexity O(-) per sample, where n denotes the total
number of layers in the task model, n—2 denotes the number of intermediate layers and
Neonfig the number of explored configurations

6. Conclusions

In this paper, we introduce an approach for sample-aware TTA in medical
image-to-image translation. The core contributions of this work are twofold:
(1) a reconstruction module that quantifies reconstruction errors across all
feature levels of a pretrained task model 7, serving as a proxy for domain

25


shift; and (2) a dynamic adaptation block that applies feature-level transfor-
mations at multiple stages of 7, guided by the reconstruction errors identified
by the reconstruction module. Through extensive evaluation on LDCT de-
noising and 7\-to-7j MRI translation tasks, we demonstrate that TTA is
most effective when applied selectively. While indiscriminate adaptation can
degrade performance on ID samples, our method delivers substantial im-
provements on OOD cases by tailoring the adaptation on a per-sample basis.
We show that dynamically selecting the optimal adaptation configuration
on a per-sample basis outperforms static, one-size-fits-all approaches in both
quantitative metrics and computational efficiency. These findings highlight
the importance of balancing adaptation quality with runtime constraints, a
critical consideration for real-world deployment.

Limitations and Future Work. Despite these promising results, our approach
has some limitations. First, OOD detection relies on a fixed percentile thresh-
old applied to reconstruction error, which may not generalize optimally across
tasks or domains. Second, although we mitigate the overhead of configuration
search, the process still introduces latency that may limit its use in real-time
or resource-constrained settings. Third, our current implementation oper-
ates on 2D slices, consistent with most prior work in medical image-to-image
translation. This choice reflects not only computational considerations, but
also our primary goal: to introduce and systematically evaluate a novel TTA
strategy, rather than to optimize translation performance. Working in 2D
enables controlled benchmarking of sample-specific adaptation mechanisms.
However, this formulation does not capture volumetric consistency, which re-
mains essential for many clinical applications. As a first direction for future
work, we aim to improve OOD detection by exploring more advanced selec-
tion strategies, such as learned thresholds, outlier detection techniques, or
uncertainty-based scoring. Additionally, we will investigate adaptive mech-
anisms for more efficient and generalizable adaptor selection across diverse
generative tasks and data modalities. We also plan to extend our frame-
work to a broader range of generative architectures such as diffusion models
or vision transformers, evaluating its model-agnostic capabilities and its ro-
bustness in real-time and multi-modal deployment scenarios. Lastly, we plan
to extend our framework to 3D architectures such as volumetric GANs or dif-
fusion models, leveraging the modularity of our adaptation design to support
fully volumetric adaptation.

26


7. Acknowledgment

Irene Iele is a Ph.D. student enrolled in the National Ph.D. in Artificial
Intelligence, XL cycle, course on Health and Life Sciences, organized by Uni-
versita Campus Bio-Medico di Roma. This work was partially funded by: i)
Kempe Foundation project JCSMK24-0094; ii) Cancerforskningsfonden Nor-
rland project MP23-1122; iii) PNRR-MCNT2-2023-12377755 LUMINATE.
The computations of this work were enabled by resources provided by the
Swedish National Infrastructure for Computing (SNIC), partially funded by
the Swedish Research Council through grant agreement no. 2018-05973.
Generative AI tools were used to assist in grammar correction and text style
refinement throughout the paper.

References

[1] Y. Pang, J. Lin, T. Qin, Z. Chen, Image-to-image translation: Methods
and applications, IEEE Transactions on Multimedia 24 (2021) 3859—
3881.

[2] S. Kaji, S. Kida, Overview of image-to-image translation by use of deep
neural networks: denoising, super-resolution, modality conversion, and
reconstruction in medical imaging, Radiological physics and technology
12 (2019) 235-248.

[3] C. Hognon, P.-H. Conze, V. Bourbonne, O. Gallinato, T. Colin,
V. Jaouen, D. Visvikis, Contrastive image adaptation for acquisition
shift reduction in medical imaging, Artificial Intelligence in Medicine
148 (2024) 102747.

[4] J. Liang, R. He, T. Tan, A comprehensive survey on test-time adaptation
under distribution shifts, International Journal of Computer Vision 133
(2025) 31-64.

[5] J. Liang, R. He, T. Tan, A comprehensive survey on test-time adapta-
tion under distribution shifts, International Journal of Computer Vision
(2024) 1-34.

[6] S. Niu, J. Wu, Y. Zhang, Y. Chen, S$. Zheng, P. Zhao, M. Tan, Eff-
cient test-time model adaptation without forgetting, in: International
conference on machine learning, PMLR, 2022, pp. 16888-16905.

27


I"

[11]

[12]

[13]

[14]

[15]

W. Ma, C. Chen, S. Zheng, J. Qin, H. Zhang, Q. Dou, Test-time adapta-
tion with calibration of medical image classification nets for label distri-
bution shift, in: International Conference on Medical Image Computing
and Computer-Assisted Intervention, Springer, 2022, pp. 313-323.

H. Yang, C. Chen, M. Jiang, Q. Liu, J. Cao, P. A. Heng, Q. Dou, Dltta:
Dynamic learning rate for test-time adaptation on cross-domain medical
images, IEEE Transactions on Medical Imaging 41 (2022) 3575-3586.

N. Karani, E. Erdil, K. Chaitanya, E. Konukoglu, Test-time adaptable
neural networks for robust medical image segmentation, Medical Image
Analysis 68 (2021) 101907.

H. Li, H. Liu, D. Hu, J. Wang, H. Johnson, O. Sherbini, F. Gavazzi,
R. D’Aiello, A. Vanderver, J. Long, et al., Self-supervised test-time
adaptation for medical image segmentation, in: International Workshop
on Machine Learning in Clinical Neuroimaging, Springer, 2022, pp. 32—
Al.

G. Valvano, A. Leo, S. A. Tsaftaris, et al., Re-using adversarial mask
discriminators for test-time training under distribution shifts, Machine
Learning for Biomedical Imaging 1 (2022) 1-27.

R. Wen, H. Yuan, D. Ni, W. Xiao, Y. Wu, From Denoising Training
to Test-Time Adaptation: Enhancing Domain Generalization for Med-
ical Image Segmentation, in: Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision, 2024, pp. 464-474.

M. Hu, T. Song, Y. Gu, X. Luo, J. Chen, Y. Chen, Y. Zhang, S. Zhang,
Fully test-time adaptation for image segmentation, in: Medical Image
Computing and Computer Assisted Intervention—-MICCAI 2021: 24th
International Conference, Strasbourg, France, September 27—October 1,
2021, Proceedings, Part III 24, Springer, 2021, pp. 251-260.

V. Jain, E. Learned-Miller, Online domain adaptation of a pre-trained
cascade of classifiers, in: CVPR 2011, IEEE, 2011, pp. 577-584.

Z. Wang, Y. Luo, L. Zheng, Z. Chen, S. Wang, Z. Huang, In search
of lost online test-time adaptation: A survey, International Journal of
Computer Vision (2024) 1-34.

28


[16]

[17

uo

[18]

[19]

[20]

Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, M. Hardt, Test-time train-
ing with self-supervision for generalization under distribution shifts, in:
International conference on machine learning, PMLR, 2020, pp. 9229—
9248.

D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, A. A. Efros, Context
encoders: Feature learning by inpainting, in: Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 2536—
2544.

S. Gidaris, P. Singh, N. Komodakis, Unsupervised representation learn-
ing by predicting image rotations, arXiv preprint arXiv:1803.07728
(2018).

Y. He, A. Carass, L. Zuo, B. E. Dewey, J. L. Prince, Autoencoder based
self-supervised test-time adaptation for medical image analysis, Medical
image analysis 72 (2021) 102136.

T. R. Moen, B. Chen, D. R. Holmes III, X. Duan, Z. Yu, L. Yu, 5. Leng,
J. G. Fletcher, C. H. McCollough, Low-dose CT image and projection
dataset, Medical physics 48 (2021) 902-911.

B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani,
J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al., The mul-
timodal brain tumor image segmentation benchmark (BRATS), [EEE
transactions on medical imaging 34 (2014) 1993-2024.

IXI Dataset, Brain Development, https://brain-development.org/
ixi-dataset/, ????

J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image
translation using cycle-consistent adversarial networks, in: Proceedings
of the IEEE international conference on computer vision, 2017, pp. 2223—
2232.

F. Di Feola, L. Tronchin, P. Soda, A comparative study between paired
and unpaired Image Quality Assessment in Low-Dose CT Denoising, in:
2023 IEEE 36th International Symposium on Computer-Based Medical
Systems (CBMS), IEEE, 2023, pp. 471-476.

29


[25]

[26]

[27]

[28]

[29]

[30]

[31]

Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, Image quality
assessment: from error visibility to structural similarity, IEEE transac-
tions on image processing 13 (2004) 600-612.

C. Bonferroni, Teoria statistica delle classi e calcolo delle probabilita,
Pubblicazioni del R istituto superiore di scienze economiche e commeri-
ciali di firenze 8 (1936) 3-62.

I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, Y. Bengio, Generative adversarial networks,
Communications of the ACM 63 (2020) 139-144.

D. E. Rumelhart, J. L. McClelland, P. R. Group, et al., Parallel dis-
tributed processing, volume 1: Explorations in the microstructure of
cognition: Foundations, The MIT press, 1986.

P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-image translation
with conditional adversarial networks, in: Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp. 1125—
1134.

J. Bergstra, R. Bardenet, Y. Bengio, B. Kégl, Algorithms for hyper-
parameter optimization, Advances in neural information processing sys-
tems 24 (2011).

J. Bergstra, D. Yamins, D. Cox, Making a science of model search:
Hyperparameter optimization in hundreds of dimensions for vision ar-
chitectures, in: International conference on machine learning, PMLR,
2013, pp. 115-123.

S. Watanabe, Tree-structured parzen estimator: Understanding its al-
gorithm components and their roles for better empirical performance,
arXiv preprint arXiv:2304.11127 (2023).

F. Wilcoxon, Individual comparisons by ranking methods, in: Break-
throughs in statistics: Methodology and distribution, Springer, 1992,
pp. 196-202.

30


Appendix A. Preliminaries

Appendix A.1. CycleGAN

CycleGAN [23], introduced in 2017, made significant changes to the tradi-
tional GANs’ architecture [27], enabling bidirectional image-to-image trans-
lation. Unlike standard GANs, which focus on unidirectional translation,
CycleGAN revolutionizes this paradigm, allowing both the translation from
the source domain to the target domain and the reverse translation from syn-
thetic target images back to the source domain. This dual transformation
enhances model stability and improves the accuracy of generated images, es-
pecially in unpaired settings, where paired training data is unavailable. The

Dx

A
G
—pid—j
@ D
—
x F

Dy Dx

G PF

f
;

Figure A.5: CycleGAN Architecture.

CycleGAN framework, as shown in figure A.5 includes two mapping func-
tions, known as generators: G: X > Y and F: Y > X, where X is the
source image domain while Y is the target image domain. It also includes
two discriminators, Dx and Dy, which evaluate the authenticity of images
translated between these domains. In particular, the role of the discrimina-
tor Dy is to differentiate between samples generated by G(x) and authentic
samples from domain Y, while Dx distinguishes between generated samples
Fy) and the real samples of the X domain. CycleGAN is composed of two
loops: the consistency of the forward cycle and the consistency of the back-
ward cycle. The first loop is a mapping from the source image domain X
to the target image domain Y and then a mapping from the target image
domain back to the source image domain; therefore, the image translation
process must ensure that any image x belonging to X, when translated into
domain Y, and then returned to domain X, remains approximately the same
original image «x.

zr G(r) =9 > F(y) =%. (A.1)

Similarly, the second loop is a mapping from the target image domain to
the source image domain and then a mapping from the source image domain

ol


back to the target image domain. So, for any image y from domain Y, the
process should ensure that after being translated to domain X, and then
back to domain Y, it remains almost identical to the original image y.

y > Fly) =% > G(4z) = 4. (A.2)

The CycleGAN optimization function is formulated as a minimax problem,
in which the generators and discriminators are trained simultaneously. The
goal is to find the optimal parameters for both the generators and discrimi-
nators that minimize the ability of the discriminators to distinguish between
generated and real samples. The minimax formulation is expressed as:

The performance of these networks is therefore highly dependent on how their
loss functions are structured, so special attention must be paid to them. The
loss used in CycleGAN is composed of three terms: adversarial loss, cycle
consistency loss and identity loss. The first term is applied to both mapping
functions. For the mapping function G : X — Y and its discriminator Dy,
we express the objective as: where G tries to generate images G(x) that
look similar to images from domain Y, while Dy aims to distinguish between
translated samples G(x) and real samples y.

Leaan(G, Dy, X, Y) = Ey pata (y) [log Dy (y)| +E a~panta (a) [log (1 ~ Dy (G(a)))] °

(A.4)
For the mapping function Ff’: Y > X and its discriminator Dx as well we
have:

Lean(F, D,,Y, Xx) = Ea paata (a) [log Dx (©)|+Eyvpasta(y) [log (1 ~ Dx(F(y)))| .
(A.5)
Where Eyvnaaia(y)> Ex~paata(z) Fepresent, respectively, the expectation with re-
spect to the distribution of real images in the Y and X domains. Dy(y) is
the probability that image y is deemed authentic in domain Y by discrim-
inator Dy; Dy(G(x)) represents the probability that the discriminator Dy
classifies the generated images G(x) as authentic within domain Y.
On the other hand, Dx(z) is the probability that image x is deemed authentic
in domain X by discriminator Dy and Dx(F'(y)) represents the probability
that the discriminator Dy classifies the generated images F'(y) as authentic
within domain X. Lagy is the sum of A.4 and A.5 expressions.

32


In CycleGANs, compared to traditional GANs, a new loss, known as Cycle
Consistency Loss, is introduced, which further regularizes the mappings: for
each image x of domain X, the image translation cycle must be able to bring
x back to the original image. This is called forward cycle consistency. Simi-
larly, for each image y from domain Y,G and F' should also satisfy backward
cycle consistency.

Leye(G, F) = Eanpaata(e) ULF (G(®)) — li] + Eyepeataty) IG) = lla]
(A.6)
In this case, the loss is computed using the L1 norm.
The third term, the Identity loss, acts as an additional parameter aimed at
elevating the overall quality of the generated images. This loss has special
relevance to avoid unwanted modifications when an image already belongs
to the target domain. The essence of identity loss is to take a sample x from
the target domain X and pass it through generator G. Similarly, a sample y
from the target domain Y is passed through generator F’. In this way, both
generators learn the identity mapping functions for their respective domains.
The equation representing the loss of identity is expressed as follows:

Lidentity(G, F) = Exwpeaca(e) [G(@) — alli] + Evrae) FY) — ull] (A-7)

Also in this case, the loss is computed using the L1 norm. So we can resume
that the CycleGAN loss is:

Loan(G, FP, Dx, Dy) = Laav(G, FP, Dx, Dy) +A Leycte(G, F)+A2Liaentity(G, F)
(A.8)

Appendiz A.2. Autoencoder

Autoencoders [28] are a class of neural networks designed for unsuper-
vised learning, in which the primary goal is to learn an efficient encoding of
the input data while allowing accurate reconstruction of the original input.
This process makes them particularly suitable for tasks aimed at dimension-
ality reduction, feature extraction, and data reconstruction. An Autoencoder
consists of two main modules:

e Encoder: This module compresses the input data into a compact la-
tent representation, capturing the most salient and essential features
of the data. The encoder reduces the input dimensionality, discarding
irrelevant or redundant information.

33


e Decoder: Reconstructs the input data from the compressed latent rep-
resentation. Its goal is to recreate the original data as faithfully as
possible, effectively reversing the transformation applied by the en-
coder.

Autoencoder training is controlled by a reconstruction loss function, which
measures the difference between the input data and its reconstruction. The
choice of loss function significantly affects the model’s ability to learn rele-
vant features. Commonly used loss functions include Mean Squared Error
(MSE), Mean Absolute Error (MAE) or Structural Similarity Index Measure
(SSIM). In our case, the MSE, defined as in following equation, was chosen.

N

1 *
where x; is the 2-th pixel of the input image, 7; is the corresponding pixel in
the reconstructed image, and N is the total number of pixels. This type of
loss is ideal for tasks requiring pixel-level accuracy.

Appendiz A.2.1. Alternative reconstruction models

For completeness, we also tested an alternative reconstruction architec-
ture based on the generator of Pix2Pix [29], which employs a U-Net structure
with skip connections and adversarial training. Despite its strong generaliza-
tion capabilities in image-to-image translation tasks, Pix2Pix yielded inferior
performance in the reconstruction-based TTA framework. In the LDCT de-
noising task, the TTAc,iq configuration with Pix2Pix achieved lower results
(SSIM: .671+.285, MAE: .086+.074, PSNR: 26.201+5.761) compared to our
autoencoder-based reconstruction models. We hypothesize that Pix2Pix’s
tendency to generalize across domains reduces its sensitivity to subtle dis-
tribution shifts, making it less suitable for TTA, where fine-grained domain
discrepancies must be identified and corrected. Based on these observations,
we opted to use autoencoders as reconstruction models throughout our ex-
periments.

Appendiz A.3. Adaptors

The adaptors are implemented as individual 1 x 1 convolutional layers.
Each feature-level adaptor consists of a 1 x 1 convolution that preserves the
spatial resolution of the feature maps while altering their channel-wise rep-
resentation. The input-level adaptor, in contrast, operates directly on the

34


input image using a sequence of convolutions that maintain its spatial dimen-
sions. In both cases, the use of 1 x 1 convolutions ensures that the spatial
structure of the image remains unchanged, while enabling pixel-wise modula-
tion of feature information. Adaptors are placed at each level corresponding
to a reconstruction model, with the exception of the final output layer, which
remains unmodified. Their purpose is to apply sample-specific transforma-
tions aimed at reducing domain shift, as estimated by the reconstruction
error computed from the reconstruction models.

39


Appendix B. Sensitivity Analysis on Threshold Selection

To complement the results presented in the main manuscript (based on
the 95" percentile threshold), we report here the performance metrics ob-
tained using alternative thresholds of 85”, 90°” and 98"" percentiles. These
thresholds were applied to determine the subset of test samples requiring
TTA, denoted as B C A. It is worth noting that, due to rounding to four
decimal places, the 85” and 90 percentiles resulted in the same numerical
threshold on the BraTS dataset.

The results show that selecting 7 equal to the 95" percentile provides
the best overall trade-off. On the one hand, lower values of 7 yield a more
liberal selection strategy, triggering adaptation for a larger portion of the test
set. While this can enhance performance on highly shifted samples, it also
increases the risk of unnecessary adaptation on ID data, potentially leading
to performance degradation due to overfitting or instability. On the other
hand, higher values of 7 result in a conservative approach, limiting adapta-
tion to only the most anomalous cases. This minimizes the risk of harming
ID performance but may miss moderately shifted OOD samples that could
benefit from adaptation. Furthermore, since TTA involves non-negligible
computational overhead, overly permissive thresholds, i.e., low 7 values, may
incur unnecessary cost by adapting a large number of samples that do not
significantly benefit from it. Therefore, the 95” percentile threshold emerges
as a reasonable and robust compromise between effectiveness, selectivity, and
computational efficiency.

For the 7; to 7; MRI translation task, the [XI dataset was used solely
as an external test set for OOD evaluation. Therefore, we did not perform a
sensitivity analysis on this dataset. Instead, we applied the threshold selected
on the BraTS 2018 dataset ensuring that the choice is not biased by the test
data distribution and mimics a realistic deployment setting.

Appendiz B.1. Denoising task on LDCT dataset

36


Experiments SSIM t MAE J PSNR t
A BCA A BCA A BCA

No TTA 693 + .199 .623 + .234' .063 4 .048 .109 + .082t 27.913 + 5.793 20.396 + 3.689¢
Competitors He et al. [19] 384+ .167 459+ .159f 388+ 105 379+ .112' 15.72341.524 15.816 J
Heetal.* 6454 .211 6434 .269' .089+.069 .103+ .070' 24.87544.277 24.280 + 5.3721

TTAgria 701 + .205 679 + .278 §=.060 + .042 = =.083 + .070 28.808 4
‘Ario* 699 + .204 061 + .044 .093 + .075 28.610 4

26.839 + 6.013
25.414 4

Our Approach ‘Arso* 701 + .205 -060 + .042 .083 + .068 28.783 + 5.26 26.664 + 5.84:
Pproe TTAgs 698 + .205 061 + .046 .094 + .084 28.614 + 5.36 25.445 + 5.944
TTApe -700 + .204 .060 + .042  .087 + .070 28.699 4 26.047

TTAga 696 + 204 =.646 + .267 = .050 + 094.105 + .094 28.449 + 5.4 24.253 + 5.239

Table B.8: Results for the denoising task on the LDCT dataset using the 85” percentile
as the threshold threshold for OOD detection. An asterisk (*) denotes that results were
averaged over three runs to ensure reproducibility. Metrics are reported for the entire test
set A and for the subset B C A. The best-performing configuration is highlighted in
green. The dagger symbol ' marks results from configurations that do not support OOD
sample identification. Nonetheless, we report their performance on the subset BC A, as
identified by our approach, to show how these configurations perform on detected OOD
samples.

Experiments SSIM t MAE | PSNR t
A A BCA A BCA
o TTA 063 + .048 103 + .069' 27.913 + 5.793 19.690 + 3.064!
Competitors He et al. [19] . 388 +105 362 + .0911 = 15.723 + 1.524 1.359%
Heet al.” 645 + 211 .089 +.069 088 + .072' 24.875 + 4.277 + 5.3051

TTAGra -701 + .204

060 + .044 070 + .057 = 28.743 + 5.365 28.093 + 5.966

TTARio*® 699 + .203 061 + .044 .076 + .052 26.770 + 5.390
Our Approach TTARso* TOL + 2083 060 + 045 .070 + .053 27.891 + 5.714
TTAps 699 + .204 061 + .045 078 + .063 26.490 + 5.597
TTApe .700 + .203 060 + .045 075 + .052 26.978 + 5.629
TTApa 696  .203 062 + .050 .095 + .090 24.994 + 5.308

Table B.9: Results for the denoising task on the LDCT dataset using the 90” percentile
as threshold for OOD detection. An asterisk (*) denotes that results were averaged over
three runs to ensure reproducibility. Metrics are reported for the entire test set A and for
the subset B C A. The best-performing configuration is highlighted in green. The dagger
symbol * marks results from configurations that do not support OOD sample identification.
Nonetheless, we report their performance on the subset B C A, as identified by our
approach, to show how these configurations perform on detected OOD samples.

37


Experiments SSIM t MAE | PSNR t
A BCA A BCA A BCA

o TTA 693 + .199 .706 + .123' 063 + .048 105 + .038' 27.913 + 5. 17.660 + 1.265¢
Competitors He et al. [19] 384+ 167 563+ 0741 388+ .105 308+ .0441 15.723 4 16.845 + 1.1514
Heet al.” 645+ .211 .840+.156' .089+.069 .065+ .043' 24.875 + 4.277 27.395 + 5.4591

TTAcria 697 + .202) 895+ .158 062+ .047 .039 + .030 28.203 + 5.663 32.256 + 4.151

TTARi0* 697+ .201 881+ .156 0634 .048 .047 + .031 5.64 30.222 + 4.196

Our Approach TTARs0* 697 + .202 894+ .158 062+ .048 .040 + .032 31.827 + 4.301
: TTAgs 696 + .201 .863+ 162 063+ .048 .053 + .038 28.964 + 5.266

TTApe 696 + .201 875+ .158 0634 .047 .049 + .035 q 30.292 + 4.669

TTApa 696 + .201 846+ .156 064.074 067+ .041 28.1124 5.644 27.699 + 4.751

Table B.10: Results for denoising task on LDCT dataset using the 98°” percentile as
threshold for OOD detection. An asterisk (*) denotes that results were averaged over
three runs to ensure reproducibility. Metrics are reported for the entire test set A and for
the subset B C A. The best results in each column are highlighted in green. The dagger
symbol * marks results from configurations that do not support OOD sample identification.
Nonetheless, we report their performance on the subset B C A, as identified by our
approach, to show how these configurations perform on detected OOD samples.

Appendix B.2. T,-T> translation task on BraTS 2018

Experiments SSIM t MAE | PSNR t
A BCA A BCA A BCA

No TTA 858+ .040 .857+ .025t 0374 .010 .040+ .006' 26.459 + 1.864 25.408 + .716t

Competitors He et al. [19] 3304 519.2924 0424 571 + .060 .060 + .055' —7.23 + .899 7.719 + .803"
He et al.* 803 + 045.7994 .033' 079 + .028 .073 + .020' 20.498 + 2.760 20.115 + 2.106¢

TTAcia 853 + .044 806 + 034.039 + 014.062 4.018 26.034 £ 2.540 21.163 + 2.258

TTARw* 8544 .043 810 + 032 039 + .013 058+ .017 26.106 + 2.409 21.886 + 2.283

Our Approach PE ARso* 853 + 044.804 + 035.040 + 015.065 + 020 26.032 + 2.554 21.144 + 2.370
ur Approach ‘Ars 8534 .044 .806+.040 0394 .013 .058+.017 26.084 + 2.493 21.662 + 2.719
‘Ape 853+ .044 .805+.035 040+ .014 .063+.018 26.031 £ 2.523 21.131 + 1.977

TTApa 853+ .044 .807 + .035 .040 + .015 + 020 26.042 + 2.545 21.247 + 2.486

Table B.11: Results for T,-T> translation task on BraTS 2018 dataset using the 85*”
percentile as threshold for OOD detection. An asterisk (*) denotes that results were
averaged over three runs to ensure reproducibility. Metrics are reported for the entire test
set A and for the subset B C A. The best results in each column are highlighted in
green. The dagger symbol ' marks results from configurations that do not support OOD
sample identification. Nonetheless, we report their performance on the subset BC A, as
identified by our approach, to show how these configurations perform on detected OOD
samples.

38


Experiments SSIM + MAE | PSNR ¢t
A BCA A BCA A BCA

No TTA 858 + 040 .862+4.0117 .0374.010 .040+ .004t 26.459 + 1.864 24.887 + .734t
Competitors He et al. [19] .330+ .519 = .273+ .026' 571+ .060 .061 + .0427 7.23 + .899 7.789 + .9241
He et al. * 803 + .045  .788+ .016' =.079 4.028 = .080 + .012* ~—- 20.498 + 2.760 18.623 + 1.5257
TTAGra 857+ 041 .791 4.018 0388+ .012 074+ .013 26.354 + 2.103 19.366 + 1.522
TTARi0* 857+ 041 807+ .016 038+ .011 051+ .006 26.417 + 1.929 22.553 + 1.032
Our Approach TTARs0* 857+ 041.7904 .019 =.088 4.012.076 + 014 =. 26.362 + 2.084 19.762 + 1.946
TTAgs 857+ .041  .793 4.028 = .088 + 011 = .066 + .020 =. 26.368 + 2.098 20.080 + 3.383
TTApe 857+ 041.789 + .018 = =.088 + .012) 077 + .016 =. 26.359 + 2.090 19.608 + 1.727
TTApa 857+ 041 = .802 4.021) = 0438 + 011 =.066 + .016 =. 26.390 + 2.059 21.185 + 2.067

Table B.12: Results for T\-T> translation task on BraTS 2018 dataset using the 98*”
percentile as threshold for OOD detection. An asterisk (*) denotes that results were
averaged over three runs to ensure reproducibility. Metrics are reported for the entire test
set A and for the subset B C A. The best results in each column are highlighted in
green. The dagger symbol ' marks results from configurations that do not support OOD
sample identification. Nonetheless, we report their performance on the subset BC A, as
identified by our approach, to show how these configurations perform on detected OOD
samples.

Appendix C. Alternative search strategies

To complement the exhaustive grid search used in our main implementa-
tion, we evaluated four additional strategies for selecting the optimal subset
of reconstruction modules for each OOD test sample. All these strategies op-
erate within the same sample-aware TTA framework and aim to reduce the
computational burden of adaptation while preserving its dynamic, sample-
specific nature.

Random Search. Algorithm 2 implements a random sampling scheme in which
only a fixed number Neonsg of configurations are sampled from the full search
space 2. This significantly reduces computational cost compared to exhaus-
tive search. However, the resulting configuration may be suboptimal, as
random sampling can fail to explore informative regions of the space, espe-
cially for small Neonig. In our experiments, we explore this strategy with
Neonfig = 10 and Neonfig = 50.

Forward Selection. Algorithm 3 introduces a greedy strategy that starts from
a minimal configuration containing only the fixed reconstruction modules at
the input and output levels (R, and Ry). The set of selected intermediate
reconstruction models, Reo, is initially empty and is iteratively expanded
by adding one candidate model at a time. At each step, the configuration

39


yielding the lowest reconstruction error €, = |y* — y®| is retained, and the
process stops when no further improvement is observed.

Backward Elimination. In contrast, Algorithm 4 starts from a full config-
uration in which all intermediate reconstruction models are included, i.e.,
Ree = R. At each step, the algorithm evaluates reduced configurations ob-
tained by removing one model at a time. If a reduced configuration yields
lower reconstruction error, it is adopted as the new candidate. The pro-
cess terminates once further removals degrade performance, and the best-
performing configuration w* is retained.

Bayesian Optimization. Finally, Algorithm 5 implements a model-based strat-
egy that builds a surrogate of the objective function to guide the search. The
first Nstart = 5 configurations are sampled randomly to initialize the history
of evaluations. Subsequent Nnérials — Nstart, With Ntrials = 20 iterations are
driven by a Tree-structured Parzen Estimator (TPE) sampler, a probabilis-
tic model that balances exploration and exploitation [80, 31, 32]. Like the
previous methods, the goal is to identify the subset of reconstruction models
that minimizes the reconstruction error €,.

40


Algorithm 2 Random Search

1: if ey > 7 then
2 ebest ¢ oo, w* + None
3 for all w € RandomSubset(Q, Neon fig) do > Select Neon fig random
4 configurations from 2
5: eet + 00 > Initialize best error for the current configuration
6 for i= 1 to M do > Iterate over adaptor update steps
7 y* =T*(a) > Run the inference with the current configuration w
8 ee &y = ||y* — yl > Evaluate the output reconstruction error
9 if « < eet. then
10: eet + € > Update best error value
11: for the current configuration w
12: end if
13: end for
14: if epee <ebet then p> If the current best error value is better than
15: the overall best error value
16: ebest cnet w* <w > Update best configuration w*
17: end if
18: end for
19: end if
20: return w* > Return best configuration w*

Al


Algorithm 3 Forward Selection

1
2
3
4:
5D:
6
7
8
9

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:

: if ce, >7 then
+ oo, w* «+ None

Reel < O > Start the search without reconstruction models
while (R — Rei) 4 0 do > Continue the search while there are
unselected reconstruction models

w= > Initialize candidate configuration
with no reconstruction models

for r € (R — Ree) do > Iterate over the unselected
reconstruction models

wie wU{r} > Update candidate configuration by adding
reconstruction model r

epee + 00 > Initialize best error for the current configuration

for i= 1 to M do > Iterate over adaptor update steps

y* =T*(x) > Inference with the current configuration w

€< €y = ||\y* — yf|| > Evaluate the output reconstruction error
if « < cbest. then

end for
end while

steps
epest He > Update best error value
for the current configuration w
end if
end for
if epee < ebest then p If the current best error value is better than
the overall best error value
ebest ¢_ epest W* Hw, Reel = Reel U {r} > Update best
configuration
else
break while
end if
return w* > Return best configuration w*

42


Algorithm 4 Backward Elimination

1
2
3
4:
5D:
6
7
8
9

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:

: if ce, >7 then
+ oo, w* «+ None

Reel = R > Start the search with all reconstruction models
while R,-) 4 ) do > Continue the search while there are
reconstruction models in the selected set

W = Reel > Initialize candidate configuration
with all reconstruction models

for r © Reg) do > Iterate over the selected reconstruction models
wiew \ {r} > Update the configuration by removing
reconstruction model r

epest + 00 > Initialize best error for the current configuration

for i= 1 to M do > Iterate over adaptor update steps

y* =T*(x) > Inference with the current configuration w

Ee &y = ||y* — y@|| > Evaluate the output reconstruction error
if « < best then

end for
end while

steps
epest e€ > Update best error value
for the current configuration w
end if
end for
if best. < ePest then > If the current best error value is better
than the overall best error value
ebest ¢_ eet, w* — WwW, Reel = Rese \ {r} > Update best
configuration
else
break while
end if
return w* > Return best configuration w*

43


Algorithm 5 Bayesian Search

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

1
2
3
4
5D:
6
7
8
9

: if ce, >7 then

ePest £ O09, w* + None

for t = 1 to niziais do > Iterate over Bayesian search trials
if t < netart then
w < RandomSample() > Randomly sample a configuration
(initialization phase)
else
w + Sampler({(wi, €y,;)}i21) > Select configuration using surrogate
model based on past evaluations
end if
epest. + 00 > Initialize best error for the current configuration
for i=1to M do > Iterate over adaptor update steps
y* = T(x) > Inference with the current configuration w
Ee &y = ||y* — YF || > Evaluate the output reconstruction error
ife< epee then
epest. + € > Update best error value
for the current configuration w
end if
end for
if best. < cbest then > If the current best error value is better
than the overall best error value
ebest cnet w* <w > Update best configuration w*
end if
end for
: end if
: return w* > Return best configuration w*

44


Appendix D. Wilcoxon Test

To assess the statistical significance of performance differences between
methods, we employ the Wilcoxon signed-rank test [33], a non-parametric
test suitable for paired, non-normally distributed data. It is particularly ap-
propriate for our setting, as it compares the distributions of metric values
(e.g., SSIM, MAE, PSNR) across all test samples without assuming Gaus-
sianity, and it is robust to outliers. We perform pairwise comparisons between
all evaluated methods, independently for each metric, and apply a Bonferroni
correction to account for the increased risk of Type I error due to multiple
comparisons [26]. We use a corrected significance threshold Qo = 0.08 where
m is the number of pairwise comparisons. Results are summarized in com-
pact tables, where each cell corresponds to a pairwise comparison (row vs.
column), and is subdivided into three subcells showing the p-values for SSIM,
MAE, and PSNR. Green subcells indicate statistically significant differences
(p < Qcorr), while red subcells denote non-significant differences (p > Qcorr).
This analysis supports a robust comparison of all TTA configurations and
highlights whether observed improvements are consistent and statistically
reliable.

Wilcoxon test results are reported in Table D.13, and Table D.14, which
correspond to the Mayo Clinic LDCT-and-Projection dataset [20], and [XI
dataset [22], respectively. No statistical test was performed on the BraTS
2018 dataset [21], as TTA consistently degrades performance in this pre-
dominantly ID scenario. Consequently, we did not explore additional search
strategies in this setting.

45


TT Aria
TT Ario
TT Apso
TT Ars
TTApe
TT Apa
NoTTA

Table D.13: Wilcoxon test with Bonferroni correction for denoising task. This table sum-
marizes the pairwise comparisons between different TTA configurations using the Wilcoxon
signed-rank test, with Bonferroni correction applied to account for multiple comparisons.
Each cell corresponds to a comparison between two methods (row vs. column) and is split
into three subcells, reporting the statistical significance (p-value) for SSIM, MAE, and
PSNR, respectively. Green subcells indicate statistically significant differences (p < Qcorr),
while red ones indicate non-significant differences (p > Qcorr).

TT Acria | TT ArRio TT Arso TT Ars TT Ape TTApa | NoTTA
TT Aria
TT Ario
TT Apso
TT Ars
TT Ape
TT Apa
NoTTA

Table D.14: Wilcoxon test with Bonferroni correction for T\-T> MRI translation on [XI
dataset. This table summarizes the pairwise comparisons between different TTA configu-
rations using the Wilcoxon signed-rank test, with Bonferroni correction applied to account
for multiple comparisons. Each cell corresponds to a comparison between two methods
(row vs. column) and is split into three subcells, reporting the statistical significance (p-
value) for SSIM, MAE, and PSNR, respectively. Green subcells indicate statistically signif-
icant differences (p < Qcorr), while red ones indicate non-significant differences (p > Qcorr)-

46