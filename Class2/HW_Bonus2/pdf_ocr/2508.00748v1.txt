2508.00748v1 [cs.CV] 1 Aug 2025

arXiv

Is It Really You? Exploring Biometric Verification Scenarios
in Photorealistic Talking-Head Avatar Videos

Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez,
Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez
Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Spain

Abstract

Photorealistic talking-head avatars are becoming in-
creasingly common in virtual meetings, gaming, and so-
cial platforms. These avatars allow for more immersive
communication, but they also introduce serious security
risks. One emerging threat is impersonation: an attacker
can Steal a user’s avatar—preserving their appearance and
voice—making it nearly impossible to detect its fraudu-
lent usage by sight or sound alone. In this paper, we ex-
plore the challenge of biometric verification in such avatar-
mediated scenarios. Our main question is whether an indi-
vidual’s facial motion patterns can serve as reliable behav-
ioral biometrics to verify their identity when the avatar’s
visual appearance is a facsimile of its owner. To an-
swer this question, we introduce a new dataset of realis-
tic avatar videos created using a state-of-the-art one-shot
avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight,
explainable spatio-temporal Graph Convolutional Network
architecture with temporal attention pooling, that uses only
facial landmarks to model dynamic facial gestures. Exper-
imental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approach-
ing 80%. The proposed benchmark and biometric system
are available! for the research community in order to bring
attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

1. Introduction

In recent years, there has been a surge in novel methods
for generating photorealistic avatars [3, 12, 13, 21] that can
create and animate high-quality 3D human avatars with a
single image or text prompt. In parallel, industry investment
in avatar technology has exploded, with companies such as

Inttps://github.com/BiDAlab/GAGAvatar-Benchmark

2
Real ID = Virtual ID Real ID = Virtual ID
De |

Avatar
generation

Avatar
theft

Impostor

Impostor assumes

~~ Driver other identity

[3 Target
Figure 1. Avatar impostor attack scenario: An impostor steals a
target identity’s avatar and pretends to be that identity in a virtual
meeting. The visual appearance and voice of the impostor’s avatar
is exactly the same as the target’s. The behavioral biometrics, such
as facial gestures, however, are driven by the impostor.

Synthesia” securing hundreds of millions of dollars in fund-
ing to scale avatar services which are being used by over one
million users. Another noteworthy example is Meta’ offer-
ing to pay users to capture their facial expressions, speech
and gestures to train its Codec Avatars model [15].

These photorealistic talking-head avatars are becoming
popular in virtual meetings, gaming and metaverse plat-
forms. A good example of this popularity was the virtual-
reality interview* between Mark Zuckerberg and Lex Frid-
man in his podcast, which had nearly 3 million views.
While these avatars allow for immersive communication,
they also pose critical security risks for society, as depicted
in Fig. 1. An impostor can steal the avatar of an identity and
impersonate him or her in real time [18].

A recent example of such identity theft was a video scam
in early 2024°, in which criminals used an AI-generated
video of a company executive to fool another employee into
transferring ~$25M USD to fraudulent accounts. This in-
cident demonstrates just how difficult it is to distinguish a
real person from an impersonated one. Indeed, as avatar

2https://cnb.cx/3WECHFt

3https: //www.uploadvr.com/meta-project—-warhol-
codec-avatars-training-paying/

4nttps://www. youtube. com/watch?v=MVYrJJNdrEg

Shttps : / / edition. cnn. com/2024/02/04/asia/
deepfake-cfo-scam—hong-kong-int1-hnk


technology improves, such impostor attacks may become
nearly impossible to detect by sight or sound alone. This
is the specific use case considered in this study, where the
avatar stolen by the impostor is so realistic in terms of ap-
pearance and voice that only behavioral biometric infor-
mation, such as facial gestures, can be used to detect if the
driver identity moving the avatar is the real owner of the
avatar or an impostor.

It is important to highlight that the aforementioned sce-
nario is different than the traditional DeepFake detection
scenario [17, 19, 20], in which the main task consists of
simply predicting if a video is real or fake, not whether the
person driving the avatar is, in fact, the owner of the avatar.

In our study, we seek to explore biometric verification in
avatar scenarios by addressing the following question:

Can an individual’s facial motion patterns serve

as a reliable biometric characteristic to verify
their identity during avatar-mediated communi-
cation?

In the context of facial avatars, each person’s manner of
speaking and emoting is distinctive and can be difficult to
imitate. By capturing these subtle dynamics, it might be
possible to verify if a photorealistic talking-head avatar is
being driven by its owner or an impostor.

To investigate this question, we design an end-to-end
experimental framework that simulates real-world avatar-
mediated impersonation attacks while isolating facial mo-
tion as the distinguishing biometric signal. To the best of
our knowledge, the only study that has previously analyzed
this scenario is [16], from NVIDIA. In their study, the au-
thors coined the term “avatar fingerprinting” to refer to
the detection of unauthorized use of avatars. While their
study reported promising results, we encountered the fol-
lowing problems when attempting to reproduce it: i) the
proposed method and its corresponding code is not publicly
available, limiting its reproducibility, and ii) most research
groups do not have access to the GPU resources required
to train NVIDIA’s verification model. Our study aims to
bridge these gaps by publicly releasing our proposed avatar
dataset and standard benchmark, as well as exploring lighter
biometric verification approaches.

The main contributions of our research are as follows:

¢ A new public avatar video dataset generated using
a state-of-the-art one-shot avatar generator, GAGA-
vatar [3], including both genuine and impostor avatar
videos.

¢ A standard public benchmark protocol for avatar
verification, defining clear train-validation-test splits
and evaluation protocols that are reproducible by the
research community.

¢ A first exploration of this challenging avatar sce-
nario proposing a lightweight, explainable behav-
ioral biometric system that is based solely on the fa-
cial motion patterns of the identities. Our approach is
based on a Graph Convolutional Network (GCN). Un-
like [16] that relies on high-dimensional pairwise dis-
tance tensors (i.e., all-pairs landmark distances), our
approach is well suited for facial motion modeling as
it explicitly encodes the mesh-like geometry of the
face through graph structure. This results in both spa-
tial awareness (by preserving local neighborhood re-
lationships among facial regions) and parameter effi-
ciency, given the small and fixed graph size typical of
landmark-based representations, allowing faster train-
ing and inference.

The remainder of the paper is organized as follows.
Sec. 2 describes the key terminology used throughout the
paper. Sec. 3 describes the real datasets and avatar video
generation. Sec. 4 describes the biometric verification sys-
tem proposed in the study. The experimental protocol and
results are described in Sec. 5 and Sec. 6, respectively.
Sec. 7 continues with a discussion and Sec. 8 concludes
with key takeaways.

2. Terminology

To provide a solid understanding of our research, we
first define the terminology used throughout this paper (see
Fig. 2 for a visual explanation of the terminology):

¢ Target Identity. The person whose appearance the
avatar has.

¢ Driver Identity: The person whose facial motion con-
trols the avatar.

¢ Avatar Video: A video with the target’s appearance and
the driver’s motion, generated from a target image and
a driver video.

* Genuine Avatar: When the driver identity and the tar-
get identity correspond to the same person, also known
as self-reenactment. Genuine avatars represent how the
avatar of a real user would appear in a virtual meeting.

¢ Impostor Avatar: When the driver identity does
not match the target identity (also known as cross-
reenactment), i.e., when the person driving the move-
ments of the avatar is not the same as the person cor-
responding to the avatar’s appearance. These avatars
simulate impersonation attacks, where an impostor
uses someone else’s avatar.

* Genuine Comparison: Comparing a reference video
from a genuine avatar against another test video gen-
erated using the same genuine avatar. A biometric


Driver + Driver j
Video Video

Target 7
Image
Target
Target j Identity 7
Image

Identity +
Real Videos

Reference N-1
Avatar ta see Genuine Test
Video . Avatar Videos

Impostors Steal

Target’s Avatar
M Real Videos
aa from Impostor
Identities

N Real Videos
“ from Identity 7

Genuine
Comparisons

M Ml y M
vee Impostor
Comparisons
M
O00 Impostor Test:
Avatar Videos

Figure 2. Avatar generation and the proposed evaluation protocol. (Left): An avatar video is generated using a target image (leftmost
column) as the appearance for the identity, plus a driving video (top row) to generate the facial motion in the avatar. (Right): For the
evaluation of our proposed biometric system in avatar scenarios, we generate all genuine and impostor comparisons for each target identity
z, using all videos from 2 and some videos from impostor identities, respectively.

verification system should consider the latter as au-
thorized, indicating that both videos correspond to the
same real identity.

¢ Impostor Comparison: Comparing a reference video
from a genuine avatar against another test video gen-
erated using an impostor avatar that has the same
appearance (target identity) as the reference genuine
avatar video. A biometric verification system should
consider the latter as unauthorized, indicating that its
identity is not the same as the reference’s.

3. Datasets

This section describes the datasets used in this study, in-
cluding the generation of a dedicated avatar video dataset to
capture identity-specific motion patterns.

3.1. Real datasets

We use two public video datasets for avatar video gener-
ation. These datasets are selected as they simulate a similar
environment to virtual meetings.

* CREMA-D* [2]: This dataset provides 7,442 short
videos of 91 actors (48 male, 43 female, ages 20-74,
diverse ethnicity) performing scripted sentences in var-
ious emotional states. Each actor utters 12 different

®nttps : / / github . com / CheyneyComputerScience /
CREMA-D

sentences with one of six basic emotions (anger, dis-
gust, fear, happy, neutral and sad) at multiple intensity
levels. These videos were recorded in a controlled set-
ting with frontal views of the speaker.

* RAVDESS’ [14]: This dataset contains 24 actors (12
female, 12 male) speaking two fixed statements with
eight emotions (calm, happy, sad, angry, fearful, sur-
prise, disgust and neutral). All videos were recorded in
a studio environment with actors facing front against a
green-screen background. In total, we use the 1,440
video-only speech files from the original dataset.

3.2. Avatar Video Dataset Generation

To generate our avatar dataset, we employ the state-
of-the-art avatar generation model GAGAvatar® [3]. This
model takes a reference image (representing the target iden-
tity’s appearance) and a driving video (providing the driver
identity’s motion), and generates a new avatar video that
preserves the visual appearance of the reference image
while reproducing the facial expressions and movements
from the driving video. The resolution of the videos gener-
ated is 512x512 with a black background. It is worth noting
that the GAGAvatar model does not use the audio from the
original videos to generate the avatars, only the visual data.

For each identity in the CREMA-D and RAVDESS
datasets, we first select a single, high-quality target image

Thttps://zenodo.org/records/1188976
8nttps://github.com/xg-chu/GAGAvatar


Landmarks
Extraction

{GO = (VO, BOE, (a yy

Temporal

\—> GCN —_ © es — §$Attention
J ‘ Fj Pooling

a? eRe ee Re

Figure 3. Proposed biometric verification system: For each frame t in a video, a graph G™ is built. All the T’ graphs from the video are

passed to the 3-layer GCN, obtaining one graph embedding ny per frame t. Finally, those T’ graph embeddings are passed to the attention

pooling module, resulting in a single embedding e per video, with e € Re Jd!) being the output dimension of the last GCN layer, L.

RY \ZNN
‘ x nie

(a) (b) (c)

Figure 4. Landmarks extracted for each video frame: (a) shows
all MediaPipe [11] landmarks. (b) shows the 109 selected land-
marks connected via Delaunay triangulation [6]. (c) The normal-
ization of landmarks is done subtracting the nose tip landmark po-
sition and scaling with intercanthal distance d over the frames.

to represent that identity’s reference appearance. Whenever
possible, we select a frontal face with neutral expression,
such as in Fig. 2, “Target Images”. Next, we generate gen-
uine avatar videos using said identity’s target image and all
of his or her real videos as driving videos. This approach al-
lows us to obtain new avatar videos in which the avatar’s ap-
pearance corresponds to the target identity and the avatar’s
facial movements are genuinely made by him or her. This
scenario would correspond to the typical use of the avatar
by its owner in a virtual meeting.

We then generate impostor avatar videos. For each tar-
get identity, we randomly sample a subset of videos from
other identities and use them as drivers. That way, we can
obtain a set of impostor avatar videos in which the appear-
ance is the target’s, while the facial movements correspond
to other people, as seen in Fig. 2.

Despite GAGAvatar being state-of-the-art, in some cases
it cannot generate avatar videos that perfectly match the ex-
pressions from the driving video. For instance, in Fig. 2
(left), the column corresponding to “Driver 7” shows that,
for the avatar generated with appearance from “Target j”’,
the facial expression is subtly different from the original
video. The avatar is not frowning as much, and its mouth
does not follow the original expression with precision.

4. Biometric Verification System for Avatars

Our proposed biometric system is described in Fig. 3. It
is based on a Graph Convolutional Network (GCN) model,
which is fed a simplified face mesh obtained from facial
landmarks that are later aggregated through time via a tem-
poral attention mechanism. Our hypothesis is that this
method will be able to capture discriminative intra-frame
spatial patterns of facial movement (e.g., which regions
move together, muscle activation patterns), while consid-
ering their temporal variations via the attention mechanism.
Next, we describe the key details of each part of the system.

4.1. Landmarks Extraction

To focus on the facial motion patterns of the identities,
we first extract the facial landmarks for each video using
MediaPipe” [11], which provides 468 3D facial landmarks
per frame, as shown in Fig. 4 (a). We manually select a
subset of 109 key landmark points (omitting redundant or
less informative points) to reduce computational complexity
while covering the main facial regions (eyes, brows, nose,
mouth, jawline, etc.), as shown in Fig. 4 (b).

We then normalize the facial landmarks using a frame-
based approach. In particular, we take the 3D position of
the nose tip landmark shown in red in Fig. 4 (c), and we sub-
tract it from all the other landmark positions in that frame
to achieve translation invariance. Then, we scale the land-
marks positions with the intercanthal distance, i.e., distance
between the two inner corners of the eyes, shown in blue in
Fig. 4 (c), resulting in scale invariance.

4.2. Graph Neural Networks

A Graph Neural Network (GNN) is a neural architecture
designed to process data represented as a graph G = (V, FE),
where V is the set of nodes and F the set of edges. Each
node v € V is associated with an initial feature vector

€ R¢ (i.e., a landmark’s 3D coordinate, with d = 3) and,
through a series of L message-passing layers, the GNN up-

dates each node’s features ni by aggregating information

*hnttps : / / ai . google . dev / edge / mediapipe /
solutions/vision/face_landmarker


from its neighbors, i.e., nodes connected to it by an edge.

By stacking L layers, each node’s representation ayy) in
corporates information from nodes up to L hops away, cap-
turing both local and global graph structure. The final node
embeddings can then be pooled (i.e., averaged or summed)
to produce a graph-level embedding, hg.

GNNs are particularly well suited for facial motion mod-
eling because they explicitly encode the mesh-like geome-
try of the face through graph structure, enabling parame-
ter sharing across spatially connected landmarks. This re-
sults in both spatial awareness (by preserving local neigh-
borhood relationships among facial regions) and parame-
ter efficiency, given the small and fixed graph size typi-
cal of landmark-based representations. In contrast to ap-
proaches that rely on high-dimensional pairwise distance
tensors (e.g., all-pairs landmark distances as in [16]), a
graph representation avoids quadratic growth in input size
and facilitates faster training and inference.

In particular, we use Graph Convolutional Networks
(GCNs), whose message-passing mechanism aggregates in-
formation from a node’s local neighbors at each layer. This
convolution-like propagation naturally models coordinated
facial movements, such as simultaneous eyebrow raises or
lip articulations, by allowing information to flow across
connected regions. Stacking multiple GCN layers increases
the receptive field over the mesh, capturing both local and
global facial dynamics.

These unique properties make GCNs an effective choice
for learning per-frame representations that are sensitive to
the spatio-structural patterns characteristic of an individ-
ual’s facial motion.

4.3. Proposed System

Our biometric system follows a spatio-temporal archi-
tecture designed to learn identity-specific facial motion pat-
terns from avatar videos. As illustrated in Fig. 3, the core
idea is to represent each video as a sequence of facial land-
mark graphs, process these with a GCN to encode spatial
structure, and then aggregate them via a temporal attention-
based pooling mechanism.

Each avatar video is divided into non-overlapping fixed-
length clips of T’ = 50 frames. This choice is motivated by
prior work [16], which found that sequences approaching
-30 frames begin to yield strong verification performance
with landmark-based inputs.

Given an avatar clip of length T’ frames, we first extract
V = 109 3D facial landmarks per frame t using MediaPipe
[11]. For each frame t, the landmarks define the nodes V of
a graph whose edges F are constructed via Delaunay trian-
gulation [6]. The result is a sequence {G“)}7_, of graphs
that encode both local and global facial structure.

Each graph G') in the sequence is then processed by a 3-
layer GCN (L = 3), in which the first layer transforms the

initial feature vector x, into an embedding a) ER

d\)
the second layer transforms ns into ni?) € Re, and
the third layer transforms ni?) into rn?) € Re, being
d) = d®) = 64, and d@) = 256. Within each layer, node
features are updated by aggregating information from their
immediate neighbors, followed by non-linear ReLU activa-
tion and dropout (p = 0.3). Applying the same GCN across
all T’ frames results in a sequence of spatial embeddings
{hye that encode the per-frame geometry of facial ex-
pressions.

To convert this sequence of frame embeddings into a sin-
gle, discriminative descriptor of the entire clip, e € Re,
we consider a temporal attention mechanism. This mod-
ule learns to assign weights to each frame-level embed-
ding based on its relevance for identity verification, yield-
ing higher attention weights in frames with distinctive fa-
cial motion patterns and smaller attention weights in less
distinctive intervals (e.g., static segments). The attention
scores are applied to the frame embeddings to compute a
weighted sum, yielding the final clip-level embedding.

5. Experimental Protocol

For reproducibility reasons, we adopt the same train-
validation-test split on CREMA-D and RAVDESS datasets
as in [16]. In this protocol, all identities are split disjointly
across sets to ensure that no identity in the test set (neither
targets nor drivers) are seen during training or validation.
The same identity can only be used as driver or target within
the same split. We refer to these data splits as Dyrain, Pyai,
and Dest. Next, we describe the key details for the training
and final evaluation protocols.

5.1. Biometric System: Training

Our biometric system is trained on Dyrain, with hyper-
parameters tuned on D,,). The final model checkpoint is
selected based on the lowest validation loss. Training is
conducted using triplet loss. Each triplet (a, p,) consists
of: i) an anchor, a, which can be any avatar video (genuine
avatar or impostor avatar), ii) a positive sample, p, which is
any avatar video driven by the same person as a (regardless
of target identity), and iii) a negative sample, n, which is
any avatar video driven by a different person than a (again,
regardless of target identity). We do not mine hard or semi-
hard triplets, but only random triplets.

The model is implemented in PyTorch, using PyTorch
Geometric for the GCN layers. We train on a single
NVIDIA RTX 4090 GPU, with typical training times of
about 2 hours for 200 epochs, using a batch size of 1,024
and an Adam optimizer with a learning rate of le~*.


5.2. Biometric System: Evaluation

For the final evaluation of the biometric system, we cre-
ate verification pairs from Pes: by generating all possible
genuine and impostor comparisons for all identities in Dest,
as can be seen in Fig. 2 (right).

For a given identity 2 in Dregs, we use a high-quality neu-
tral image (target image), from which we generate a genuine
avatar. We use all original videos from identity 2 to gener-
ate N genuine avatar video clips, which simulate the real
usage of 2’s avatar. From all those genuine avatar clips, we
take one as reference clip, and we build all the N — 1 gen-
uine comparisons with the remaining genuine avatar clips.
We gather all other identities in D,-s,, and we generate
impostor avatar clips by using the impostor videos and the
target’s avatar. We then use the same reference genuine
avatar clip to compare with the M/ impostor avatar clips,
obtaining M impostor comparisons.

We repeat this process using all identities in Dies, and
using each of their genuine avatar clips as a reference clip
for the comparisons. We use the scores obtained by our
biometric system from all the genuine comparisons and all
the impostor comparisons to obtain the performance results.

This protocol explicitly tests if the biometric system can
distinguish whether two avatar videos share the same under-
lying driver despite identical visual appearance. In addition,
as Diest contains identities that are unseen during training or
validation, the reported results reflect the generalization ca-
pability of the biometric system on new identities.

Similar to [16], we use Area Under the Curve (AUC) as
the primary metric for verification performance.

6. Experimental Results

This section evaluates our proposed biometric verifi-
cation system in multiple settings. All experiments use
the same identity-disjoint train-validation-test splits as de-
scribed in Sec. 5, ensuring that identities included in the
final evaluation data split D;es, are entirely unseen.

6.1. Results on Real Data (w/o Avatars)

First, to validate our proposed biometric verification sys-
tem, we evaluate the performance of the system on the orig-
inal test videos (i.e., the real videos without avatar genera-
tion). For this experiment, we train our system using avatar
videos, and then evaluate on the original videos (without
avatars) from different datasets. Because these real videos
perfectly preserve each user’s natural facial motion, this
setup helps us estimate the potential upper-bound perfor-
mance achievable if the avatar generation process perfectly
preserved the original facial motion, i.e., how well our
system can discriminate facial motion signatures when no
avatar generation artifacts are present.

As can be seen in Fig. 5 (curves labeled “RAVDESS

Reals” and “CREMA-D Reals”), our biometric system
achieves AUC values up to 83%, demonstrating that the pro-
posed approach is effective for biometric verification using
only landmark-based motion cues.

6.2. Intra-Dataset Analysis

Next, we analyze the virtual meeting scenario using
avatars, following the experimental protocol described in
Sec. 5, focusing on the intra-dataset analysis. That is, we
train and test the biometric system on avatar clips derived
from the same dataset (CREMA-D or RAVDESS). These
intra-dataset experiments simulate a scenario where enroll-
ment and verification stages use avatars generated under
similar conditions and from similar populations.

For the RAVDESS dataset, Fig. 5 (left), our system
achieves 69.92% AUC when trained on the same dataset.
In the case of CREMA-D dataset, Fig. 5 (center), the AUC
is 82.58%. This higher performance on CREMA-D dataset
might be due to: i) CREMA-D dataset has a larger num-
ber of identities and clips than RAVDESS, which improves
training diversity by adding variability to the facial land-
mark layouts, so the model does not focus in “absolute”
landmark positions; and ii) CREMA-D has richer and more
varied emotional expressions and speech content, which
yield stronger behavioral biometric information.

6.3. Inter-Dataset Analysis

To assess generalization, we evaluate the performance
of our biometric system across datasets. We train on one
dataset (e.g., RAVDESS Dyrain) and test on the other (e.g.,
CREMA-D Pres), evaluating whether the system learns
generalizable facial motion representations, or overfits to
recording conditions, actor pools, or expression styles of a
specific dataset. We follow the same protocol as described
in Sec. 5. These inter-dataset experiments represent a more
realistic real-world scenario.

In the case of training with CREMA-D Dyain and eval-
uating on RAVDESS Dest, Fig. 5 (center), our biomet-
ric system achieves 75.38% AUC, which is lower than its
corresponding intra-dataset result for CREMA-D (82.58%
AUC), as expected, while in the case of training with
RAVDESS Prain and evaluating on CREMA-D Dest, Fig. 5
(eft), we obtain 78.78% AUC, which is higher than its
corresponding intra-dataset result for RAVDESS (69.92%
AUC). This counterintuitive result might be explained by
the richer expressive variation in the CREMA-D test set.
While RAVDESS training enforces more general motion
representations due to its limited variability, the CREMA-D
test data offers more diverse and distinctive facial motions,
making impostor and genuine comparisons more separable
in embedding space. Thus, domain shift here actually acts
as a form of regularization, and the richer test-time variation
improves verification performance.


Trained on RAVDESS

Trained on CREMA-D

1 gurained on RAVDESS + CREMA-D

10 rd LO Sc ae a F
L- » , oe oe “ -
ler ae LY Z 7 v4 a
0.84 4 ee ra 84 F . ae 0.8 4 7|
aa a ed a “
ec 147 “ oc iy “ a “
Oy yA “ Oo I/ amy “
A Vas a sa 1; oa “
0.4.47 a Ayit 0.44 a
uf “ if ¢ Test dataset
t Test dataset | Test dataset == CREMA-D: AUC=83.31
0.2 44 == CREMA-D: AUC=78.78 27 ye CREMA-D: AUC=82.58 0.24 —- RAVDESS: AUC=74.66
“ —=* RAVDESS: AUC=69.92 “om RAVDESS: AUC=75.38 ra CREMA-D Reals: AUC=82.82
“ RAVDESS Reals: AUC=72.56 rae CREMA-D Reals: AUC=83.41 “ — RAVDESS Reals: AUC=75.39
00 02 O04 06 08 1.0 0.0 0.2 06 08 1.0 0.0 O02 04 06 O8 41.0
FPR FPR FPR

Figure 5. Experimental results using our proposed biometric system. (Left) ROC curves obtained when we train our model with avatar
clips based on RAVDESS data. (Center) ROC curves when we train our model with avatar clips based on CREMA-D data. (Right) ROC
curves when we train our model with avatar clips based on RAVDESS and CREMA-D data combined.

6.4. Combined Datasets

In this experiment, we evaluate whether increasing
training data diversity improves verification performance.
Specifically, we train our biometric system on the avatar
datasets generated from combining both CREMA-D and
RAVDESS identities. We hypothesize that by exposing the
model to a broader variety of identities, expressions, emo-
tions, and speaking styles, it would help it learn more gen-
eralizable facial motion-based representations.

For evaluation, we use the standard Dies splits of each
dataset separately, ensuring no identity overlap with the
training data. As can be seen in Fig. 5 (right), we can ob-
serve an improvement in AUC compared to their respec-
tive intra-dataset results, for both CREMA-D (from 82.58%
to 83.31% AUC) and RAVDESS (from 69.92% to 74.66%
AUC) datasets. If evaluated in their corresponding real
test datasets, Fig. 5 (right, labeled “RAVDESS Reals” and
“CREMA-D Reals”), the AUC obtained is very similar,
which supports the hypothesis that our system works simi-
larly for avatar videos or real videos.

These gains demonstrate that combining datasets intro-
duces useful variability that enhances the model’s ability to
capture consistent, identity-specific facial motion patterns.

6.5. Explainability

To better understand what our biometric system learns,
we analyze the temporal attention weights produced by the
pooling mechanism. An example of the attention curves
over time is shown in Fig. 6, in which we include the at-
tention weight curves and some video frames around the at-
tention peaks for two genuine avatar clips and an impostor
avatar clip for the same target appearance.

Visual inspection of attention curves across multiple
clips reveals that the weights tend to peak around frames
close to the apex of identity-characteristic expressions,

such as wide mouth movements, eyebrow raises, or other
distinctive facial gestures or behavioral patterns. These
peaks indicate that the system assigns higher importance
to frames with rich, discriminative facial motion patterns,
while down-weighting more neutral or static frames.

These observations suggest that the model relies on be-
havioral biometrics rather than the static structure of facial
landmarks. Even though the appearance is identical in all
avatar videos, the system learns to pick out subtle dynamic
patterns unique to each individual.

This behavior confirms our design hypothesis that tem-
poral attention can automatically identify and emphasize
the most informative facial motion patterns without requir-
ing predefined expression labels, facial gesture or action
units detection, or any alternative method for facial gesture
detection. It also provides intuitive explainability, helping
users understand which parts are driving verification.

7. Discussion

Our experiments show that facial motion-based bio-
metric verification is feasible even in challenging avatar-
mediated scenarios. Across multiple settings and datasets,
our proposed biometric system has achieved AUC values
surpassing 80% in the best cases. It is worth noting that
our system exclusively focuses on landmarks alone, with-
out using any facial appearance cues or conventional Deep-
Fake detection features. This is a key design choice: in real
avatar-mediated communication, all users share the same
high-quality rendering pipeline, and appearance artifacts
will not distinguish impostors from genuine users.

Our dataset generation strategy (using impostor avatars
that perfectly match the victim’s appearance) forces the ver-
ification model to focus exclusively on behavioral facial
motion patterns rather than static facial structure. If we had
simply used the original real videos, the model could triv-


Genuine Avatar Clip 1

0.10

0.05

Attention

0.00

Genuine Avatar Clip 2

Attention

0.0

Impostor Avatar Clip

0.100

0.075

ion

0.050

Attent

0.025

1 10 20 30 40 50
Frame

Figure 6. Example of attention weights obtained for two genuine avatar videos (first and second row) and one impostor avatar
video (bottom row) for an identity from the RAVDESS dataset: The attention weights peak at frame Tmax *%, around frames with
characteristic facial gestures from the underlying identity. In this case, the woman shows very similar facial expressions in the frames with
highest attention values in both genuine videos. In the third row, the impostor avatar is making a facial gesture around frame Tyaz that is
probably distinctive from the real underlying impostor identity, not the target identity.

ially rely on differences in facial geometry to verify identi-
ties. Such structural cues would be useless in a real attack
scenario, where a stolen avatar would exactly replicate the
victim’s face. By constructing impostor avatars, we ensure
the model is trained on the realistic and challenging prob-
lem of detecting who is driving the avatar’s movements.

Compared to the only other published system for this
task [16], which reports up to 87% AUC using a propri-
etary model trained on three datasets, our approach remains
competitively despite using fewer data and GPU resources.
Our biometric system is also lighter with reduced compu-
tational complexity, and provides explainability through the
temporal attention weights revealing which frames and ex-
pressions the model considers most informative. This posi-
tive aspect is crucial, as described in the literature [4, 5].

One limitation of our biometric system is its exclusive
reliance on landmark-based representations: while inter-
pretable and efficient, they depend on the quality of the
landmark estimator. We have used MediaPipe [11], a
widely adopted and robust model, but inaccuracies can oc-
cur, especially under challenging poses or expressions, po-
tentially reducing verification accuracy.

Another limitation is inherent to the avatar generation
process itself. While GAGAvatar represents the state-of-
the-art in one-shot talking-head synthesis [3], it does not
always faithfully reproduce subtle or extreme facial expres-
sions from the driver. Improving avatar generation fidelity,
particularly in capturing fine-grained facial dynamics, could
directly benefit biometric performance.

8. Conclusions

In this work, we have introduced the challenge of
biometric verification in photorealistic talking-head avatar
videos, where impostors can perfectly impersonate a vic-
tim’s appearance. We have proposed a lightweight, explain-
able system based solely on facial landmarks’ motion, using
a spatio-temporal GCN with temporal attention-based pool-
ing to learn discriminative behavioral signatures.

In addition to the proposed biometric system and code,
we have also released a public standard benchmark of gen-
uine and impostor avatar videos for this scenario. Our ex-
periments have preliminary shown that facial motion pat-
terns are effective biometric cues, demonstrating the poten-
tial of behavioral biometrics as a defense against avatar-
based impersonation. Future work will be oriented to im-
prove the performance through novel deep learning archi-
tectures [7, 9, 10] and loss functions [1, 8] and the evalu-
ation of the biometric system under more challenging sce-
narios and avatar generation models [3, 12, 13, 21].

Acknowledgements

This study has received funding from INTER-ACTION
(PID2021-126521OBI00 MICINN/FEDER), HumanCAIC
(TED2021-131787B-100 MICINN), Catedra ENIA UAM-
VERIDAS en IA Responsable (NextGenerationEU PRTR
TSI-100927-2023-2), and PowerAI+ (SI4/PJI/2024-00062
Comunidad de Madrid and UAM).


References

(1)

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper. Elas-
ticFace: Elastic Margin Loss for Deep Face Recognition. In
Proc. IEEE/CVF Computer Vision and Pattern Recognition
Conference, 2022. 8

H. Cao, D. G. Cooper, and M. K. 0. Keutmann. CREMA-
D: Crowd-Sourced Emotional Multimodal Actors Dataset.
IEEE Transactions on Affective Computing, 5:377-390,
2014. 3

X. Chu and T. Harada. Generalizable and Animatable Gaus-
sian Head Avatar. In Proc. Advances in Neural Information
Processing Systems, 2024. 1, 2,3, 8

I. DeAndres-Tame, M. Faisal, R. Tolosana, et al. From
Pixels to Words: Leveraging Explainability in Face Recog-
nition Through Interactive Natural Language Processing.
In Proc. International Conference on Pattern Recognition
Workshops, 2024. 8

I. Deandres-Tame, R. Tolosana, R. Vera-Rodriguez, et al.
How Good is ChatGPT at Face Biometrics? A First Look
into Recognition, Soft Biometrics, and Explainability. JEEE
Access, 12:34390-34401, 2024. 8

B. Delaunay. Sur la sphére vide. a la mémoire de georges
voronoi. Bulletin of Academy of Sciences of the USSR,
(6):793-800, 1934. 4, 5

P. Delgado-Santos, R. Tolosana, R. Guest, et al. Exploring
Transformers for Behavioural Biometrics: A Case Study in
Gait Recognition. Pattern Recognition, 143:109798, 2023. 8
N. Gonzalez, G. Stragapede, R. Vera-Rodriguez, et al.
Type2Branch: Keystroke Biometrics based on a Dual-branch
Architecture with Attention Mechanisms and Set2set Loss.
IEEE Transactions on Information Forensics and Security,
2025. 8

M. Goswami, K. Szafer, A. Choudhry, et al. MOMENT: A
Family of Open Time-Series Foundation Models. In Proc.
International Conference on Machine Learning, 2024. 8

M. Ivanovska, L. Todorov, N. Damer, et al. SelfMAD:
Enhancing Generalization and Robustness in Morphing At-
tack Detection via Self-Supervised Learning. In Proc. IEEE
International Conference on Automatic Face and Gesture
Recognition, 2025. 8

Y. Kartynnik, A. Ablavatski, I. Grishchenko, et al. Real-time
Facial Surface Geometry from Monocular Video on Mobile
GPUs. arXiv preprint arXiv: 1907.06724, 2019. 4,5, 8

L. Li, Y. Li, Y. Weng, et al. RGBAvatar: Reduced Gaus-
sian Blendshapes for Online Modeling of Head Avatars. In
Proc. IEEE/CVF Computer Vision and Pattern Recognition
Conference, 2025. 1, 8

H. Liu, X. Wang, Z. Wan, et al. AvatarArtist: Open-Domain
4D Avatarization. In Proc. IEEE/CVF Computer Vision and
Pattern Recognition Conference, 2025. 1, 8

S. R. Livingstone and F. A. Russo. The Ryerson
Audio-Visual Database of Emotional Speech and Song
(RAVDESS): A Dynamic, Multimodal Set of Facial and
Vocal Expressions in North American English. PloS one,
13:e0196391, 2018. 3

J. Martinez, E. Kim, J. Romero, et al. Codec Avatar Studio:
Paired Human Captures for Complete, Driveable, and Gen-

[16]

[17]

[18]

[19]

[20]

[21]

eralizable Avatars. In Proc. Advances in Neural Information
Processing Systems, 2024. 1

E. Prashnani, K. Nagano, S. De Mello, et al. Avatar Fin-
gerprinting for Authorized Use of Synthetic Talking-Head
Videos. In Proc. European Conference on Computer Vision,
2024. 2,5, 6, 8

C. Rathgeb, R. Tolosana, R. Vera-Rodriguez, et al. Hand-
book of Digital Face Manipulation and Detection: From
DeepFakes to Morphing Attacks. Springer Nature, 2022. 2
S. Tariq, A. Abuadbba, and K. Moore. Deepfake in the Meta-
verse: Security Implications for Virtual Gaming, Meetings,
and Offices. In Proc. 2nd Workshop on Security Implications
of Deepfakes and Cheapfakes, 2023. |

R. Tolosana, R. Vera-Rodriguez, J. Fierrez, et al. DeepFakes
and Beyond: A Survey of Face Manipulation and Fake De-
tection. Information Fusion, 64:131-148, 2020. 2

L. Verdoliva. | Media Forensics and DeepFakes: An
Overview. IEEE Journal of Selected Topics in Signal Pro-
cessing, 14(5):910—932, 2020. 2

Y. Xu, B. Chen, Z. Li, et al. Gaussian Head Avatar: Ultra
High-fidelity Head Avatar via Dynamic Gaussians. In Proc.
IEEE/CVF Computer Vision and Pattern Recognition Con-
ference, 2024. 1,8